{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Tumor Data\n",
    "\n",
    "Here we estimate a generalized lasso model (sparse fused lasso) via the constrained lasso. \n",
    "\n",
    "In this example, we use a version of the comparative genomic hybridization (CGH) data from [Bredel et al. (2005)](../references.md#2) that was modified and studied by [Tibshirani and Wang (2008)](../references.md#6)\n",
    "\n",
    "The dataset here contains CGH measurements from 2 glioblastoma multiforme (GBM) brain tumors. Tibshirani and Wang (2008) proposed using the sparse fused lasso to approximate the CGH signal by a sparse, piecewise constant function in order to determine the areas with non-zero values, as positive (negative) CGH values correspond to possible gains (losses). The sparse fused lasso (Tibshirani et al., 2005) is given by\n",
    "\n",
    "```math\n",
    "\\begin{split}\n",
    "\\text{minimize} \\hspace{1em} \\frac 12 ||\\boldsymbol{y}-\\boldsymbol{\\beta}||_2^2 + \\rho_1||\\boldsymbol{\\beta}||_1 + \\rho_2\\sum_{j=2}^p |\\beta_j - \\beta_{j-1}| \\hspace{5em} (1)\n",
    "\\end{split}\n",
    "```\n",
    "The sparse fused lasso is a special case of the generalized lasso with the penalty matrix. Therefore, the problem ``(1)`` is equivalent to the following: \n",
    "\n",
    "```math \n",
    "\\begin{split} \n",
    "\\text{minimize} \\hspace{1em} \\frac 12 ||\\boldsymbol{y}-\\boldsymbol{X\\beta}||_2^2 + \\rho ||\\boldsymbol{D\\beta}||_1 \\hspace{5em} (2)\n",
    "\\end{split}\n",
    "```\n",
    "where \n",
    "\n",
    "```math\n",
    "\\boldsymbol{D} = \\begin{pmatrix} \n",
    "1 & -1 &     &    \t  &       & \t& \\\\\n",
    "  & 1  & -1  &    \t  &  \t\t&\t& \\\\\n",
    "  &    &  1  & -1 \t  & \t\t& \t& \\\\\n",
    "  &\t\t&\t\t& \\ddots & \\ddots &  & \\\\\n",
    "  &\t\t&\t\t&\t\t &\t\t\t& 1 & -1 \\\\\n",
    "1 &  &     &    \t  &       & \t& \\\\\n",
    "  & 1  &   &    \t  &  \t\t&\t& \\\\\n",
    "  &    &  \\ddots  &  \t  & \t\t& \t& \\\\\n",
    "  & \t&          & & \\ddots & & \\\\\n",
    "  &\t\t&\t\t     &      &       & 1 & \\\\\n",
    "  &\t\t&\t\t&\t\t &\t\t\t    &  & 1\\\\  \n",
    "\\end{pmatrix} \\in \\mathbb{R}^{(2P-1)\\times p}.\n",
    "\n",
    "```\n",
    "As discussed in [Gaines, B.R. and Zhou, H., (2016)](../references.md), the sparse fused lasso can be reformulated and solved as a constrained lasso problem. The generalized lasso problem ``(2)`` is equivalent to \n",
    "\n",
    "```math \n",
    "\\begin{split}\n",
    "& \\text{minimize} \\hspace{1em} \\frac 12 ||\\widetilde{\\boldsymbol{y}} -\\widetilde{\\boldsymbol{X}}\\boldsymbol{\\alpha}||_2^2 + \\rho||\\boldsymbol{\\alpha}||_1 \\hspace{5em} (3) \\\\\n",
    "& \\text{subject to} \\hspace{1em} \\boldsymbol{U}^T_2\\boldsymbol{\\alpha} = \\boldsymbol{0}\n",
    "\\end{split}\n",
    "```\n",
    "where $\\widetilde{\\boldsymbol{y}} = (\\boldsymbol{I}-\\boldsymbol{P}_{\\boldsymbol{XV}_2})\\boldsymbol{y},  \\hspace{0.5em} \\widetilde{\\boldsymbol{X}} = (\\boldsymbol{I}-\\boldsymbol{P}_{\\boldsymbol{XV}_2})\\boldsymbol{XD}^+$. Note $D^+$ is the Moore-Penrose inverse of the matrix $\\boldsymbol{D}.$ and $\\boldsymbol{U_2}, \\boldsymbol{V_2}$ are obtained from singular value decomposition (SVD) of ``\\boldsymbol{D}`` matrix. Then, the solution path ``\n",
    "\\widehat{\\boldsymbol{\\alpha}}(\\rho)`` can be translated back to that of the original generalized lasso problem via \n",
    "\n",
    "```math \n",
    "\\widehat{\\boldsymbol{\\beta}}(\\rho) = [\\boldsymbol{I}-\\boldsymbol{V}_2(\\boldsymbol{V}_2^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{V}_2)^-\\boldsymbol{V}_2^T\\boldsymbol{X}^T\\boldsymbol{X}]\\boldsymbol{D}^+\\hat{\\boldsymbol{\\alpha}}(\\rho)+ \\boldsymbol{V}_2(\\boldsymbol{V}_2^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{V}_2)^-\\boldsymbol{V}_2^T\\boldsymbol{X}^T\\boldsymbol{y}\n",
    "```\n",
    "where $\\boldsymbol{X}^-$ denotes the generalized inverse of a matrix $\\boldsymbol{X}$.\n",
    "\n",
    "Details are found in Section 2 of [[3](../references.md)]. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using ConstrainedLasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load and organize the data first. Here, `y` is the response vector. The design matrix `X` is an identity matrix since the objective function in ``(1)`` does not involve `X`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = readdlm(\"misc/tumor.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = p = size(y, 1)\n",
    "X = eye(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a penalty matrix `D`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D = [eye(p-1) zeros(p-1, 1)] - [zeros(p-1, 1) eye(p-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transform the problem to the constrained lasso problem. We do the singular value decomposition on `D` and extract singular values and necessary submatrices.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = size(D, 1)\n",
    "F = svdfact!(D, thin = false)\n",
    "singvals = F[:S]\n",
    "rankD = countnz(F[:S] .> abs(F[:S][1]) * eps(F[:S][1]) * maximum(size(D)))\n",
    "\n",
    "V1 = F[:V][:, 1:rankD]\n",
    "V2 = F[:V][:, rankD+1:end]\n",
    "U1 = F[:U][:, 1:rankD]\n",
    "U2 = F[:U][:, rankD+1:end];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the Moore-Penrose inverse of `D`, which is ``D^+`` in ``(3)``, and transform the design matrix by multiplying by ``D^+``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Dplus = V1 * broadcast(*, U1', 1./F[:S])\n",
    "XDplus = X * Dplus;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code snippet, `Pxv2` is a projection matrix onto `C(XV2)` and `Mxv2` is the orthogonal projection matrix. Then we obtain the design matrix and response vector in their tilde form as shown in ``(3)``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "XV2 = X * V2\n",
    "Pxv2 = (1 / dot(XV2, XV2)) * A_mul_Bt(XV2, XV2)\n",
    "Mxv2 = eye(size(XV2, 1)) - Pxv2\n",
    "ỹ = vec(Mxv2 * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X̃ = Mxv2 * XDplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We solve the constrained lasso problem and obtain $\\widehat{\\boldsymbol{\\alpha}}(\\rho)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "α̂path, ρpath, = lsq_classopath(X̃, ỹ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to transform $\\widehat{\\boldsymbol{\\alpha}} (\\rho)$ back to $\\widehat{\\boldsymbol{\\beta}} (\\rho)$ as seen in (3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform back to beta\n",
    "β̂path = Base.LinAlg.BLAS.ger!(1.0, vec(V2 * ((1 / dot(XV2, XV2)) * \n",
    "\t\tAt_mul_B(XV2, y))), ones(size(ρpath)), (eye(size(V2, 1)) - \n",
    "\t\tV2 * ((1 / dot(XV2, XV2)) * At_mul_B(XV2, X))) * Dplus * α̂path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the constrained lasso solution path below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Plots; pyplot(); # hide\n",
    "plot(ρpath, β̂path', label=\"\", xaxis = (\"ρ\", (minimum(ρpath),\n",
    "      maximum(ρpath))), yaxis = (\"β̂(ρ)\"), width=0.5)\n",
    "title!(\"Brain Tumor Data: Solution Path via Constrained Lasso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "savefig(\"misc/tumor1.svg\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tumor1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare our estimates with those from generalized lasso.  \n",
    "\n",
    "Variables `lambda_path` and `beta_path_fused` are lambda values and estimated beta coefficients, respectively, obtained from `genlasso` package in `R`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambda_path = readdlm(\"misc/lambda_path.txt\")\n",
    "beta_path_fused = readdlm(\"misc/beta_path_fused.txt\")[2:end, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure plots generalized lasso solution path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(lambda_path, beta_path_fused', label=\"\", xaxis = (\"λ\", (minimum(lambda_path),\n",
    "      maximum(lambda_path))), yaxis = (\"β̂(λ)\"), width=0.5)\n",
    "title!(\"Brain Tumor Data: Generalized Lasso Solution Path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "savefig(\"misc/tumor2.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract common values of $\\rho$ and compare estimates at those values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sameρ = intersect(round.(ρpath, 4), round.(lambda_path, 4))\n",
    "sameρ_err = []\n",
    "for i in eachindex(sameρ)\n",
    " curρ = sameρ[i]\n",
    " idx1 = findmin(abs.(ρpath - curρ))[2]\n",
    " idx2 = findmin(abs.(lambda_path - curρ))[2]\n",
    " push!(sameρ_err, maximum(abs.(β̂path[:, idx1] - beta_path_fused[:, idx2])))\n",
    "end\n",
    "sameρ_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the mean, median, and maximum of the errors between estimated coefficients at common ``\\rho`` values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "println([mean(sameρ_err); median(sameρ_err); maximum(sameρ_err)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
