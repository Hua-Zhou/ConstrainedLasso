{
    "docs": [
        {
            "location": "/", 
            "text": "ConstrainedLasso\n\n\nConstrainedLasso\n estimates the following constrained lasso problem\n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\boldsymbol{A\\beta}=\\boldsymbol{b} \\text{ and } \\boldsymbol{C\\beta} \\leq \\boldsymbol{d}\n\\end{split}\n\n\n\n\n\nwhere $\\boldsymbol{y} \\in \\mathbb{R}^n$ is the response vector, $\\boldsymbol{X}\\in \\mathbb{R}^{n\\times p}$ is the design matrix of predictor or covariates, $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ is the vector of unknown regression coefficients, and $\\rho \\geq 0$ is a tuning parameter that controls the amount of regularization.\n\n\n\n\nInstallation\n\n\nWithin Julia, use the package manager to install \nConstainedLasso\n:\n\n\nPkg\n.\nclone\n(\nhttps://github.com/Hua-Zhou/ConstrainedLasso.git\n)\n\n\n\n\n\n\nThis package supports Julia v0.6.\n\n\n\n\nCitation\n\n\nIf you use ConstrainedLasso package in your research, please cite the following paper on the algorithms:\n\n\nB.R. Gaines, H. Zhou. (2016) Algorithms for Fitting the Constrained Lasso. \nhttps://arxiv.org/abs/1611.01511\n\n\nOriginal method paper on the constrained lasso is\n\n\nG.M. James, C. Paulson and P. Rusmevichientong. (2013) Penalized and constrained regression. \nhttp://www-bcf.usc.edu/~gareth/research/PAC.pdf", 
            "title": "Home"
        }, 
        {
            "location": "/#constrainedlasso", 
            "text": "ConstrainedLasso  estimates the following constrained lasso problem   \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\boldsymbol{A\\beta}=\\boldsymbol{b} \\text{ and } \\boldsymbol{C\\beta} \\leq \\boldsymbol{d}\n\\end{split}   where $\\boldsymbol{y} \\in \\mathbb{R}^n$ is the response vector, $\\boldsymbol{X}\\in \\mathbb{R}^{n\\times p}$ is the design matrix of predictor or covariates, $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ is the vector of unknown regression coefficients, and $\\rho \\geq 0$ is a tuning parameter that controls the amount of regularization.", 
            "title": "ConstrainedLasso"
        }, 
        {
            "location": "/#installation", 
            "text": "Within Julia, use the package manager to install  ConstainedLasso :  Pkg . clone ( https://github.com/Hua-Zhou/ConstrainedLasso.git )   This package supports Julia v0.6.", 
            "title": "Installation"
        }, 
        {
            "location": "/#citation", 
            "text": "If you use ConstrainedLasso package in your research, please cite the following paper on the algorithms:  B.R. Gaines, H. Zhou. (2016) Algorithms for Fitting the Constrained Lasso.  https://arxiv.org/abs/1611.01511  Original method paper on the constrained lasso is  G.M. James, C. Paulson and P. Rusmevichientong. (2013) Penalized and constrained regression.  http://www-bcf.usc.edu/~gareth/research/PAC.pdf", 
            "title": "Citation"
        }, 
        {
            "location": "/interface/", 
            "text": "Interface\n\n\nFunctions exported from \nConstrainedLasso\n:\n\n\n#\n\n\nConstrainedLasso.lsq_constrsparsereg\n \n \nFunction\n.\n\n\n  lsq_constrsparsereg(X, y, \u03c1 = zero(eltype(X));\n      Aeq       :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq       :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq     :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq     :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      obswt     :: AbstractVector = ones(eltype(y), length(y)),\n      penwt     :: AbstractVector = ones(eltype(X), size(X, 2)),\n      warmstart :: Bool = false,\n      solver = SCSSolver(verbose=0, max_iters=10e8)\n      )\n\n\n\n\n\nFit constrained lasso at fixed tuning parameter value(s) by minimizing     \n0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(penwt .* \u03b2)\n subject to linear constraints.\n\n\nArguments\n\n\n\n\nX\n       : predictor matrix.\n\n\ny\n       : response vector.\n\n\n\u03c1\n       : tuning parameter. Can be a number or a list of numbers. Default 0.\n\n\n\n\nOptional arguments\n\n\n\n\nAeq\n     : equality constraint matrix.\n\n\nbeq\n     : equality constraint vector.\n\n\nAineq\n   : inequality constraint matrix.\n\n\nbineq\n   : inequality constraint vector.\n\n\nobswt\n   : observation weights.\n\n\npenwt\n   : predictor penalty weights. Default is \n[1 1 1 ... 1]\n.\n\n\nsolver\n  : a solver Convex.jl supports. Default is SCS. \nhttp://convexjl.readthedocs.io/en/latest/solvers.html\n\n\n\u03b20\n      : starting point for warm start.\n\n\n\n\nReturns\n\n\n\n\n\u03b2\n       : estimated coefficents.\n\n\nobjval\n  : optimal objective value.\n\n\nproblem\n : Convex.jl problem.\n\n\n\n\nsource\n\n\n#\n\n\nConstrainedLasso.lsq_classopath\n \n \nFunction\n.\n\n\n  lsq_classopath(X, y;\n      Aeq    :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq    :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq  :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq  :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      \u03c1ridge :: Number = zero(eltype(X)),\n      penidx ::Array{Bool} = fill(true, size(X, 2)),\n      solver = SCSSolver(verbose=0, max_iters=10e8)\n      )\n\nCalculate the solution path of the constrained lasso problem that minimizes\n    `0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(penwt .* \u03b2)`\nsubject to linear constraints.\n\n\n\n\n\nArguments\n\n\n\n\nX\n       : predictor matrix\n\n\ny\n       : response vector\n\n\n\n\nOptional arguments\n\n\n\n\nAeq\n     : equality constraint matrix\n\n\nbeq\n     : equality constraint vector\n\n\nAineq\n   : inequality constraint matrix\n\n\nbineq\n   : inequality constraint vector\n\n\n\u03c1ridge\n  : tuning parameter for ridge penalty. Default is 0.\n\n\npenidx\n  : a logical vector indicating penalized coefficients\n\n\nsolver\n  : a solver Convex.jl supports. Default is SCS. \nhttp://convexjl.readthedocs.io/en/latest/solvers.html\n\n\n\n\nExamples\n\n\nSee tutorial examples at https://github.com/Hua-Zhou/ConstrainedLasso\n\n\nsource\n\n\nPrivate function in \nConstrainedLasso\n:\n\n\n#\n\n\nConstrainedLasso.find_\u03c1max\n \n \nFunction\n.\n\n\n  find_\u03c1max(X, y;\n      Aeq    :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq    :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq  :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq  :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      penidx :: Array{Bool} = fill(true, size(X, 2)),\n      solver = SCSSolver(verbose=0, max_iters=10e8)\n\n\n\n\n\nFind the maximum tuning parameter value \n\u03c1max\n to kick-start the solution path.\n\n\nsource", 
            "title": "Interface"
        }, 
        {
            "location": "/interface/#interface", 
            "text": "Functions exported from  ConstrainedLasso :  #  ConstrainedLasso.lsq_constrsparsereg     Function .    lsq_constrsparsereg(X, y, \u03c1 = zero(eltype(X));\n      Aeq       :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq       :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq     :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq     :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      obswt     :: AbstractVector = ones(eltype(y), length(y)),\n      penwt     :: AbstractVector = ones(eltype(X), size(X, 2)),\n      warmstart :: Bool = false,\n      solver = SCSSolver(verbose=0, max_iters=10e8)\n      )  Fit constrained lasso at fixed tuning parameter value(s) by minimizing      0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(penwt .* \u03b2)  subject to linear constraints.  Arguments   X        : predictor matrix.  y        : response vector.  \u03c1        : tuning parameter. Can be a number or a list of numbers. Default 0.   Optional arguments   Aeq      : equality constraint matrix.  beq      : equality constraint vector.  Aineq    : inequality constraint matrix.  bineq    : inequality constraint vector.  obswt    : observation weights.  penwt    : predictor penalty weights. Default is  [1 1 1 ... 1] .  solver   : a solver Convex.jl supports. Default is SCS.  http://convexjl.readthedocs.io/en/latest/solvers.html  \u03b20       : starting point for warm start.   Returns   \u03b2        : estimated coefficents.  objval   : optimal objective value.  problem  : Convex.jl problem.   source  #  ConstrainedLasso.lsq_classopath     Function .    lsq_classopath(X, y;\n      Aeq    :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq    :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq  :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq  :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      \u03c1ridge :: Number = zero(eltype(X)),\n      penidx ::Array{Bool} = fill(true, size(X, 2)),\n      solver = SCSSolver(verbose=0, max_iters=10e8)\n      )\n\nCalculate the solution path of the constrained lasso problem that minimizes\n    `0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(penwt .* \u03b2)`\nsubject to linear constraints.  Arguments   X        : predictor matrix  y        : response vector   Optional arguments   Aeq      : equality constraint matrix  beq      : equality constraint vector  Aineq    : inequality constraint matrix  bineq    : inequality constraint vector  \u03c1ridge   : tuning parameter for ridge penalty. Default is 0.  penidx   : a logical vector indicating penalized coefficients  solver   : a solver Convex.jl supports. Default is SCS.  http://convexjl.readthedocs.io/en/latest/solvers.html   Examples  See tutorial examples at https://github.com/Hua-Zhou/ConstrainedLasso  source  Private function in  ConstrainedLasso :  #  ConstrainedLasso.find_\u03c1max     Function .    find_\u03c1max(X, y;\n      Aeq    :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq    :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq  :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq  :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      penidx :: Array{Bool} = fill(true, size(X, 2)),\n      solver = SCSSolver(verbose=0, max_iters=10e8)  Find the maximum tuning parameter value  \u03c1max  to kick-start the solution path.  source", 
            "title": "Interface"
        }, 
        {
            "location": "/demo/fixedparam/", 
            "text": "Optimize at fixed tuning parameter value(s)\n\n\nlsq_constrsparsereg.jl\n fits constrained lasso\n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\boldsymbol{A\\beta}=\\boldsymbol{b} \\text{ and } \\boldsymbol{C\\beta} \\leq \\boldsymbol{d}\n\\end{split}\n\n\n\n\n\nat a fixed tuning parameter value $\\rho$ or several tuning parameter values provided by user.\n\n\n\n\nSingle tuning parameter value\n\n\nWe demonstrate using a sum-to-zero constraint example\n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\sum_j \\beta_j = 0\n\\end{split}\n\n\n\n\n\nFirst, let's define a true parameter \n\u03b2\n such that \nsum(\u03b2) = 0\n.\n\n\nusing\n \nConstrainedLasso\n,\n \nBase\n.\nTest\n\n\n\nn\n,\n \np\n \n=\n \n100\n,\n \n20\n\n\n\u03b2\n \n=\n \nzeros\n(\np\n)\n\n\n\u03b2\n[\n1\n:\nround\n(\nInt\n,\n \np\n \n/\n \n4\n)]\n \n=\n \n0\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \np\n \n/\n \n4\n)\n \n+\n \n1\n)\n:\nround\n(\nInt\n,\n \np\n \n/\n \n2\n)]\n \n=\n \n1\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \np\n \n/\n \n2\n)\n \n+\n \n1\n)\n:\nround\n(\nInt\n,\n \n3\np\n \n/\n \n4\n)]\n \n=\n \n0\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \n3\np\n \n/\n \n4\n)\n \n+\n \n1\n)\n:\np\n]\n \n=\n \n-\n1\n\n\n\u03b2\n\n\n\n\n\n\n20-element Array{Float64,1}:\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  1.0\n  1.0\n  1.0\n  1.0\n  1.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n\n\n\n\n\nNext we generate data based on the true parameter \n\u03b2\n.\n\n\nsrand\n(\n123\n)\n\n\nX\n \n=\n \nrandn\n(\nn\n,\n \np\n)\n\n\n\n\n\n\n100\u00d720 Array{Float64,2}:\n  1.19027     0.376264    0.346589    0.458099   \u2026   0.0523088   2.23365    \n  2.04818    -0.405272    1.60431     0.139124      -0.168468    1.29252    \n  1.14265     1.33585    -0.0246589  -0.230745      -0.247202   -0.822482   \n  0.459416    1.60076    -0.106035    1.35195       -1.66701     0.0134896  \n -0.396679   -1.45789    -1.29118    -0.106316      -1.24891     0.466877   \n -0.664713    0.800589   -0.337985   -0.205883   \u2026  -0.623667    1.32134    \n  0.980968    0.895878   -0.177092   -0.612003       0.372048    0.581506   \n -0.0754831  -0.691934    0.57499    -1.39397       -0.633969    1.35917    \n  0.273815   -1.50876    -1.37834     1.73135       -0.74986     1.27668    \n -0.194229   -0.754523   -0.867869    2.61556       -2.07352     1.59349    \n -0.339366    0.115622   -0.400076    1.76909    \u2026   0.496242    0.806838   \n -0.843878    0.242595    0.295087    0.240332       0.5764      1.55243    \n -0.888936   -0.223211    0.817696   -1.42384       -0.859119    1.49178    \n  \u22ee                                              \u22f1                          \n -0.733961    0.911747   -0.618047   -0.319891       1.44548    -1.1144     \n  0.459398    0.0138789  -1.08527    -0.529198       0.395225    0.822061   \n  1.70619     2.2959      0.4024      1.47241    \u2026  -0.260034   -0.746822   \n  0.678443    0.934982    0.425372    1.17431        0.780863    0.439673   \n  0.28718     2.00606    -1.18929     1.35692       -0.545467   -0.40497    \n  1.06816    -0.379291    0.11631     2.48089       -1.04331     1.24328    \n -0.306877    0.20646    -1.34497    -0.584326      -1.8609     -0.383338   \n -1.92021    -0.276028    0.426339    0.38792    \u2026   2.16327    -1.02578    \n  1.6696      1.19586    -0.783625    0.718697       1.13162    -1.31358    \n -0.213558   -1.2965      0.648433   -0.289336       0.263283    0.000636658\n -0.163711    0.575279   -0.176555   -0.0457259      0.152164    0.1559     \n -0.902986   -0.166001   -1.27924    -1.31238        0.49458    -0.171711\n\n\n\n\n\ny\n \n=\n \nX\n \n*\n \n\u03b2\n \n+\n \nrandn\n(\nn\n)\n\n\n\n\n\n\n100-element Array{Float64,1}:\n -6.50888  \n -3.00423  \n -2.3809   \n  2.50638  \n -2.24753  \n -2.51881  \n  3.60092  \n -1.38597  \n  0.0562454\n  0.787719 \n  3.17731  \n -2.1989   \n -1.86609  \n  \u22ee        \n  0.87746  \n -3.68264  \n -3.19285  \n -0.961258 \n  0.793834 \n  0.140524 \n  1.71841  \n -6.31781  \n -1.95472  \n  1.4803   \n -2.15804  \n -6.66724\n\n\n\n\n\nSince the equality constraint can be written as\n\n\n\n\n\n\\begin{split}\n\\begin{pmatrix} 1 & 1 & \\cdots & 1 \\end{pmatrix} \\beta = 0,\n\\end{split}\n\n\n\n\n\nwe define the constraint as below.\n\n\nbeq\n   \n=\n \n[\n0.0\n]\n\n\nAeq\n   \n=\n \nones\n(\n1\n,\n \np\n)\n\n\n\n\n\n\n1\u00d720 Array{Float64,2}:\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \u2026  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n\n\n\n\n\nNow we are ready to fit the constrained lasso problem, say at \n\u03c1=10\n.\n\n\n\u03c1\n \n=\n \n10.0\n\n\n\u03b2\u0302\n,\n \n=\n \nlsq_constrsparsereg\n(\nX\n,\n \ny\n,\n \n\u03c1\n;\n \nAeq\n \n=\n \nAeq\n,\n \nbeq\n \n=\n \nbeq\n);\n\n\n\n\n\n\n\u03b2\u0302\n\n\n\n\n\n\n20\u00d71 Array{Float64,2}:\n  1.06543e-7 \n  1.16499e-7 \n  0.184567   \n -1.07162e-8 \n  6.88319e-8 \n  0.891789   \n  0.867016   \n  0.89869    \n  0.830042   \n  0.740962   \n  3.21933e-8 \n -7.74822e-8 \n -6.04844e-12\n -6.67961e-8 \n  0.0155802  \n -0.918892   \n -0.927699   \n -0.874741   \n -1.00308    \n -0.704239\n\n\n\n\n\nWe see if the sum of estimated $\\beta$ coefficients equal to 0.\n\n\n@test\n \nsum\n(\n\u03b2\u0302\n)\n\u22480\n.\n0\n \natol\n=\n1e-5\n\n\n\n\n\n\n\u001bTest Passed\n\n\n\n\n\n\n\nMultiple tuning parameter values\n\n\nDefine \n\u03c1list\n to be a sequence of values from 1 to 10.\n\n\n\u03c1list\n \n=\n \n1.0\n:\n10.0\n\n\n\n\n\n\n1.0:1.0:10.0\n\n\n\n\n\nUsing the same equality constraints, we fit the constrained lasso.\n\n\n\u03b2\u0302\n,\n \n=\n \nlsq_constrsparsereg\n(\nX\n,\n \ny\n,\n \n\u03c1list\n;\n \nAeq\n \n=\n \nAeq\n,\n \nbeq\n \n=\n \nbeq\n);\n\n\n\n\n\n\n\u03b2\u0302\n\n\n\n\n\n\n20\u00d710 Array{Float64,2}:\n  0.0654744    0.0528563    0.0435276   \u2026   1.36207e-7   1.06543e-7 \n  0.0984093    0.0769516    0.0557896       1.55064e-7   1.16499e-7 \n  0.256568     0.246248     0.236865        0.19077      0.184567   \n -0.0161896    1.24814e-6   3.09904e-7     -1.14652e-8  -1.07162e-8 \n  0.0442985    0.0355132    0.0266877       9.93987e-8   6.88319e-8 \n  0.978801     0.965257     0.953956    \u2026   0.900293     0.891789   \n  0.962883     0.952699     0.943744        0.880242     0.867016   \n  0.910347     0.907963     0.907171        0.900036     0.89869    \n  0.968082     0.947724     0.931277        0.845411     0.830042   \n  0.936793     0.911403     0.887538        0.76097      0.740962   \n -9.39273e-8  -2.28297e-7  -3.90716e-8  \u2026   4.38155e-8   3.21933e-8 \n -0.0819607   -0.0711052   -0.0585947      -0.00364202  -7.74822e-8 \n -3.72613e-8   2.38111e-7   8.712e-8        6.32877e-9  -6.04844e-12\n -0.0609711   -0.0433998   -0.0252136      -8.37749e-8  -6.67961e-8 \n  0.0730144    0.0671585    0.0608885       0.0235996    0.0155802  \n -1.04989     -1.03264     -1.01746     \u2026  -0.931341    -0.918892   \n -1.00933     -0.998772    -0.98755        -0.934456    -0.927699   \n -1.04134     -1.01861     -0.996088       -0.889298    -0.874741   \n -1.14266     -1.12595     -1.11046        -1.01705     -1.00308    \n -0.89232     -0.873294    -0.85207        -0.725531    -0.704239\n\n\n\n\n\nNow let's test if coefficients sum to 0 at each parameter value.\n\n\n@testset\n \nzero-sum for multiple param values\n \nbegin\n \nfor\n \ni\n \nin\n \nsum\n(\n\u03b2\u0302\n,\n \n1\n)\n\n  \n@test\n \ni\u22480\n.\n0\n \natol\n=\n1.0e-5\n\n\nend\n\n\nend\n\n\n\n\n\n\n\u001bTest Summary:                      | \u001bPass  \u001bTotal\u001b\nzero-sum for multiple param values |   \u001b10  \u001b   10\u001b", 
            "title": "Fixed tuning param value"
        }, 
        {
            "location": "/demo/fixedparam/#optimize-at-fixed-tuning-parameter-values", 
            "text": "lsq_constrsparsereg.jl  fits constrained lasso   \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\boldsymbol{A\\beta}=\\boldsymbol{b} \\text{ and } \\boldsymbol{C\\beta} \\leq \\boldsymbol{d}\n\\end{split}   at a fixed tuning parameter value $\\rho$ or several tuning parameter values provided by user.", 
            "title": "Optimize at fixed tuning parameter value(s)"
        }, 
        {
            "location": "/demo/fixedparam/#single-tuning-parameter-value", 
            "text": "We demonstrate using a sum-to-zero constraint example   \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\sum_j \\beta_j = 0\n\\end{split}   First, let's define a true parameter  \u03b2  such that  sum(\u03b2) = 0 .  using   ConstrainedLasso ,   Base . Test  n ,   p   =   100 ,   20  \u03b2   =   zeros ( p )  \u03b2 [ 1 : round ( Int ,   p   /   4 )]   =   0  \u03b2 [( round ( Int ,   p   /   4 )   +   1 ) : round ( Int ,   p   /   2 )]   =   1  \u03b2 [( round ( Int ,   p   /   2 )   +   1 ) : round ( Int ,   3 p   /   4 )]   =   0  \u03b2 [( round ( Int ,   3 p   /   4 )   +   1 ) : p ]   =   - 1  \u03b2   20-element Array{Float64,1}:\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  1.0\n  1.0\n  1.0\n  1.0\n  1.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0  Next we generate data based on the true parameter  \u03b2 .  srand ( 123 )  X   =   randn ( n ,   p )   100\u00d720 Array{Float64,2}:\n  1.19027     0.376264    0.346589    0.458099   \u2026   0.0523088   2.23365    \n  2.04818    -0.405272    1.60431     0.139124      -0.168468    1.29252    \n  1.14265     1.33585    -0.0246589  -0.230745      -0.247202   -0.822482   \n  0.459416    1.60076    -0.106035    1.35195       -1.66701     0.0134896  \n -0.396679   -1.45789    -1.29118    -0.106316      -1.24891     0.466877   \n -0.664713    0.800589   -0.337985   -0.205883   \u2026  -0.623667    1.32134    \n  0.980968    0.895878   -0.177092   -0.612003       0.372048    0.581506   \n -0.0754831  -0.691934    0.57499    -1.39397       -0.633969    1.35917    \n  0.273815   -1.50876    -1.37834     1.73135       -0.74986     1.27668    \n -0.194229   -0.754523   -0.867869    2.61556       -2.07352     1.59349    \n -0.339366    0.115622   -0.400076    1.76909    \u2026   0.496242    0.806838   \n -0.843878    0.242595    0.295087    0.240332       0.5764      1.55243    \n -0.888936   -0.223211    0.817696   -1.42384       -0.859119    1.49178    \n  \u22ee                                              \u22f1                          \n -0.733961    0.911747   -0.618047   -0.319891       1.44548    -1.1144     \n  0.459398    0.0138789  -1.08527    -0.529198       0.395225    0.822061   \n  1.70619     2.2959      0.4024      1.47241    \u2026  -0.260034   -0.746822   \n  0.678443    0.934982    0.425372    1.17431        0.780863    0.439673   \n  0.28718     2.00606    -1.18929     1.35692       -0.545467   -0.40497    \n  1.06816    -0.379291    0.11631     2.48089       -1.04331     1.24328    \n -0.306877    0.20646    -1.34497    -0.584326      -1.8609     -0.383338   \n -1.92021    -0.276028    0.426339    0.38792    \u2026   2.16327    -1.02578    \n  1.6696      1.19586    -0.783625    0.718697       1.13162    -1.31358    \n -0.213558   -1.2965      0.648433   -0.289336       0.263283    0.000636658\n -0.163711    0.575279   -0.176555   -0.0457259      0.152164    0.1559     \n -0.902986   -0.166001   -1.27924    -1.31238        0.49458    -0.171711  y   =   X   *   \u03b2   +   randn ( n )   100-element Array{Float64,1}:\n -6.50888  \n -3.00423  \n -2.3809   \n  2.50638  \n -2.24753  \n -2.51881  \n  3.60092  \n -1.38597  \n  0.0562454\n  0.787719 \n  3.17731  \n -2.1989   \n -1.86609  \n  \u22ee        \n  0.87746  \n -3.68264  \n -3.19285  \n -0.961258 \n  0.793834 \n  0.140524 \n  1.71841  \n -6.31781  \n -1.95472  \n  1.4803   \n -2.15804  \n -6.66724  Since the equality constraint can be written as   \n\\begin{split}\n\\begin{pmatrix} 1 & 1 & \\cdots & 1 \\end{pmatrix} \\beta = 0,\n\\end{split}   we define the constraint as below.  beq     =   [ 0.0 ]  Aeq     =   ones ( 1 ,   p )   1\u00d720 Array{Float64,2}:\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \u2026  1.0  1.0  1.0  1.0  1.0  1.0  1.0  Now we are ready to fit the constrained lasso problem, say at  \u03c1=10 .  \u03c1   =   10.0  \u03b2\u0302 ,   =   lsq_constrsparsereg ( X ,   y ,   \u03c1 ;   Aeq   =   Aeq ,   beq   =   beq );   \u03b2\u0302   20\u00d71 Array{Float64,2}:\n  1.06543e-7 \n  1.16499e-7 \n  0.184567   \n -1.07162e-8 \n  6.88319e-8 \n  0.891789   \n  0.867016   \n  0.89869    \n  0.830042   \n  0.740962   \n  3.21933e-8 \n -7.74822e-8 \n -6.04844e-12\n -6.67961e-8 \n  0.0155802  \n -0.918892   \n -0.927699   \n -0.874741   \n -1.00308    \n -0.704239  We see if the sum of estimated $\\beta$ coefficients equal to 0.  @test   sum ( \u03b2\u0302 ) \u22480 . 0   atol = 1e-5   \u001bTest Passed", 
            "title": "Single tuning parameter value"
        }, 
        {
            "location": "/demo/fixedparam/#multiple-tuning-parameter-values", 
            "text": "Define  \u03c1list  to be a sequence of values from 1 to 10.  \u03c1list   =   1.0 : 10.0   1.0:1.0:10.0  Using the same equality constraints, we fit the constrained lasso.  \u03b2\u0302 ,   =   lsq_constrsparsereg ( X ,   y ,   \u03c1list ;   Aeq   =   Aeq ,   beq   =   beq );   \u03b2\u0302   20\u00d710 Array{Float64,2}:\n  0.0654744    0.0528563    0.0435276   \u2026   1.36207e-7   1.06543e-7 \n  0.0984093    0.0769516    0.0557896       1.55064e-7   1.16499e-7 \n  0.256568     0.246248     0.236865        0.19077      0.184567   \n -0.0161896    1.24814e-6   3.09904e-7     -1.14652e-8  -1.07162e-8 \n  0.0442985    0.0355132    0.0266877       9.93987e-8   6.88319e-8 \n  0.978801     0.965257     0.953956    \u2026   0.900293     0.891789   \n  0.962883     0.952699     0.943744        0.880242     0.867016   \n  0.910347     0.907963     0.907171        0.900036     0.89869    \n  0.968082     0.947724     0.931277        0.845411     0.830042   \n  0.936793     0.911403     0.887538        0.76097      0.740962   \n -9.39273e-8  -2.28297e-7  -3.90716e-8  \u2026   4.38155e-8   3.21933e-8 \n -0.0819607   -0.0711052   -0.0585947      -0.00364202  -7.74822e-8 \n -3.72613e-8   2.38111e-7   8.712e-8        6.32877e-9  -6.04844e-12\n -0.0609711   -0.0433998   -0.0252136      -8.37749e-8  -6.67961e-8 \n  0.0730144    0.0671585    0.0608885       0.0235996    0.0155802  \n -1.04989     -1.03264     -1.01746     \u2026  -0.931341    -0.918892   \n -1.00933     -0.998772    -0.98755        -0.934456    -0.927699   \n -1.04134     -1.01861     -0.996088       -0.889298    -0.874741   \n -1.14266     -1.12595     -1.11046        -1.01705     -1.00308    \n -0.89232     -0.873294    -0.85207        -0.725531    -0.704239  Now let's test if coefficients sum to 0 at each parameter value.  @testset   zero-sum for multiple param values   begin   for   i   in   sum ( \u03b2\u0302 ,   1 ) \n   @test   i\u22480 . 0   atol = 1.0e-5  end  end   \u001bTest Summary:                      | \u001bPass  \u001bTotal\u001b\nzero-sum for multiple param values |   \u001b10  \u001b   10\u001b", 
            "title": "Multiple tuning parameter values"
        }, 
        {
            "location": "/references/", 
            "text": "References\n\n\n\n\n[1] Altenbuchinger, M., Rehberg, T., Zacharias, H.U., St\u00e4mmler, F., Dettmer, K., Weber, D., Hiergeist, A., Gessner, A., Holler, E., Oefner, P.J. and Spang, R. (2016). Reference point insensitive molecular data analysis. \nBioinformatics\n, 33(2):219-226.\n\n\n[2] Bredel, M., Bredel, C., Juric, D., Harsh, G. R., Vogel, H., Recht, L. D., and Sikic, B. I. (2005). High-resolution genome-wide mapping of genetic alterations in human glial brain tumors. \nCancer Research\n, 65:4088\u20134096.\n\n\n[3] Gaines, B.R. and Zhou, H. (2016). Algorithms for fitting the constrained lasso. arXiv preprint arXiv:1611.01511.\n\n\n[4] Stamey, T.A., Kabalin, J.N., McNeal, J.E., Johnstone, I.M., Freiha, F., Redwine, E.A. and Yang, N. (1989). Prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate: II. radical prostatectomy treated patients. \nJournal of Urology\n 141(5):1076\u20131083.\n\n\n[5] Tibshirani, R. J., Hoefling, H., and Tibshirani, R. (2011). Nearly-isotonic regression. \nTechnometrics\n, 53:54\u201361.\n\n\n[6] Tibshirani, R. and Wang, P. (2008). Spatial smoothing and hot spot detection for CGH data using the fused lasso. \nBiostatistics\n, 9:18\u201329.\n\n\n[7] Weber, D., Oefner, P. J., Hiergeist, A., Koestler, J., Gessner, A., Weber, M., Hahn, J., Wolff, D., Sta \u0308mmler, F., Spang, R., Herr, W., Dettmer, K., and Holler, E. (2015). Low urinary undoxyl sulfate levels early after transplantation reflect a disrupted microbiome and are associated with poor outcome. \nBlood\n, 126:1723\u20131728.\n\n\n[8] Wu, W. B., Woodroofe, M., and Mentz, G. (2001). Isotonic regression: another look at the changepoint problem. \nBiometrika\n, 88:793\u2013804.", 
            "title": "References"
        }, 
        {
            "location": "/references/#references", 
            "text": "[1] Altenbuchinger, M., Rehberg, T., Zacharias, H.U., St\u00e4mmler, F., Dettmer, K., Weber, D., Hiergeist, A., Gessner, A., Holler, E., Oefner, P.J. and Spang, R. (2016). Reference point insensitive molecular data analysis.  Bioinformatics , 33(2):219-226.  [2] Bredel, M., Bredel, C., Juric, D., Harsh, G. R., Vogel, H., Recht, L. D., and Sikic, B. I. (2005). High-resolution genome-wide mapping of genetic alterations in human glial brain tumors.  Cancer Research , 65:4088\u20134096.  [3] Gaines, B.R. and Zhou, H. (2016). Algorithms for fitting the constrained lasso. arXiv preprint arXiv:1611.01511.  [4] Stamey, T.A., Kabalin, J.N., McNeal, J.E., Johnstone, I.M., Freiha, F., Redwine, E.A. and Yang, N. (1989). Prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate: II. radical prostatectomy treated patients.  Journal of Urology  141(5):1076\u20131083.  [5] Tibshirani, R. J., Hoefling, H., and Tibshirani, R. (2011). Nearly-isotonic regression.  Technometrics , 53:54\u201361.  [6] Tibshirani, R. and Wang, P. (2008). Spatial smoothing and hot spot detection for CGH data using the fused lasso.  Biostatistics , 9:18\u201329.  [7] Weber, D., Oefner, P. J., Hiergeist, A., Koestler, J., Gessner, A., Weber, M., Hahn, J., Wolff, D., Sta \u0308mmler, F., Spang, R., Herr, W., Dettmer, K., and Holler, E. (2015). Low urinary undoxyl sulfate levels early after transplantation reflect a disrupted microbiome and are associated with poor outcome.  Blood , 126:1723\u20131728.  [8] Wu, W. B., Woodroofe, M., and Mentz, G. (2001). Isotonic regression: another look at the changepoint problem.  Biometrika , 88:793\u2013804.", 
            "title": "References"
        }
    ]
}