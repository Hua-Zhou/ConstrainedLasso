{
    "docs": [
        {
            "location": "/", 
            "text": "ConstrainedLasso\n\n\nConstrainedLasso\n estimates the following constrained lasso problem\n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\boldsymbol{A\\beta}=\\boldsymbol{b} \\text{ and } \\boldsymbol{C\\beta} \\leq \\boldsymbol{d}\n\\end{split}\n\n\n\n\n\nwhere $\\boldsymbol{y} \\in \\mathbb{R}^n$ is the response vector, $\\boldsymbol{X}\\in \\mathbb{R}^{n\\times p}$ is the design matrix of predictor or covariates, $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ is the vector of unknown regression coefficients, and $\\rho \\geq 0$ is a tuning parameter that controls the amount of regularization.\n\n\n\n\nInstallation\n\n\nWithin Julia, use the package manager to install \nConstainedLasso\n:\n\n\nPkg\n.\nclone\n(\nhttps://github.com/Hua-Zhou/ConstrainedLasso.git\n)\n\n\n\n\n\n\nThis package supports Julia v0.6.\n\n\n\n\nCitation\n\n\nIf you use ConstrainedLasso package in your research, please cite the following paper on the algorithms:\n\n\nB.R. Gaines, H. Zhou. (2016) Algorithms for Fitting the Constrained Lasso. \nhttps://arxiv.org/abs/1611.01511\n\n\nOriginal method paper on the constrained lasso is\n\n\nG.M. James, C. Paulson and P. Rusmevichientong. (2013) Penalized and constrained regression. \nhttp://www-bcf.usc.edu/~gareth/research/PAC.pdf", 
            "title": "Home"
        }, 
        {
            "location": "/#constrainedlasso", 
            "text": "ConstrainedLasso  estimates the following constrained lasso problem   \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\boldsymbol{A\\beta}=\\boldsymbol{b} \\text{ and } \\boldsymbol{C\\beta} \\leq \\boldsymbol{d}\n\\end{split}   where $\\boldsymbol{y} \\in \\mathbb{R}^n$ is the response vector, $\\boldsymbol{X}\\in \\mathbb{R}^{n\\times p}$ is the design matrix of predictor or covariates, $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ is the vector of unknown regression coefficients, and $\\rho \\geq 0$ is a tuning parameter that controls the amount of regularization.", 
            "title": "ConstrainedLasso"
        }, 
        {
            "location": "/#installation", 
            "text": "Within Julia, use the package manager to install  ConstainedLasso :  Pkg . clone ( https://github.com/Hua-Zhou/ConstrainedLasso.git )   This package supports Julia v0.6.", 
            "title": "Installation"
        }, 
        {
            "location": "/#citation", 
            "text": "If you use ConstrainedLasso package in your research, please cite the following paper on the algorithms:  B.R. Gaines, H. Zhou. (2016) Algorithms for Fitting the Constrained Lasso.  https://arxiv.org/abs/1611.01511  Original method paper on the constrained lasso is  G.M. James, C. Paulson and P. Rusmevichientong. (2013) Penalized and constrained regression.  http://www-bcf.usc.edu/~gareth/research/PAC.pdf", 
            "title": "Citation"
        }, 
        {
            "location": "/interface/", 
            "text": "Interface\n\n\nFunctions exported from \nConstrainedLasso\n:\n\n\n#\n\n\nConstrainedLasso.lsq_constrsparsereg\n \n \nFunction\n.\n\n\n  lsq_constrsparsereg(X, y, \u03c1 = zero(eltype(X));\n      Aeq       :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq       :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq     :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq     :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      obswt     :: AbstractVector = ones(eltype(y), length(y)),\n      penwt     :: AbstractVector = ones(eltype(X), size(X, 2)),\n      warmstart :: Bool = false,\n      solver = SCSSolver(verbose=0, max_iters=10e8)\n      )\n\n\n\n\n\nFit constrained lasso at fixed tuning parameter value(s) by minimizing     \n0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(penwt .* \u03b2)\n subject to linear constraints.\n\n\nArguments\n\n\n\n\nX\n       : predictor matrix.\n\n\ny\n       : response vector.\n\n\n\u03c1\n       : tuning parameter. Can be a number or a list of numbers. Default 0.\n\n\n\n\nOptional arguments\n\n\n\n\nAeq\n     : equality constraint matrix.\n\n\nbeq\n     : equality constraint vector.\n\n\nAineq\n   : inequality constraint matrix.\n\n\nbineq\n   : inequality constraint vector.\n\n\nobswt\n   : observation weights.\n\n\npenwt\n   : predictor penalty weights. Default is \n[1 1 1 ... 1]\n.\n\n\nsolver\n  : a solver Convex.jl supports. Default is SCS. \nhttp://convexjl.readthedocs.io/en/latest/solvers.html\n\n\n\u03b20\n      : starting point for warm start.\n\n\n\n\nReturns\n\n\n\n\n\u03b2\n       : estimated coefficents.\n\n\nobjval\n  : optimal objective value.\n\n\nproblem\n : Convex.jl problem.\n\n\n\n\nsource\n\n\n#\n\n\nConstrainedLasso.lsq_classopath\n \n \nFunction\n.\n\n\n  lsq_classopath(X, y;\n      Aeq    :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq    :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq  :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq  :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      \u03c1ridge :: Number = zero(eltype(X)),\n      penidx ::Array{Bool} = fill(true, size(X, 2)),\n      solver = SCSSolver(verbose=0, max_iters=10e8)\n      )\n\nCalculate the solution path of the constrained lasso problem that minimizes\n    `0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(penwt .* \u03b2)`\nsubject to linear constraints.\n\n\n\n\n\nArguments\n\n\n\n\nX\n       : predictor matrix\n\n\ny\n       : response vector\n\n\n\n\nOptional arguments\n\n\n\n\nAeq\n     : equality constraint matrix\n\n\nbeq\n     : equality constraint vector\n\n\nAineq\n   : inequality constraint matrix\n\n\nbineq\n   : inequality constraint vector\n\n\n\u03c1ridge\n  : tuning parameter for ridge penalty. Default is 0.\n\n\npenidx\n  : a logical vector indicating penalized coefficients\n\n\nsolver\n  : a solver Convex.jl supports. Default is SCS. \nhttp://convexjl.readthedocs.io/en/latest/solvers.html\n\n\n\n\nExamples\n\n\nSee tutorial examples at https://github.com/Hua-Zhou/ConstrainedLasso\n\n\nsource\n\n\nPrivate function in \nConstrainedLasso\n:\n\n\n#\n\n\nConstrainedLasso.find_\u03c1max\n \n \nFunction\n.\n\n\n  find_\u03c1max(X, y;\n      Aeq    :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq    :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq  :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq  :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      penidx :: Array{Bool} = fill(true, size(X, 2)),\n      solver = SCSSolver(verbose=0, max_iters=10e8)\n\n\n\n\n\nFind the maximum tuning parameter value \n\u03c1max\n to kick-start the solution path.\n\n\nsource", 
            "title": "Interface"
        }, 
        {
            "location": "/interface/#interface", 
            "text": "Functions exported from  ConstrainedLasso :  #  ConstrainedLasso.lsq_constrsparsereg     Function .    lsq_constrsparsereg(X, y, \u03c1 = zero(eltype(X));\n      Aeq       :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq       :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq     :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq     :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      obswt     :: AbstractVector = ones(eltype(y), length(y)),\n      penwt     :: AbstractVector = ones(eltype(X), size(X, 2)),\n      warmstart :: Bool = false,\n      solver = SCSSolver(verbose=0, max_iters=10e8)\n      )  Fit constrained lasso at fixed tuning parameter value(s) by minimizing      0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(penwt .* \u03b2)  subject to linear constraints.  Arguments   X        : predictor matrix.  y        : response vector.  \u03c1        : tuning parameter. Can be a number or a list of numbers. Default 0.   Optional arguments   Aeq      : equality constraint matrix.  beq      : equality constraint vector.  Aineq    : inequality constraint matrix.  bineq    : inequality constraint vector.  obswt    : observation weights.  penwt    : predictor penalty weights. Default is  [1 1 1 ... 1] .  solver   : a solver Convex.jl supports. Default is SCS.  http://convexjl.readthedocs.io/en/latest/solvers.html  \u03b20       : starting point for warm start.   Returns   \u03b2        : estimated coefficents.  objval   : optimal objective value.  problem  : Convex.jl problem.   source  #  ConstrainedLasso.lsq_classopath     Function .    lsq_classopath(X, y;\n      Aeq    :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq    :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq  :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq  :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      \u03c1ridge :: Number = zero(eltype(X)),\n      penidx ::Array{Bool} = fill(true, size(X, 2)),\n      solver = SCSSolver(verbose=0, max_iters=10e8)\n      )\n\nCalculate the solution path of the constrained lasso problem that minimizes\n    `0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(penwt .* \u03b2)`\nsubject to linear constraints.  Arguments   X        : predictor matrix  y        : response vector   Optional arguments   Aeq      : equality constraint matrix  beq      : equality constraint vector  Aineq    : inequality constraint matrix  bineq    : inequality constraint vector  \u03c1ridge   : tuning parameter for ridge penalty. Default is 0.  penidx   : a logical vector indicating penalized coefficients  solver   : a solver Convex.jl supports. Default is SCS.  http://convexjl.readthedocs.io/en/latest/solvers.html   Examples  See tutorial examples at https://github.com/Hua-Zhou/ConstrainedLasso  source  Private function in  ConstrainedLasso :  #  ConstrainedLasso.find_\u03c1max     Function .    find_\u03c1max(X, y;\n      Aeq    :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq    :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq  :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq  :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      penidx :: Array{Bool} = fill(true, size(X, 2)),\n      solver = SCSSolver(verbose=0, max_iters=10e8)  Find the maximum tuning parameter value  \u03c1max  to kick-start the solution path.  source", 
            "title": "Interface"
        }, 
        {
            "location": "/demo/fixedparam/", 
            "text": "Optimize at a fixed tuning parameter value\n\n\nlsq_constrsparsereg.jl\n fits constrained lasso\n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\boldsymbol{A\\beta}=\\boldsymbol{b} \\text{ and } \\boldsymbol{C\\beta} \\leq \\boldsymbol{d}\n\\end{split}\n\n\n\n\n\nat a fixed tuning parameter value $\\rho$ or several tuning parameter values provided by user.\n\n\n\n\nSingle tuning parameter value\n\n\nWe demonstrate using a sum-to-zero constraint example\n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\sum_j \\beta_j = 0\n\\end{split}\n\n\n\n\n\nFirst, let's define a true parameter \n\u03b2\n such that \nsum(\u03b2) = 0\n.\n\n\nusing\n \nConstrainedLasso\n,\n \nBase\n.\nTest\n\n\n\nn\n,\n \np\n \n=\n \n100\n,\n \n20\n\n\n\u03b2\n \n=\n \nzeros\n(\np\n)\n\n\n\u03b2\n[\n1\n:\nround\n(\nInt\n,\n \np\n \n/\n \n4\n)]\n \n=\n \n0\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \np\n \n/\n \n4\n)\n \n+\n \n1\n)\n:\nround\n(\nInt\n,\n \np\n \n/\n \n2\n)]\n \n=\n \n1\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \np\n \n/\n \n2\n)\n \n+\n \n1\n)\n:\nround\n(\nInt\n,\n \n3\np\n \n/\n \n4\n)]\n \n=\n \n0\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \n3\np\n \n/\n \n4\n)\n \n+\n \n1\n)\n:\np\n]\n \n=\n \n-\n1\n\n\nnothing\n \n# hide output\n\n\n\n\n\n\nNext we generate data based on the true parameter \n\u03b2\n.\n\n\nsrand\n(\n123\n)\n\n\nX\n \n=\n \nrandn\n(\nn\n,\n \np\n)\n\n\n\n\n\n\n100\u00d720 Array{Float64,2}:\n  1.19027     0.376264   0.346589    0.458099   \u2026   0.0523088   2.23365\n  2.04818    -0.405272   1.60431     0.139124      -0.168468    1.29252\n  1.14265     1.33585   -0.0246589  -0.230745      -0.247202   -0.822482\n  0.459416    1.60076   -0.106035    1.35195       -1.66701     0.0134896\n -0.396679   -1.45789   -1.29118    -0.106316      -1.24891     0.466877\n -0.664713    0.800589  -0.337985   -0.205883   \u2026  -0.623667    1.32134\n  0.980968    0.895878  -0.177092   -0.612003       0.372048    0.581506\n -0.0754831  -0.691934   0.57499    -1.39397       -0.633969    1.35917\n  0.273815   -1.50876   -1.37834     1.73135       -0.74986     1.27668\n -0.194229   -0.754523  -0.867869    2.61556       -2.07352     1.59349\n  \u22ee                                             \u22f1\n  0.678443    0.934982   0.425372    1.17431        0.780863    0.439673\n  0.28718     2.00606   -1.18929     1.35692       -0.545467   -0.40497\n  1.06816    -0.379291   0.11631     2.48089       -1.04331     1.24328\n -0.306877    0.20646   -1.34497    -0.584326      -1.8609     -0.383338\n -1.92021    -0.276028   0.426339    0.38792    \u2026   2.16327    -1.02578\n  1.6696      1.19586   -0.783625    0.718697       1.13162    -1.31358\n -0.213558   -1.2965     0.648433   -0.289336       0.263283    0.000636658\n -0.163711    0.575279  -0.176555   -0.0457259      0.152164    0.1559\n -0.902986   -0.166001  -1.27924    -1.31238        0.49458    -0.171711\n\n\n\n\n\ny\n \n=\n \nX\n \n*\n \n\u03b2\n \n+\n \nrandn\n(\nn\n)\n\n\n\n\n\n\n100-element Array{Float64,1}:\n -5.25671\n -3.12013\n -1.51739\n  4.29692\n  1.27458\n -1.88724\n  3.85962\n -0.546135\n  0.0889302\n  1.34159\n  \u22ee\n -2.58354\n  0.457617\n  0.0892258\n  2.67329\n -4.80027\n -4.35654\n  2.06607\n -1.0041\n -6.03571\n\n\n\n\n\nSince the equality constraint can be written as\n\n\n\n\n\n\\begin{pmatrix} 1 & 1 & \\cdots & 1 \\end{pmatrix} \\beta = 0,\n\n\n\n\n\nwe define the constraint as below.\n\n\nbeq\n   \n=\n \n[\n0.0\n]\n\n\nAeq\n   \n=\n \nones\n(\n1\n,\n \np\n)\n\n\n\n\n\n\n1\u00d720 Array{Float64,2}:\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \u2026  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n\n\n\n\n\nNow we are ready to fit the constrained lasso problem, say at \n\u03c1=10\n.\n\n\n\u03c1\n \n=\n \n10.0\n\n\n\u03b2\u0302\n,\n \n=\n \nlsq_constrsparsereg\n(\nX\n,\n \ny\n,\n \n\u03c1\n;\n \nAeq\n \n=\n \nAeq\n,\n \nbeq\n \n=\n \nbeq\n);\n\n\n\n\n\n\n([1.06543e-7; 1.16499e-7; \u2026 ; -1.00308; -0.704239], 127.35094120783674, Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint:\n\n= constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal)\n\n\n\n\n\nWe see if the sum of estimated $\\beta$ coefficients equal to 0.\n\n\n@test\n \nsum\n(\n\u03b2\u0302\n)\n\u22480\n.\n0\n \natol\n=\n1e-5\n\n\n\n\n\n\nTest Passed\n\n\n\n\n\n\n\nMultiple tuning parameter values\n\n\nDefine \n\u03c1list\n to be a sequence of values from 1 to 10.\n\n\n\u03c1list\n \n=\n \n1.0\n:\n10.0\n\n\n\n\n\n\n1.0:1.0:10.0\n\n\n\n\n\nUsing the same equality constraints, we fit the constrained lasso.\n\n\n\u03b2\u0302\n,\n \n=\n \nlsq_constrsparsereg\n(\nX\n,\n \ny\n,\n \n\u03c1list\n;\n \nAeq\n \n=\n \nAeq\n,\n \nbeq\n \n=\n \nbeq\n);\n\n\n\n\n\n\n([0.0654744 0.0528563 \u2026 1.36207e-7 1.06543e-7; 0.0984093 0.0769516 \u2026 1.55064e-7 1.16499e-7; \u2026 ; -1.14266 -1.12595 \u2026 -1.01705 -1.00308; -0.89232 -0.873294 \u2026 -0.725531 -0.704239], [40.7849, 51.2411, 61.4508, 71.4287, 81.1815, 90.7322, 100.116, 109.345, 118.421, 127.351], Any[Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint:\n\n= constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal, Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint:\n\n= constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal, Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint:\n\n= constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal, Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint:\n\n= constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal, Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint:\n\n= constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal, Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint:\n\n= constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal, Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint:\n\n= constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal, Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint:\n\n= constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal, Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint:\n\n= constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal, Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint:\n\n= constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal])\n\n\n\n\n\nNow let's test if coefficients sum to 0 at each parameter value.\n\n\n@testset\n \nzero-sum for multiple param values\n \nbegin\n \nfor\n \ni\n \nin\n \nsum\n(\n\u03b2\u0302\n,\n \n1\n)\n\n  \n@test\n \ni\u22480\n.\n0\n \natol\n=\n1.0e-5\n\n\nend\n\n\nend\n\n\n\n\n\n\nTest Summary:                      | Pass  Total\nzero-sum for multiple param values |   10     10\nBase.Test.DefaultTestSet(\nzero-sum for multiple param values\n, Any[], 10, false)", 
            "title": "Fixed tuning param value(s)"
        }, 
        {
            "location": "/demo/fixedparam/#optimize-at-a-fixed-tuning-parameter-value", 
            "text": "lsq_constrsparsereg.jl  fits constrained lasso   \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\boldsymbol{A\\beta}=\\boldsymbol{b} \\text{ and } \\boldsymbol{C\\beta} \\leq \\boldsymbol{d}\n\\end{split}   at a fixed tuning parameter value $\\rho$ or several tuning parameter values provided by user.", 
            "title": "Optimize at a fixed tuning parameter value"
        }, 
        {
            "location": "/demo/fixedparam/#single-tuning-parameter-value", 
            "text": "We demonstrate using a sum-to-zero constraint example   \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\sum_j \\beta_j = 0\n\\end{split}   First, let's define a true parameter  \u03b2  such that  sum(\u03b2) = 0 .  using   ConstrainedLasso ,   Base . Test  n ,   p   =   100 ,   20  \u03b2   =   zeros ( p )  \u03b2 [ 1 : round ( Int ,   p   /   4 )]   =   0  \u03b2 [( round ( Int ,   p   /   4 )   +   1 ) : round ( Int ,   p   /   2 )]   =   1  \u03b2 [( round ( Int ,   p   /   2 )   +   1 ) : round ( Int ,   3 p   /   4 )]   =   0  \u03b2 [( round ( Int ,   3 p   /   4 )   +   1 ) : p ]   =   - 1  nothing   # hide output   Next we generate data based on the true parameter  \u03b2 .  srand ( 123 )  X   =   randn ( n ,   p )   100\u00d720 Array{Float64,2}:\n  1.19027     0.376264   0.346589    0.458099   \u2026   0.0523088   2.23365\n  2.04818    -0.405272   1.60431     0.139124      -0.168468    1.29252\n  1.14265     1.33585   -0.0246589  -0.230745      -0.247202   -0.822482\n  0.459416    1.60076   -0.106035    1.35195       -1.66701     0.0134896\n -0.396679   -1.45789   -1.29118    -0.106316      -1.24891     0.466877\n -0.664713    0.800589  -0.337985   -0.205883   \u2026  -0.623667    1.32134\n  0.980968    0.895878  -0.177092   -0.612003       0.372048    0.581506\n -0.0754831  -0.691934   0.57499    -1.39397       -0.633969    1.35917\n  0.273815   -1.50876   -1.37834     1.73135       -0.74986     1.27668\n -0.194229   -0.754523  -0.867869    2.61556       -2.07352     1.59349\n  \u22ee                                             \u22f1\n  0.678443    0.934982   0.425372    1.17431        0.780863    0.439673\n  0.28718     2.00606   -1.18929     1.35692       -0.545467   -0.40497\n  1.06816    -0.379291   0.11631     2.48089       -1.04331     1.24328\n -0.306877    0.20646   -1.34497    -0.584326      -1.8609     -0.383338\n -1.92021    -0.276028   0.426339    0.38792    \u2026   2.16327    -1.02578\n  1.6696      1.19586   -0.783625    0.718697       1.13162    -1.31358\n -0.213558   -1.2965     0.648433   -0.289336       0.263283    0.000636658\n -0.163711    0.575279  -0.176555   -0.0457259      0.152164    0.1559\n -0.902986   -0.166001  -1.27924    -1.31238        0.49458    -0.171711  y   =   X   *   \u03b2   +   randn ( n )   100-element Array{Float64,1}:\n -5.25671\n -3.12013\n -1.51739\n  4.29692\n  1.27458\n -1.88724\n  3.85962\n -0.546135\n  0.0889302\n  1.34159\n  \u22ee\n -2.58354\n  0.457617\n  0.0892258\n  2.67329\n -4.80027\n -4.35654\n  2.06607\n -1.0041\n -6.03571  Since the equality constraint can be written as   \n\\begin{pmatrix} 1 & 1 & \\cdots & 1 \\end{pmatrix} \\beta = 0,   we define the constraint as below.  beq     =   [ 0.0 ]  Aeq     =   ones ( 1 ,   p )   1\u00d720 Array{Float64,2}:\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \u2026  1.0  1.0  1.0  1.0  1.0  1.0  1.0  Now we are ready to fit the constrained lasso problem, say at  \u03c1=10 .  \u03c1   =   10.0  \u03b2\u0302 ,   =   lsq_constrsparsereg ( X ,   y ,   \u03c1 ;   Aeq   =   Aeq ,   beq   =   beq );   ([1.06543e-7; 1.16499e-7; \u2026 ; -1.00308; -0.704239], 127.35094120783674, Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint: = constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal)  We see if the sum of estimated $\\beta$ coefficients equal to 0.  @test   sum ( \u03b2\u0302 ) \u22480 . 0   atol = 1e-5   Test Passed", 
            "title": "Single tuning parameter value"
        }, 
        {
            "location": "/demo/fixedparam/#multiple-tuning-parameter-values", 
            "text": "Define  \u03c1list  to be a sequence of values from 1 to 10.  \u03c1list   =   1.0 : 10.0   1.0:1.0:10.0  Using the same equality constraints, we fit the constrained lasso.  \u03b2\u0302 ,   =   lsq_constrsparsereg ( X ,   y ,   \u03c1list ;   Aeq   =   Aeq ,   beq   =   beq );   ([0.0654744 0.0528563 \u2026 1.36207e-7 1.06543e-7; 0.0984093 0.0769516 \u2026 1.55064e-7 1.16499e-7; \u2026 ; -1.14266 -1.12595 \u2026 -1.01705 -1.00308; -0.89232 -0.873294 \u2026 -0.725531 -0.704239], [40.7849, 51.2411, 61.4508, 71.4287, 81.1815, 90.7322, 100.116, 109.345, 118.421, 127.351], Any[Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint: = constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal, Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint: = constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal, Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint: = constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal, Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint: = constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal, Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint: = constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal, Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint: = constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal, Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint: = constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal, Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint: = constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal, Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint: = constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal, Problem:\nminimize AbstractExpr with\nhead: +\nsize: (1, 1)\nsign: Convex.Positive()\nvexity: Convex.ConvexVexity()\n\nsubject to\nConstraint:\n== constraint\nlhs: AbstractExpr with\nhead: *\nsize: (1, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: [0.0]\nvexity: Convex.AffineVexity()\n        Constraint: = constraint\nlhs: AbstractExpr with\nhead: *\nsize: (0, 1)\nsign: Convex.NoSign()\nvexity: Convex.AffineVexity()\n\nrhs: Float64[]\nvexity: Convex.AffineVexity()\ncurrent status: Optimal])  Now let's test if coefficients sum to 0 at each parameter value.  @testset   zero-sum for multiple param values   begin   for   i   in   sum ( \u03b2\u0302 ,   1 ) \n   @test   i\u22480 . 0   atol = 1.0e-5  end  end   Test Summary:                      | Pass  Total\nzero-sum for multiple param values |   10     10\nBase.Test.DefaultTestSet( zero-sum for multiple param values , Any[], 10, false)", 
            "title": "Multiple tuning parameter values"
        }, 
        {
            "location": "/references/", 
            "text": "References\n\n\n\n\n[1] Altenbuchinger, M., Rehberg, T., Zacharias, H.U., St\u00e4mmler, F., Dettmer, K., Weber, D., Hiergeist, A., Gessner, A., Holler, E., Oefner, P.J. and Spang, R. (2016). Reference point insensitive molecular data analysis. \nBioinformatics\n, 33(2):219-226.\n\n\n[2] Bredel, M., Bredel, C., Juric, D., Harsh, G. R., Vogel, H., Recht, L. D., and Sikic, B. I. (2005). High-resolution genome-wide mapping of genetic alterations in human glial brain tumors. \nCancer Research\n, 65:4088\u20134096.\n\n\n[3] Gaines, B.R. and Zhou, H. (2016). Algorithms for fitting the constrained lasso. arXiv preprint arXiv:1611.01511.\n\n\n[4] Stamey, T.A., Kabalin, J.N., McNeal, J.E., Johnstone, I.M., Freiha, F., Redwine, E.A. and Yang, N. (1989). Prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate: II. radical prostatectomy treated patients. \nJournal of Urology\n 141(5):1076\u20131083.\n\n\n[5] Tibshirani, R. J., Hoefling, H., and Tibshirani, R. (2011). Nearly-isotonic regression. \nTechnometrics\n, 53:54\u201361.\n\n\n[6] Tibshirani, R. and Wang, P. (2008). Spatial smoothing and hot spot detection for CGH data using the fused lasso. \nBiostatistics\n, 9:18\u201329.\n\n\n[7] Weber, D., Oefner, P. J., Hiergeist, A., Koestler, J., Gessner, A., Weber, M., Hahn, J., Wolff, D., Sta \u0308mmler, F., Spang, R., Herr, W., Dettmer, K., and Holler, E. (2015). Low urinary undoxyl sulfate levels early after transplantation reflect a disrupted microbiome and are associated with poor outcome. \nBlood\n, 126:1723\u20131728.\n\n\n[8] Wu, W. B., Woodroofe, M., and Mentz, G. (2001). Isotonic regression: another look at the changepoint problem. \nBiometrika\n, 88:793\u2013804.", 
            "title": "References"
        }, 
        {
            "location": "/references/#references", 
            "text": "[1] Altenbuchinger, M., Rehberg, T., Zacharias, H.U., St\u00e4mmler, F., Dettmer, K., Weber, D., Hiergeist, A., Gessner, A., Holler, E., Oefner, P.J. and Spang, R. (2016). Reference point insensitive molecular data analysis.  Bioinformatics , 33(2):219-226.  [2] Bredel, M., Bredel, C., Juric, D., Harsh, G. R., Vogel, H., Recht, L. D., and Sikic, B. I. (2005). High-resolution genome-wide mapping of genetic alterations in human glial brain tumors.  Cancer Research , 65:4088\u20134096.  [3] Gaines, B.R. and Zhou, H. (2016). Algorithms for fitting the constrained lasso. arXiv preprint arXiv:1611.01511.  [4] Stamey, T.A., Kabalin, J.N., McNeal, J.E., Johnstone, I.M., Freiha, F., Redwine, E.A. and Yang, N. (1989). Prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate: II. radical prostatectomy treated patients.  Journal of Urology  141(5):1076\u20131083.  [5] Tibshirani, R. J., Hoefling, H., and Tibshirani, R. (2011). Nearly-isotonic regression.  Technometrics , 53:54\u201361.  [6] Tibshirani, R. and Wang, P. (2008). Spatial smoothing and hot spot detection for CGH data using the fused lasso.  Biostatistics , 9:18\u201329.  [7] Weber, D., Oefner, P. J., Hiergeist, A., Koestler, J., Gessner, A., Weber, M., Hahn, J., Wolff, D., Sta \u0308mmler, F., Spang, R., Herr, W., Dettmer, K., and Holler, E. (2015). Low urinary undoxyl sulfate levels early after transplantation reflect a disrupted microbiome and are associated with poor outcome.  Blood , 126:1723\u20131728.  [8] Wu, W. B., Woodroofe, M., and Mentz, G. (2001). Isotonic regression: another look at the changepoint problem.  Biometrika , 88:793\u2013804.", 
            "title": "References"
        }
    ]
}