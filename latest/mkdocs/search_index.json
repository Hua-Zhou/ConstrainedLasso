{
    "docs": [
        {
            "location": "/", 
            "text": "ConstrainedLasso\n\n\nConstrainedLasso\n estimates the following constrained lasso problem\n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\boldsymbol{A\\beta}=\\boldsymbol{b} \\text{ and } \\boldsymbol{C\\beta} \\leq \\boldsymbol{d}\n\\end{split}\n\n\n\n\n\nwhere $\\boldsymbol{y} \\in \\mathbb{R}^n$ is the response vector, $\\boldsymbol{X}\\in \\mathbb{R}^{n\\times p}$ is the design matrix of predictor or covariates, $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ is the vector of unknown regression coefficients, and $\\rho \\geq 0$ is a tuning parameter that controls the amount of regularization.\n\n\n\n\nInstallation\n\n\nWithin Julia, use the package manager to install \nConstainedLasso\n:\n\n\nPkg\n.\nclone\n(\nhttps://github.com/Hua-Zhou/ConstrainedLasso.git\n)\n\n\n\n\n\n\nThis package supports Julia v0.6.\n\n\n\n\nCitation\n\n\nIf you use ConstrainedLasso package in your research, please cite the following paper on the algorithms:\n\n\nB.R. Gaines, H. Zhou. (2016) Algorithms for Fitting the Constrained Lasso. \nhttps://arxiv.org/abs/1611.01511\n\n\nOriginal method paper on the constrained lasso is\n\n\nG.M. James, C. Paulson and P. Rusmevichientong. (2013) Penalized and constrained regression. \nhttp://www-bcf.usc.edu/~gareth/research/PAC.pdf", 
            "title": "Home"
        }, 
        {
            "location": "/#constrainedlasso", 
            "text": "ConstrainedLasso  estimates the following constrained lasso problem   \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\boldsymbol{A\\beta}=\\boldsymbol{b} \\text{ and } \\boldsymbol{C\\beta} \\leq \\boldsymbol{d}\n\\end{split}   where $\\boldsymbol{y} \\in \\mathbb{R}^n$ is the response vector, $\\boldsymbol{X}\\in \\mathbb{R}^{n\\times p}$ is the design matrix of predictor or covariates, $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ is the vector of unknown regression coefficients, and $\\rho \\geq 0$ is a tuning parameter that controls the amount of regularization.", 
            "title": "ConstrainedLasso"
        }, 
        {
            "location": "/#installation", 
            "text": "Within Julia, use the package manager to install  ConstainedLasso :  Pkg . clone ( https://github.com/Hua-Zhou/ConstrainedLasso.git )   This package supports Julia v0.6.", 
            "title": "Installation"
        }, 
        {
            "location": "/#citation", 
            "text": "If you use ConstrainedLasso package in your research, please cite the following paper on the algorithms:  B.R. Gaines, H. Zhou. (2016) Algorithms for Fitting the Constrained Lasso.  https://arxiv.org/abs/1611.01511  Original method paper on the constrained lasso is  G.M. James, C. Paulson and P. Rusmevichientong. (2013) Penalized and constrained regression.  http://www-bcf.usc.edu/~gareth/research/PAC.pdf", 
            "title": "Citation"
        }, 
        {
            "location": "/interface/", 
            "text": "Interface\n\n\nFunctions exported from \nConstrainedLasso\n:\n\n\n#\n\n\nConstrainedLasso.lsq_constrsparsereg\n \n \nFunction\n.\n\n\n  lsq_constrsparsereg(X, y, \u03c1 = zero(eltype(X));\n      Aeq       :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq       :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq     :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq     :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      obswt     :: AbstractVector = ones(eltype(y), length(y)),\n      penwt     :: AbstractVector = ones(eltype(X), size(X, 2)),\n      warmstart :: Bool = false,\n      solver = SCSSolver(verbose=0, max_iters=10e8)\n      )\n\n\n\n\n\nFit constrained lasso at fixed tuning parameter value(s) by minimizing     \n0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(penwt .* \u03b2)\n subject to linear constraints.\n\n\nArguments\n\n\n\n\nX\n       : predictor matrix.\n\n\ny\n       : response vector.\n\n\n\u03c1\n       : tuning parameter. Can be a number or a list of numbers. Default 0.\n\n\n\n\nOptional arguments\n\n\n\n\nAeq\n     : equality constraint matrix.\n\n\nbeq\n     : equality constraint vector.\n\n\nAineq\n   : inequality constraint matrix.\n\n\nbineq\n   : inequality constraint vector.\n\n\nobswt\n   : observation weights.\n\n\npenwt\n   : predictor penalty weights. Default is \n[1 1 1 ... 1]\n.\n\n\nsolver\n  : a solver Convex.jl supports. Default is SCS. \nhttp://convexjl.readthedocs.io/en/latest/solvers.html\n\n\n\u03b20\n      : starting point for warm start.\n\n\n\n\nReturns\n\n\n\n\n\u03b2\n       : estimated coefficents.\n\n\nobjval\n  : optimal objective value.\n\n\nproblem\n : Convex.jl problem.\n\n\n\n\nsource\n\n\n#\n\n\nConstrainedLasso.lsq_classopath\n \n \nFunction\n.\n\n\n  lsq_classopath(X, y;\n      Aeq    :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq    :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq  :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq  :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      \u03c1ridge :: Number = zero(eltype(X)),\n      penidx ::Array{Bool} = fill(true, size(X, 2)),\n      solver = SCSSolver(verbose=0, max_iters=10e8)\n      )\n\nCalculate the solution path of the constrained lasso problem that minimizes\n    `0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(penwt .* \u03b2)`\nsubject to linear constraints.\n\n\n\n\n\nArguments\n\n\n\n\nX\n       : predictor matrix\n\n\ny\n       : response vector\n\n\n\n\nOptional arguments\n\n\n\n\nAeq\n     : equality constraint matrix\n\n\nbeq\n     : equality constraint vector\n\n\nAineq\n   : inequality constraint matrix\n\n\nbineq\n   : inequality constraint vector\n\n\n\u03c1ridge\n  : tuning parameter for ridge penalty. Default is 0.\n\n\npenidx\n  : a logical vector indicating penalized coefficients\n\n\nsolver\n  : a solver Convex.jl supports. Default is SCS. \nhttp://convexjl.readthedocs.io/en/latest/solvers.html\n\n\n\n\nExamples\n\n\nSee tutorial examples at https://github.com/Hua-Zhou/ConstrainedLasso\n\n\nsource\n\n\nPrivate function in \nConstrainedLasso\n:\n\n\n#\n\n\nConstrainedLasso.find_\u03c1max\n \n \nFunction\n.\n\n\n  find_\u03c1max(X, y;\n      Aeq    :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq    :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq  :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq  :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      penidx :: Array{Bool} = fill(true, size(X, 2)),\n      solver = SCSSolver(verbose=0, max_iters=10e8)\n\n\n\n\n\nFind the maximum tuning parameter value \n\u03c1max\n to kick-start the solution path.\n\n\nsource", 
            "title": "Interface"
        }, 
        {
            "location": "/interface/#interface", 
            "text": "Functions exported from  ConstrainedLasso :  #  ConstrainedLasso.lsq_constrsparsereg     Function .    lsq_constrsparsereg(X, y, \u03c1 = zero(eltype(X));\n      Aeq       :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq       :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq     :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq     :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      obswt     :: AbstractVector = ones(eltype(y), length(y)),\n      penwt     :: AbstractVector = ones(eltype(X), size(X, 2)),\n      warmstart :: Bool = false,\n      solver = SCSSolver(verbose=0, max_iters=10e8)\n      )  Fit constrained lasso at fixed tuning parameter value(s) by minimizing      0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(penwt .* \u03b2)  subject to linear constraints.  Arguments   X        : predictor matrix.  y        : response vector.  \u03c1        : tuning parameter. Can be a number or a list of numbers. Default 0.   Optional arguments   Aeq      : equality constraint matrix.  beq      : equality constraint vector.  Aineq    : inequality constraint matrix.  bineq    : inequality constraint vector.  obswt    : observation weights.  penwt    : predictor penalty weights. Default is  [1 1 1 ... 1] .  solver   : a solver Convex.jl supports. Default is SCS.  http://convexjl.readthedocs.io/en/latest/solvers.html  \u03b20       : starting point for warm start.   Returns   \u03b2        : estimated coefficents.  objval   : optimal objective value.  problem  : Convex.jl problem.   source  #  ConstrainedLasso.lsq_classopath     Function .    lsq_classopath(X, y;\n      Aeq    :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq    :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq  :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq  :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      \u03c1ridge :: Number = zero(eltype(X)),\n      penidx ::Array{Bool} = fill(true, size(X, 2)),\n      solver = SCSSolver(verbose=0, max_iters=10e8)\n      )\n\nCalculate the solution path of the constrained lasso problem that minimizes\n    `0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(penwt .* \u03b2)`\nsubject to linear constraints.  Arguments   X        : predictor matrix  y        : response vector   Optional arguments   Aeq      : equality constraint matrix  beq      : equality constraint vector  Aineq    : inequality constraint matrix  bineq    : inequality constraint vector  \u03c1ridge   : tuning parameter for ridge penalty. Default is 0.  penidx   : a logical vector indicating penalized coefficients  solver   : a solver Convex.jl supports. Default is SCS.  http://convexjl.readthedocs.io/en/latest/solvers.html   Examples  See tutorial examples at https://github.com/Hua-Zhou/ConstrainedLasso  source  Private function in  ConstrainedLasso :  #  ConstrainedLasso.find_\u03c1max     Function .    find_\u03c1max(X, y;\n      Aeq    :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq    :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq  :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq  :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      penidx :: Array{Bool} = fill(true, size(X, 2)),\n      solver = SCSSolver(verbose=0, max_iters=10e8)  Find the maximum tuning parameter value  \u03c1max  to kick-start the solution path.  source", 
            "title": "Interface"
        }, 
        {
            "location": "/demo/fixedparam/", 
            "text": "Optimize at fixed tuning parameter value(s)\n\n\nlsq_constrsparsereg.jl\n fits constrained lasso\n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\boldsymbol{A\\beta}=\\boldsymbol{b} \\text{ and } \\boldsymbol{C\\beta} \\leq \\boldsymbol{d}\n\\end{split}\n\n\n\n\n\nat a fixed tuning parameter value $\\rho$ or several tuning parameter values provided by user.\n\n\n\n\nSingle tuning parameter value\n\n\nWe demonstrate using a sum-to-zero constraint example\n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\sum_j \\beta_j = 0\n\\end{split}\n\n\n\n\n\nFirst, let's define a true parameter \n\u03b2\n such that \nsum(\u03b2) = 0\n.\n\n\nusing\n \nConstrainedLasso\n,\n \nBase\n.\nTest\n\n\n\nn\n,\n \np\n \n=\n \n100\n,\n \n20\n\n\n\u03b2\n \n=\n \nzeros\n(\np\n)\n\n\n\u03b2\n[\n1\n:\nround\n(\nInt\n,\n \np\n \n/\n \n4\n)]\n \n=\n \n0\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \np\n \n/\n \n4\n)\n \n+\n \n1\n)\n:\nround\n(\nInt\n,\n \np\n \n/\n \n2\n)]\n \n=\n \n1\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \np\n \n/\n \n2\n)\n \n+\n \n1\n)\n:\nround\n(\nInt\n,\n \n3\np\n \n/\n \n4\n)]\n \n=\n \n0\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \n3\np\n \n/\n \n4\n)\n \n+\n \n1\n)\n:\np\n]\n \n=\n \n-\n1\n\n\n\u03b2\n\n\n\n\n\n\n20-element Array{Float64,1}:\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  1.0\n  1.0\n  1.0\n  1.0\n  1.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n\n\n\n\n\nNext we generate data based on the true parameter \n\u03b2\n.\n\n\nsrand\n(\n123\n)\n\n\nX\n \n=\n \nrandn\n(\nn\n,\n \np\n)\n\n\n\n\n\n\n100\u00d720 Array{Float64,2}:\n  1.19027     0.376264    0.346589    0.458099   \u2026   0.0523088   2.23365    \n  2.04818    -0.405272    1.60431     0.139124      -0.168468    1.29252    \n  1.14265     1.33585    -0.0246589  -0.230745      -0.247202   -0.822482   \n  0.459416    1.60076    -0.106035    1.35195       -1.66701     0.0134896  \n -0.396679   -1.45789    -1.29118    -0.106316      -1.24891     0.466877   \n -0.664713    0.800589   -0.337985   -0.205883   \u2026  -0.623667    1.32134    \n  0.980968    0.895878   -0.177092   -0.612003       0.372048    0.581506   \n -0.0754831  -0.691934    0.57499    -1.39397       -0.633969    1.35917    \n  0.273815   -1.50876    -1.37834     1.73135       -0.74986     1.27668    \n -0.194229   -0.754523   -0.867869    2.61556       -2.07352     1.59349    \n -0.339366    0.115622   -0.400076    1.76909    \u2026   0.496242    0.806838   \n -0.843878    0.242595    0.295087    0.240332       0.5764      1.55243    \n -0.888936   -0.223211    0.817696   -1.42384       -0.859119    1.49178    \n  \u22ee                                              \u22f1                          \n -0.733961    0.911747   -0.618047   -0.319891       1.44548    -1.1144     \n  0.459398    0.0138789  -1.08527    -0.529198       0.395225    0.822061   \n  1.70619     2.2959      0.4024      1.47241    \u2026  -0.260034   -0.746822   \n  0.678443    0.934982    0.425372    1.17431        0.780863    0.439673   \n  0.28718     2.00606    -1.18929     1.35692       -0.545467   -0.40497    \n  1.06816    -0.379291    0.11631     2.48089       -1.04331     1.24328    \n -0.306877    0.20646    -1.34497    -0.584326      -1.8609     -0.383338   \n -1.92021    -0.276028    0.426339    0.38792    \u2026   2.16327    -1.02578    \n  1.6696      1.19586    -0.783625    0.718697       1.13162    -1.31358    \n -0.213558   -1.2965      0.648433   -0.289336       0.263283    0.000636658\n -0.163711    0.575279   -0.176555   -0.0457259      0.152164    0.1559     \n -0.902986   -0.166001   -1.27924    -1.31238        0.49458    -0.171711\n\n\n\n\n\ny\n \n=\n \nX\n \n*\n \n\u03b2\n \n+\n \nrandn\n(\nn\n)\n\n\n\n\n\n\n100-element Array{Float64,1}:\n -6.50888  \n -3.00423  \n -2.3809   \n  2.50638  \n -2.24753  \n -2.51881  \n  3.60092  \n -1.38597  \n  0.0562454\n  0.787719 \n  3.17731  \n -2.1989   \n -1.86609  \n  \u22ee        \n  0.87746  \n -3.68264  \n -3.19285  \n -0.961258 \n  0.793834 \n  0.140524 \n  1.71841  \n -6.31781  \n -1.95472  \n  1.4803   \n -2.15804  \n -6.66724\n\n\n\n\n\nSince the equality constraint can be written as\n\n\n\n\n\n\\begin{split}\n\\begin{pmatrix} 1 & 1 & \\cdots & 1 \\end{pmatrix} \\beta = 0,\n\\end{split}\n\n\n\n\n\nwe define the constraint as below.\n\n\nbeq\n   \n=\n \n[\n0.0\n]\n\n\nAeq\n   \n=\n \nones\n(\n1\n,\n \np\n)\n\n\n\n\n\n\n1\u00d720 Array{Float64,2}:\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \u2026  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n\n\n\n\n\nNow we are ready to fit the constrained lasso problem, say at \n\u03c1=10\n.\n\n\n\u03c1\n \n=\n \n10.0\n\n\n\u03b2\u0302\n,\n \n=\n \nlsq_constrsparsereg\n(\nX\n,\n \ny\n,\n \n\u03c1\n;\n \nAeq\n \n=\n \nAeq\n,\n \nbeq\n \n=\n \nbeq\n);\n\n\n\n\n\n\n\u03b2\u0302\n\n\n\n\n\n\n20\u00d71 Array{Float64,2}:\n  1.06543e-7 \n  1.16499e-7 \n  0.184567   \n -1.07162e-8 \n  6.88319e-8 \n  0.891789   \n  0.867016   \n  0.89869    \n  0.830042   \n  0.740962   \n  3.21933e-8 \n -7.74822e-8 \n -6.04844e-12\n -6.67961e-8 \n  0.0155802  \n -0.918892   \n -0.927699   \n -0.874741   \n -1.00308    \n -0.704239\n\n\n\n\n\nWe see if the sum of estimated $\\beta$ coefficients equal to 0.\n\n\n@test\n \nsum\n(\n\u03b2\u0302\n)\n\u22480\n.\n0\n \natol\n=\n1e-5\n\n\n\n\n\n\n\u001bTest Passed\n\n\n\n\n\n\n\nMultiple tuning parameter values\n\n\nDefine \n\u03c1list\n to be a sequence of values from 1 to 10.\n\n\n\u03c1list\n \n=\n \n1.0\n:\n10.0\n\n\n\n\n\n\n1.0:1.0:10.0\n\n\n\n\n\nUsing the same equality constraints, we fit the constrained lasso.\n\n\n\u03b2\u0302\n,\n \n=\n \nlsq_constrsparsereg\n(\nX\n,\n \ny\n,\n \n\u03c1list\n;\n \nAeq\n \n=\n \nAeq\n,\n \nbeq\n \n=\n \nbeq\n);\n\n\n\n\n\n\n\u03b2\u0302\n\n\n\n\n\n\n20\u00d710 Array{Float64,2}:\n  0.0654744    0.0528563    0.0435276   \u2026   1.36207e-7   1.06543e-7 \n  0.0984093    0.0769516    0.0557896       1.55064e-7   1.16499e-7 \n  0.256568     0.246248     0.236865        0.19077      0.184567   \n -0.0161896    1.24814e-6   3.09904e-7     -1.14652e-8  -1.07162e-8 \n  0.0442985    0.0355132    0.0266877       9.93987e-8   6.88319e-8 \n  0.978801     0.965257     0.953956    \u2026   0.900293     0.891789   \n  0.962883     0.952699     0.943744        0.880242     0.867016   \n  0.910347     0.907963     0.907171        0.900036     0.89869    \n  0.968082     0.947724     0.931277        0.845411     0.830042   \n  0.936793     0.911403     0.887538        0.76097      0.740962   \n -9.39273e-8  -2.28297e-7  -3.90716e-8  \u2026   4.38155e-8   3.21933e-8 \n -0.0819607   -0.0711052   -0.0585947      -0.00364202  -7.74822e-8 \n -3.72613e-8   2.38111e-7   8.712e-8        6.32877e-9  -6.04844e-12\n -0.0609711   -0.0433998   -0.0252136      -8.37749e-8  -6.67961e-8 \n  0.0730144    0.0671585    0.0608885       0.0235996    0.0155802  \n -1.04989     -1.03264     -1.01746     \u2026  -0.931341    -0.918892   \n -1.00933     -0.998772    -0.98755        -0.934456    -0.927699   \n -1.04134     -1.01861     -0.996088       -0.889298    -0.874741   \n -1.14266     -1.12595     -1.11046        -1.01705     -1.00308    \n -0.89232     -0.873294    -0.85207        -0.725531    -0.704239\n\n\n\n\n\nNow let's test if coefficients sum to 0 at each parameter value.\n\n\n@testset\n \nzero-sum for multiple param values\n \nbegin\n \nfor\n \ni\n \nin\n \nsum\n(\n\u03b2\u0302\n,\n \n1\n)\n\n  \n@test\n \ni\u22480\n.\n0\n \natol\n=\n1.0e-5\n\n\nend\n\n\nend\n\n\n\n\n\n\n\u001bTest Summary:                      | \u001bPass  \u001bTotal\u001b\nzero-sum for multiple param values |   \u001b10  \u001b   10\u001b", 
            "title": "Fixed tuning param value"
        }, 
        {
            "location": "/demo/fixedparam/#optimize-at-fixed-tuning-parameter-values", 
            "text": "lsq_constrsparsereg.jl  fits constrained lasso   \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\boldsymbol{A\\beta}=\\boldsymbol{b} \\text{ and } \\boldsymbol{C\\beta} \\leq \\boldsymbol{d}\n\\end{split}   at a fixed tuning parameter value $\\rho$ or several tuning parameter values provided by user.", 
            "title": "Optimize at fixed tuning parameter value(s)"
        }, 
        {
            "location": "/demo/fixedparam/#single-tuning-parameter-value", 
            "text": "We demonstrate using a sum-to-zero constraint example   \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\sum_j \\beta_j = 0\n\\end{split}   First, let's define a true parameter  \u03b2  such that  sum(\u03b2) = 0 .  using   ConstrainedLasso ,   Base . Test  n ,   p   =   100 ,   20  \u03b2   =   zeros ( p )  \u03b2 [ 1 : round ( Int ,   p   /   4 )]   =   0  \u03b2 [( round ( Int ,   p   /   4 )   +   1 ) : round ( Int ,   p   /   2 )]   =   1  \u03b2 [( round ( Int ,   p   /   2 )   +   1 ) : round ( Int ,   3 p   /   4 )]   =   0  \u03b2 [( round ( Int ,   3 p   /   4 )   +   1 ) : p ]   =   - 1  \u03b2   20-element Array{Float64,1}:\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  1.0\n  1.0\n  1.0\n  1.0\n  1.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0  Next we generate data based on the true parameter  \u03b2 .  srand ( 123 )  X   =   randn ( n ,   p )   100\u00d720 Array{Float64,2}:\n  1.19027     0.376264    0.346589    0.458099   \u2026   0.0523088   2.23365    \n  2.04818    -0.405272    1.60431     0.139124      -0.168468    1.29252    \n  1.14265     1.33585    -0.0246589  -0.230745      -0.247202   -0.822482   \n  0.459416    1.60076    -0.106035    1.35195       -1.66701     0.0134896  \n -0.396679   -1.45789    -1.29118    -0.106316      -1.24891     0.466877   \n -0.664713    0.800589   -0.337985   -0.205883   \u2026  -0.623667    1.32134    \n  0.980968    0.895878   -0.177092   -0.612003       0.372048    0.581506   \n -0.0754831  -0.691934    0.57499    -1.39397       -0.633969    1.35917    \n  0.273815   -1.50876    -1.37834     1.73135       -0.74986     1.27668    \n -0.194229   -0.754523   -0.867869    2.61556       -2.07352     1.59349    \n -0.339366    0.115622   -0.400076    1.76909    \u2026   0.496242    0.806838   \n -0.843878    0.242595    0.295087    0.240332       0.5764      1.55243    \n -0.888936   -0.223211    0.817696   -1.42384       -0.859119    1.49178    \n  \u22ee                                              \u22f1                          \n -0.733961    0.911747   -0.618047   -0.319891       1.44548    -1.1144     \n  0.459398    0.0138789  -1.08527    -0.529198       0.395225    0.822061   \n  1.70619     2.2959      0.4024      1.47241    \u2026  -0.260034   -0.746822   \n  0.678443    0.934982    0.425372    1.17431        0.780863    0.439673   \n  0.28718     2.00606    -1.18929     1.35692       -0.545467   -0.40497    \n  1.06816    -0.379291    0.11631     2.48089       -1.04331     1.24328    \n -0.306877    0.20646    -1.34497    -0.584326      -1.8609     -0.383338   \n -1.92021    -0.276028    0.426339    0.38792    \u2026   2.16327    -1.02578    \n  1.6696      1.19586    -0.783625    0.718697       1.13162    -1.31358    \n -0.213558   -1.2965      0.648433   -0.289336       0.263283    0.000636658\n -0.163711    0.575279   -0.176555   -0.0457259      0.152164    0.1559     \n -0.902986   -0.166001   -1.27924    -1.31238        0.49458    -0.171711  y   =   X   *   \u03b2   +   randn ( n )   100-element Array{Float64,1}:\n -6.50888  \n -3.00423  \n -2.3809   \n  2.50638  \n -2.24753  \n -2.51881  \n  3.60092  \n -1.38597  \n  0.0562454\n  0.787719 \n  3.17731  \n -2.1989   \n -1.86609  \n  \u22ee        \n  0.87746  \n -3.68264  \n -3.19285  \n -0.961258 \n  0.793834 \n  0.140524 \n  1.71841  \n -6.31781  \n -1.95472  \n  1.4803   \n -2.15804  \n -6.66724  Since the equality constraint can be written as   \n\\begin{split}\n\\begin{pmatrix} 1 & 1 & \\cdots & 1 \\end{pmatrix} \\beta = 0,\n\\end{split}   we define the constraint as below.  beq     =   [ 0.0 ]  Aeq     =   ones ( 1 ,   p )   1\u00d720 Array{Float64,2}:\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \u2026  1.0  1.0  1.0  1.0  1.0  1.0  1.0  Now we are ready to fit the constrained lasso problem, say at  \u03c1=10 .  \u03c1   =   10.0  \u03b2\u0302 ,   =   lsq_constrsparsereg ( X ,   y ,   \u03c1 ;   Aeq   =   Aeq ,   beq   =   beq );   \u03b2\u0302   20\u00d71 Array{Float64,2}:\n  1.06543e-7 \n  1.16499e-7 \n  0.184567   \n -1.07162e-8 \n  6.88319e-8 \n  0.891789   \n  0.867016   \n  0.89869    \n  0.830042   \n  0.740962   \n  3.21933e-8 \n -7.74822e-8 \n -6.04844e-12\n -6.67961e-8 \n  0.0155802  \n -0.918892   \n -0.927699   \n -0.874741   \n -1.00308    \n -0.704239  We see if the sum of estimated $\\beta$ coefficients equal to 0.  @test   sum ( \u03b2\u0302 ) \u22480 . 0   atol = 1e-5   \u001bTest Passed", 
            "title": "Single tuning parameter value"
        }, 
        {
            "location": "/demo/fixedparam/#multiple-tuning-parameter-values", 
            "text": "Define  \u03c1list  to be a sequence of values from 1 to 10.  \u03c1list   =   1.0 : 10.0   1.0:1.0:10.0  Using the same equality constraints, we fit the constrained lasso.  \u03b2\u0302 ,   =   lsq_constrsparsereg ( X ,   y ,   \u03c1list ;   Aeq   =   Aeq ,   beq   =   beq );   \u03b2\u0302   20\u00d710 Array{Float64,2}:\n  0.0654744    0.0528563    0.0435276   \u2026   1.36207e-7   1.06543e-7 \n  0.0984093    0.0769516    0.0557896       1.55064e-7   1.16499e-7 \n  0.256568     0.246248     0.236865        0.19077      0.184567   \n -0.0161896    1.24814e-6   3.09904e-7     -1.14652e-8  -1.07162e-8 \n  0.0442985    0.0355132    0.0266877       9.93987e-8   6.88319e-8 \n  0.978801     0.965257     0.953956    \u2026   0.900293     0.891789   \n  0.962883     0.952699     0.943744        0.880242     0.867016   \n  0.910347     0.907963     0.907171        0.900036     0.89869    \n  0.968082     0.947724     0.931277        0.845411     0.830042   \n  0.936793     0.911403     0.887538        0.76097      0.740962   \n -9.39273e-8  -2.28297e-7  -3.90716e-8  \u2026   4.38155e-8   3.21933e-8 \n -0.0819607   -0.0711052   -0.0585947      -0.00364202  -7.74822e-8 \n -3.72613e-8   2.38111e-7   8.712e-8        6.32877e-9  -6.04844e-12\n -0.0609711   -0.0433998   -0.0252136      -8.37749e-8  -6.67961e-8 \n  0.0730144    0.0671585    0.0608885       0.0235996    0.0155802  \n -1.04989     -1.03264     -1.01746     \u2026  -0.931341    -0.918892   \n -1.00933     -0.998772    -0.98755        -0.934456    -0.927699   \n -1.04134     -1.01861     -0.996088       -0.889298    -0.874741   \n -1.14266     -1.12595     -1.11046        -1.01705     -1.00308    \n -0.89232     -0.873294    -0.85207        -0.725531    -0.704239  Now let's test if coefficients sum to 0 at each parameter value.  @testset   zero-sum for multiple param values   begin   for   i   in   sum ( \u03b2\u0302 ,   1 ) \n   @test   i\u22480 . 0   atol = 1.0e-5  end  end   \u001bTest Summary:                      | \u001bPass  \u001bTotal\u001b\nzero-sum for multiple param values |   \u001b10  \u001b   10\u001b", 
            "title": "Multiple tuning parameter values"
        }, 
        {
            "location": "/demo/sim/", 
            "text": "Path algorithm\n\n\n\n\nSum-to-zero constraint\n\n\nIn this example, we will solve a problem defined by \n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\boldsymbol{\\beta}||_1  \\\\\n& \\text{subject to} \\hspace{1em} \\sum_j \\beta_j = 0\n\\end{split}\n\n\n\n\n\nNote that we can re-write the constraint as  $\\boldsymbol{A\\beta} = \\boldsymbol{b}$\n\n\nwhere \n\n\n\n\n\n\\boldsymbol{A} = \\begin{pmatrix} 1 & 1 & \\cdots & 1 \\end{pmatrix} \\text{ and } \\boldsymbol{b} = 0.\n\n\n\n\n\nFirst let's generate the predictor matrix \nX\n and response vector \ny\n. To do so, we need a true parameter vector \n\u03b2\n whose sum equals to 0. Note \nn\n is the number of observations \nn\n and \np\n is the number of predictors. \n\n\nn\n,\n \np\n \n=\n \n50\n,\n \n100\n  \n\n\u03b2\n \n=\n \nzeros\n(\np\n)\n\n\n\u03b2\n[\n1\n:\nround\n(\nInt\n,\n \np\n \n/\n \n4\n)]\n \n=\n \n0\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \np\n \n/\n \n4\n)\n \n+\n \n1\n)\n:\nround\n(\nInt\n,\n \np\n \n/\n \n2\n)]\n \n=\n \n1\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \np\n \n/\n \n2\n)\n \n+\n \n1\n)\n:\nround\n(\nInt\n,\n \n3\np\n \n/\n \n4\n)]\n \n=\n \n0\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \n3\np\n \n/\n \n4\n)\n \n+\n \n1\n)\n:\np\n]\n \n=\n \n-\n1\n\n\nsrand\n(\n41\n)\n\n\nX\n \n=\n \nrandn\n(\nn\n,\n \np\n)\n\n\n\n\n\n\n50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627\n\n\n\n\n\ny\n \n=\n \nX\n \n*\n \n\u03b2\n \n+\n \nrandn\n(\nn\n)\n\n\n\n\n\n\n50-element Array{Float64,1}:\n  -9.90585 \n  -5.40562 \n   5.24289 \n  -6.29951 \n  -4.9586  \n  -6.1342  \n  -7.90981 \n   2.51009 \n  -5.79548 \n   1.61355 \n  -0.722766\n  10.4522  \n   4.03935 \n   \u22ee       \n   0.397781\n  -2.6661  \n   5.36896 \n  -3.56537 \n  -2.402   \n   0.11478 \n  -5.39248 \n   4.38391 \n   0.706801\n -10.1066  \n  -1.12558 \n  14.2473\n\n\n\n\n\nSince the problem has equality constraints only, we define the constraints as below. \n\n\nbeq\n \n=\n \n[\n0\n]\n\n\nAeq\n \n=\n \nones\n(\n1\n,\n \np\n)\n\n\n\n\n\n\n1\u00d7100 Array{Float64,2}:\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \u2026  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n\n\n\n\n\nusing\n \nConstrainedLasso\n\n\n\u03b2\u0302path1\n,\n \n\u03c1path1\n,\n \nobjpath\n,\n \n=\n \nlsq_classopath\n(\nX\n,\n \ny\n;\n \nAeq\n \n=\n \nAeq\n,\n \nbeq\n \n=\n \nbeq\n);\n\n\n\n\n\n\nNow we are ready to obtain the solution path using the path algorithm. By default, we use the solver SCS. \n\n\n\u03b2\u0302path1\n\n\n\n\n\n\n100\u00d764 Array{Float64,2}:\n 0.0  0.0   0.0         0.0       \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.206561   0.212696   0.22402 \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0          -0.378352  -0.411385  -0.41288 \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0       \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.325283   0.3467     0.357212\n 0.0  0.0   0.0         0.0          -0.19861   -0.181371  -0.181397\n 0.0  0.0   0.0         0.0       \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n \u22ee                                \u22f1                                 \n 0.0  0.0   0.0         0.0          -0.46258   -0.452943  -0.458164\n 0.0  0.0   0.0         0.0          -0.401578  -0.359423  -0.358849\n 0.0  0.0   0.0         0.0       \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0  -0.0558231  -0.101399     -0.850614  -0.874227  -0.881474\n 0.0  0.0   0.0         0.0          -1.07203   -1.05001   -1.06761 \n 0.0  0.0   0.0         0.0          -0.674324  -0.621432  -0.622139\n 0.0  0.0   0.0         0.0       \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0          -1.25239   -1.20357   -1.2081  \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0\n\n\n\n\n\nLet's see if sums of coefficients at all $\\rho$ values are approximately 0. \n\n\nall\n(\nabs\n.\n(\nsum\n(\n\u03b2\u0302path1\n,\n \n1\n))\n \n.\n \n1e-6\n)\n\n\n\n\n\n\ntrue\n\n\n\n\n\nWe plot the solution path below. \n\n\nusing\n \nPlots\n;\n \npyplot\n();\n\n\nplot\n(\n\u03c1path1\n,\n \n\u03b2\u0302path1\n,\n \nlabel\n=\n,\n \nxaxis\n \n=\n \n(\n\u03c1\n,\n \n(\nminimum\n(\n\u03c1path1\n),\n\n      \nmaximum\n(\n\u03c1path1\n))),\n \nyaxis\n \n=\n \n(\n\u03b2\u0302(\u03c1)\n),\n \nwidth\n=\n0.5\n)\n \n\ntitle!\n(\nSimulation 1: Solution Path via Constrained Lasso\n)\n \n\n\n\n\n\n\n\n\n\nNon-negativity constraint\n\n\nIn this example, the problem is defined by \n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\boldsymbol{\\beta}||_1  \\\\\n& \\text{subject to} \\hspace{1em} \\beta_j \\geq 0 \\forall j\n\\end{split}\n\n\n\n\n\nWe can re-write the inequality constraint as $\\boldsymbol{C\\beta} \\leq \\boldsymbol{d}$ where \n\n\n\n\n\n\\boldsymbol{C} = \\begin{pmatrix} \n-1 & & & \\\\\n    & -1 & & \\\\\n    &   & \\ddots & \\\\\n    &   &   & -1\n\\end{pmatrix}\n\\text{ and } \\boldsymbol{d} = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}\n\n\n\n\n\nFirst we define a true parameter vector \n\u03b2\n that is sparse with a few non-zero coefficients. Let \nn\n and \np\n be the number of observations and predictors, respectively. \n\n\nn\n,\n \np\n \n=\n \n50\n,\n \n100\n   \n\n\u03b2\n \n=\n \nzeros\n(\np\n)\n\n\n\u03b2\n[\n1\n:\n10\n]\n \n=\n \n1\n:\n10\n\n\nsrand\n(\n41\n)\n\n\nX\n \n=\n \nrandn\n(\nn\n,\n \np\n)\n\n\n\n\n\n\n50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627\n\n\n\n\n\ny\n \n=\n \nX\n \n*\n \n\u03b2\n \n+\n \nrandn\n(\nn\n)\n\n\n\n\n\n\n50-element Array{Float64,1}:\n  12.6173  \n  40.3776  \n   2.2169  \n  27.4631  \n  38.592   \n   7.82023 \n  22.7367  \n   7.88475 \n  -7.47037 \n   0.621035\n  -4.91899 \n -14.9363  \n   8.26901 \n   \u22ee       \n   7.83882 \n  -9.30699 \n -29.7205  \n  15.2482  \n -19.1784  \n  14.9865  \n   2.32728 \n  -9.11988 \n -15.3472  \n  22.9679  \n  -0.997964\n  42.6068\n\n\n\n\n\nNow set up the inequality constraint for the problem.\n\n\nbineq\n \n=\n \nzeros\n(\np\n)\n\n\nAineq\n \n=\n \n-\n \neye\n(\np\n)\n\n\n\n\n\n\n100\u00d7100 Array{Float64,2}:\n -1.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -1.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -1.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -1.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -1.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -1.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n  \u22ee                             \u22ee    \u22f1         \u22ee                          \n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -1.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -1.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -1.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -1.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -1.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -1.0\n\n\n\n\n\nNow we are ready to obtain the solution path using the path algorithm. Here, let's try using different solver \nECOS\n for \nConvex.jl\n. \n\n\nusing\n \nECOS\n;\n \nsolver\n=\nECOSSolver\n(\nverbose\n=\n0\n,\n \nmaxit\n=\n1e8\n);\n\n\n\u03b2\u0302path2\n,\n \n\u03c1path2\n,\n \n=\n \nlsq_classopath\n(\nX\n,\n \ny\n;\n \nAineq\n \n=\n \nAineq\n,\n \nbineq\n \n=\n \nbineq\n,\n \nsolver\n \n=\n \nsolver\n)\n \n\n\n\n\n\n\u03b2\u0302path2\n\n\n\n\n\n\n100\u00d7183 Array{Float64,2}:\n 0.0         0.0      0.0      0.0      \u2026  0.783939   0.791708   0.796529 \n 0.0         0.0      0.0      0.0         2.17561    2.18099    2.18875  \n 0.0         0.0      0.0      0.0         2.99935    3.008      3.01471  \n 0.0         0.0      0.0      0.0         4.30984    4.31056    4.30849  \n 0.0         0.0      0.0      0.0         4.98995    4.99358    4.9955   \n 0.0         0.0      0.0      0.0      \u2026  6.18666    6.18814    6.18596  \n 0.0         0.0      0.0      0.0         6.92076    6.92371    6.92749  \n 0.0         0.0      0.0      0.0         8.56963    8.55907    8.54642  \n 0.0         0.0      0.0      0.0         8.86323    8.864      8.86137  \n 0.00616069  2.01444  2.41323  2.42264     9.8864     9.89486    9.90491  \n 0.0         0.0      0.0      0.0      \u2026  0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n \u22ee                                      \u22f1  \u22ee                              \n 0.0         0.0      0.0      0.0         0.127693   0.122633   0.114126 \n 0.0         0.0      0.0      0.0         0.257807   0.261246   0.265255 \n 0.0         0.0      0.0      0.0      \u2026  0.294213   0.285664   0.272772 \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0      \u2026  0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0838146  0.0914735  0.0978112\n 0.0         0.0      0.0      0.0         0.200482   0.202642   0.201151 \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0\n\n\n\n\n\nWe plot the solution path below. \n\n\nplot\n(\n\u03c1path2\n,\n \n\u03b2\u0302path2\n,\n \nlabel\n=\n,\n \nxaxis\n \n=\n \n(\n\u03c1\n,\n \n(\nminimum\n(\n\u03c1path2\n),\n\n      \nmaximum\n(\n\u03c1path2\n))),\n \nyaxis\n \n=\n \n(\n\u03b2\u0302(\u03c1)\n),\n \nwidth\n=\n0.5\n)\n \n\ntitle!\n(\nSimulation 2: Solution Path via Constrained Lasso\n)", 
            "title": "Path algorithm"
        }, 
        {
            "location": "/demo/sim/#path-algorithm", 
            "text": "", 
            "title": "Path algorithm"
        }, 
        {
            "location": "/demo/sim/#sum-to-zero-constraint", 
            "text": "In this example, we will solve a problem defined by    \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\boldsymbol{\\beta}||_1  \\\\\n& \\text{subject to} \\hspace{1em} \\sum_j \\beta_j = 0\n\\end{split}   Note that we can re-write the constraint as  $\\boldsymbol{A\\beta} = \\boldsymbol{b}$  where    \n\\boldsymbol{A} = \\begin{pmatrix} 1 & 1 & \\cdots & 1 \\end{pmatrix} \\text{ and } \\boldsymbol{b} = 0.   First let's generate the predictor matrix  X  and response vector  y . To do so, we need a true parameter vector  \u03b2  whose sum equals to 0. Note  n  is the number of observations  n  and  p  is the number of predictors.   n ,   p   =   50 ,   100    \u03b2   =   zeros ( p )  \u03b2 [ 1 : round ( Int ,   p   /   4 )]   =   0  \u03b2 [( round ( Int ,   p   /   4 )   +   1 ) : round ( Int ,   p   /   2 )]   =   1  \u03b2 [( round ( Int ,   p   /   2 )   +   1 ) : round ( Int ,   3 p   /   4 )]   =   0  \u03b2 [( round ( Int ,   3 p   /   4 )   +   1 ) : p ]   =   - 1  srand ( 41 )  X   =   randn ( n ,   p )   50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627  y   =   X   *   \u03b2   +   randn ( n )   50-element Array{Float64,1}:\n  -9.90585 \n  -5.40562 \n   5.24289 \n  -6.29951 \n  -4.9586  \n  -6.1342  \n  -7.90981 \n   2.51009 \n  -5.79548 \n   1.61355 \n  -0.722766\n  10.4522  \n   4.03935 \n   \u22ee       \n   0.397781\n  -2.6661  \n   5.36896 \n  -3.56537 \n  -2.402   \n   0.11478 \n  -5.39248 \n   4.38391 \n   0.706801\n -10.1066  \n  -1.12558 \n  14.2473  Since the problem has equality constraints only, we define the constraints as below.   beq   =   [ 0 ]  Aeq   =   ones ( 1 ,   p )   1\u00d7100 Array{Float64,2}:\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \u2026  1.0  1.0  1.0  1.0  1.0  1.0  1.0  using   ConstrainedLasso  \u03b2\u0302path1 ,   \u03c1path1 ,   objpath ,   =   lsq_classopath ( X ,   y ;   Aeq   =   Aeq ,   beq   =   beq );   Now we are ready to obtain the solution path using the path algorithm. By default, we use the solver SCS.   \u03b2\u0302path1   100\u00d764 Array{Float64,2}:\n 0.0  0.0   0.0         0.0       \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.206561   0.212696   0.22402 \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0          -0.378352  -0.411385  -0.41288 \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0       \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.325283   0.3467     0.357212\n 0.0  0.0   0.0         0.0          -0.19861   -0.181371  -0.181397\n 0.0  0.0   0.0         0.0       \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n \u22ee                                \u22f1                                 \n 0.0  0.0   0.0         0.0          -0.46258   -0.452943  -0.458164\n 0.0  0.0   0.0         0.0          -0.401578  -0.359423  -0.358849\n 0.0  0.0   0.0         0.0       \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0  -0.0558231  -0.101399     -0.850614  -0.874227  -0.881474\n 0.0  0.0   0.0         0.0          -1.07203   -1.05001   -1.06761 \n 0.0  0.0   0.0         0.0          -0.674324  -0.621432  -0.622139\n 0.0  0.0   0.0         0.0       \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0          -1.25239   -1.20357   -1.2081  \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0  Let's see if sums of coefficients at all $\\rho$ values are approximately 0.   all ( abs . ( sum ( \u03b2\u0302path1 ,   1 ))   .   1e-6 )   true  We plot the solution path below.   using   Plots ;   pyplot ();  plot ( \u03c1path1 ,   \u03b2\u0302path1 ,   label = ,   xaxis   =   ( \u03c1 ,   ( minimum ( \u03c1path1 ), \n       maximum ( \u03c1path1 ))),   yaxis   =   ( \u03b2\u0302(\u03c1) ),   width = 0.5 )   title! ( Simulation 1: Solution Path via Constrained Lasso )", 
            "title": "Sum-to-zero constraint"
        }, 
        {
            "location": "/demo/sim/#non-negativity-constraint", 
            "text": "In this example, the problem is defined by    \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\boldsymbol{\\beta}||_1  \\\\\n& \\text{subject to} \\hspace{1em} \\beta_j \\geq 0 \\forall j\n\\end{split}   We can re-write the inequality constraint as $\\boldsymbol{C\\beta} \\leq \\boldsymbol{d}$ where    \n\\boldsymbol{C} = \\begin{pmatrix} \n-1 & & & \\\\\n    & -1 & & \\\\\n    &   & \\ddots & \\\\\n    &   &   & -1\n\\end{pmatrix}\n\\text{ and } \\boldsymbol{d} = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}   First we define a true parameter vector  \u03b2  that is sparse with a few non-zero coefficients. Let  n  and  p  be the number of observations and predictors, respectively.   n ,   p   =   50 ,   100     \u03b2   =   zeros ( p )  \u03b2 [ 1 : 10 ]   =   1 : 10  srand ( 41 )  X   =   randn ( n ,   p )   50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627  y   =   X   *   \u03b2   +   randn ( n )   50-element Array{Float64,1}:\n  12.6173  \n  40.3776  \n   2.2169  \n  27.4631  \n  38.592   \n   7.82023 \n  22.7367  \n   7.88475 \n  -7.47037 \n   0.621035\n  -4.91899 \n -14.9363  \n   8.26901 \n   \u22ee       \n   7.83882 \n  -9.30699 \n -29.7205  \n  15.2482  \n -19.1784  \n  14.9865  \n   2.32728 \n  -9.11988 \n -15.3472  \n  22.9679  \n  -0.997964\n  42.6068  Now set up the inequality constraint for the problem.  bineq   =   zeros ( p )  Aineq   =   -   eye ( p )   100\u00d7100 Array{Float64,2}:\n -1.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -1.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -1.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -1.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -1.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -1.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n  \u22ee                             \u22ee    \u22f1         \u22ee                          \n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -1.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -1.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -1.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -1.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -1.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -1.0  Now we are ready to obtain the solution path using the path algorithm. Here, let's try using different solver  ECOS  for  Convex.jl .   using   ECOS ;   solver = ECOSSolver ( verbose = 0 ,   maxit = 1e8 );  \u03b2\u0302path2 ,   \u03c1path2 ,   =   lsq_classopath ( X ,   y ;   Aineq   =   Aineq ,   bineq   =   bineq ,   solver   =   solver )    \u03b2\u0302path2   100\u00d7183 Array{Float64,2}:\n 0.0         0.0      0.0      0.0      \u2026  0.783939   0.791708   0.796529 \n 0.0         0.0      0.0      0.0         2.17561    2.18099    2.18875  \n 0.0         0.0      0.0      0.0         2.99935    3.008      3.01471  \n 0.0         0.0      0.0      0.0         4.30984    4.31056    4.30849  \n 0.0         0.0      0.0      0.0         4.98995    4.99358    4.9955   \n 0.0         0.0      0.0      0.0      \u2026  6.18666    6.18814    6.18596  \n 0.0         0.0      0.0      0.0         6.92076    6.92371    6.92749  \n 0.0         0.0      0.0      0.0         8.56963    8.55907    8.54642  \n 0.0         0.0      0.0      0.0         8.86323    8.864      8.86137  \n 0.00616069  2.01444  2.41323  2.42264     9.8864     9.89486    9.90491  \n 0.0         0.0      0.0      0.0      \u2026  0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n \u22ee                                      \u22f1  \u22ee                              \n 0.0         0.0      0.0      0.0         0.127693   0.122633   0.114126 \n 0.0         0.0      0.0      0.0         0.257807   0.261246   0.265255 \n 0.0         0.0      0.0      0.0      \u2026  0.294213   0.285664   0.272772 \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0      \u2026  0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0838146  0.0914735  0.0978112\n 0.0         0.0      0.0      0.0         0.200482   0.202642   0.201151 \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0  We plot the solution path below.   plot ( \u03c1path2 ,   \u03b2\u0302path2 ,   label = ,   xaxis   =   ( \u03c1 ,   ( minimum ( \u03c1path2 ), \n       maximum ( \u03c1path2 ))),   yaxis   =   ( \u03b2\u0302(\u03c1) ),   width = 0.5 )   title! ( Simulation 2: Solution Path via Constrained Lasso )", 
            "title": "Non-negativity constraint"
        }, 
        {
            "location": "/demo/prostate/", 
            "text": "Prostate Data\n\n\nThis demonstration solves a regular, unconstrained lasso problem using the constrained lasso solution path (\nlsq_classopath.jl\n).\n\n\nThe \nprostate\n data come from a study that examined the correlation between the level of prostate specific antigen and a number of clinical measures in men who were about to receive a radical prostatectomy. (\nStamey et al. (1989)\n)\n\n\nLet's load and organize the \nprostate\n data. Since we are interested in the following variables as predictors, we extract them and create a design matrix \nXz\n:\n\n\n\n\nlcavol\n : log(cancer volume)\n\n\nlweight\n: log(prostate weight)\n\n\nage\n    : age\n\n\nlbph\n   : log(benign prostatic hyperplasia amount)\n\n\nsvi\n    : seminal vesicle invasion\n\n\nlcp\n    : log(capsular penetration)\n\n\ngleason\n: Gleason score\n\n\npgg45\n  : percentage Gleason scores 4 or 5\n\n\n\n\nThe response variable is \nlpsa\n, which is log(prostate specific antigen). \n\n\nusing\n \nConstrainedLasso\n \n\nprostate\n \n=\n \nreadcsv\n(\ndata/prostate.csv\n,\n \nheader\n=\ntrue\n)\n\n\ntmp\n \n=\n \n[]\n\n\nlabels\n \n=\n \n[\nlcavol\n \nlweight\n \nage\n \nlbph\n \nsvi\n \nlcp\n \ngleason\n \npgg45\n]\n\n\nfor\n \ni\n \nin\n \nlabels\n\n    \npush!\n(\ntmp\n,\n \nfind\n(\nx\n \n-\n \nx\n \n==\n \ni\n,\n \nprostate\n[\n2\n])[\n1\n])\n\n\nend\n\n\nXz\n \n=\n \nArray\n{\nFloat64\n}(\nprostate\n[\n1\n][\n:\n,\n \ntmp\n])\n\n\n\n\n\n\n97\u00d78 Array{Float64,2}:\n -0.579818  2.76946  50.0  -1.38629   0.0  -1.38629   6.0   0.0\n -0.994252  3.31963  58.0  -1.38629   0.0  -1.38629   6.0   0.0\n -0.510826  2.69124  74.0  -1.38629   0.0  -1.38629   7.0  20.0\n -1.20397   3.28279  58.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.751416  3.43237  62.0  -1.38629   0.0  -1.38629   6.0   0.0\n -1.04982   3.22883  50.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.737164  3.47352  64.0   0.615186  0.0  -1.38629   6.0   0.0\n  0.693147  3.53951  58.0   1.53687   0.0  -1.38629   6.0   0.0\n -0.776529  3.53951  47.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.223144  3.24454  63.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.254642  3.60414  65.0  -1.38629   0.0  -1.38629   6.0   0.0\n -1.34707   3.59868  63.0   1.26695   0.0  -1.38629   6.0   0.0\n  1.61343   3.02286  63.0  -1.38629   0.0  -0.597837  7.0  30.0\n  \u22ee                                         \u22ee                  \n  3.30285   3.51898  64.0  -1.38629   1.0   2.32728   7.0  60.0\n  2.02419   3.7317   58.0   1.639     0.0  -1.38629   6.0   0.0\n  1.73166   3.36902  62.0  -1.38629   1.0   0.300105  7.0  30.0\n  2.80759   4.71805  65.0  -1.38629   1.0   2.46385   7.0  60.0\n  1.56235   3.69511  76.0   0.936093  1.0   0.81093   7.0  75.0\n  3.24649   4.10182  68.0  -1.38629   0.0  -1.38629   6.0   0.0\n  2.5329    3.67757  61.0   1.34807   1.0  -1.38629   7.0  15.0\n  2.83027   3.8764   68.0  -1.38629   1.0   1.32176   7.0  60.0\n  3.821     3.89691  44.0  -1.38629   1.0   2.16905   7.0  40.0\n  2.90745   3.39619  52.0  -1.38629   1.0   2.46385   7.0  10.0\n  2.88256   3.77391  68.0   1.55814   1.0   1.55814   7.0  80.0\n  3.47197   3.975    68.0   0.438255  1.0   2.90417   7.0  20.0\n\n\n\n\n\ny\n \n=\n \nArray\n{\nFloat64\n}(\nprostate\n[\n1\n][\n:\n,\n \nend\n-\n1\n])\n\n\n\n\n\n\n97-element Array{Float64,1}:\n -0.430783\n -0.162519\n -0.162519\n -0.162519\n  0.371564\n  0.765468\n  0.765468\n  0.854415\n  1.04732 \n  1.04732 \n  1.26695 \n  1.26695 \n  1.26695 \n  \u22ee       \n  3.63099 \n  3.68009 \n  3.71235 \n  3.98434 \n  3.9936  \n  4.02981 \n  4.12955 \n  4.38515 \n  4.68444 \n  5.14312 \n  5.47751 \n  5.58293\n\n\n\n\n\nFirst we standardize the data by subtracting its mean and dividing by its standard deviation. \n\n\nn\n,\n \np\n \n=\n \nsize\n(\nXz\n)\n\n\nfor\n \ni\n \nin\n \n1\n:\nsize\n(\nXz\n,\n2\n)\n\n    \nXz\n[\n:\n,\n \ni\n]\n \n-=\n \nmean\n(\nXz\n[\n:\n,\n \ni\n])\n\n    \nXz\n[\n:\n,\n \ni\n]\n \n/=\n \nstd\n(\nXz\n[\n:\n,\n \ni\n])\n\n\nend\n\n\nXz\n\n\n\n\n\n\n97\u00d78 Array{Float64,2}:\n -1.63736   -2.00621    -1.86243    \u2026  -0.863171  -1.04216   -0.864467\n -1.98898   -0.722009   -0.787896      -0.863171  -1.04216   -0.864467\n -1.57882   -2.18878     1.36116       -0.863171   0.342627  -0.155348\n -2.16692   -0.807994   -0.787896      -0.863171  -1.04216   -0.864467\n -0.507874  -0.458834   -0.250631      -0.863171  -1.04216   -0.864467\n -2.03613   -0.933955   -1.86243    \u2026  -0.863171  -1.04216   -0.864467\n -0.519967  -0.362793    0.0180011     -0.863171  -1.04216   -0.864467\n -0.557313  -0.208757   -0.787896      -0.863171  -1.04216   -0.864467\n -1.80425   -0.208757   -2.26537       -0.863171  -1.04216   -0.864467\n -0.956085  -0.897266   -0.116315      -0.863171  -1.04216   -0.864467\n -0.92936   -0.0578992   0.152317   \u2026  -0.863171  -1.04216   -0.864467\n -2.28833   -0.0706369  -0.116315      -0.863171  -1.04216   -0.864467\n  0.223498  -1.41472    -0.116315      -0.299282   0.342627   0.199211\n  \u22ee                                 \u22f1   \u22ee                             \n  1.65688   -0.256675    0.0180011  \u2026   1.7927     0.342627   1.26289 \n  0.572009   0.239854   -0.787896      -0.863171  -1.04216   -0.864467\n  0.323806  -0.606718   -0.250631       0.342907   0.342627   0.199211\n  1.23668    2.54221     0.152317       1.89038    0.342627   1.26289 \n  0.180156   0.154448    1.6298         0.70824    0.342627   1.79473 \n  1.60906    1.10379     0.555266   \u2026  -0.863171  -1.04216   -0.864467\n  1.00362    0.113497   -0.384948      -0.863171   0.342627  -0.332628\n  1.25592    0.577607    0.555266       1.07357    0.342627   1.26289 \n  2.09651    0.625489   -2.66832        1.67954    0.342627   0.55377 \n  1.3214    -0.543304   -1.59379        1.89038    0.342627  -0.509907\n  1.30029    0.338384    0.555266   \u2026   1.24263    0.342627   1.97201 \n  1.80037    0.807764    0.555266       2.20528    0.342627  -0.155348\n\n\n\n\n\nNow we solve the problem using solution path algorithm. \n\n\n\u03b2path\n,\n \n\u03c1path\n,\n \n=\n \nlsq_classopath\n(\nXz\n,\n \ny\n);\n\n\n\u03b2path\n\n\n\n\n\n\n8\u00d79 Array{Float64,2}:\n 0.0  0.421131  0.461588   0.557453  \u2026   0.597269    0.602923    0.665147 \n 0.0  0.0       0.0        0.171805      0.232686    0.246293    0.26648  \n 0.0  0.0       0.0        0.0          -0.0598626  -0.0937415  -0.158195 \n 0.0  0.0       0.0        0.0           0.088065    0.108124    0.140311 \n 0.0  0.0       0.0404563  0.182941      0.2436      0.252692    0.315329 \n 0.0  0.0       0.0        0.0       \u2026   0.0         0.0        -0.148286 \n 0.0  0.0       0.0        0.0           0.0         0.0123116   0.0355492\n 0.0  0.0       0.0        0.0           0.0645835   0.0700037   0.12572\n\n\n\n\n\nWe plot the solution path below. \n\n\nusing\n \nPlots\n;\n \npyplot\n();\n \n\ncolors\n \n=\n \n[\n:\ngreen\n \n:\norange\n \n:\nblack\n \n:\npurple\n \n:\nred\n \n:\ngrey\n \n:\nbrown\n \n:\nblue\n]\n \n\nplot\n(\n\u03c1path\n,\n \n\u03b2path\n,\n \nxaxis\n \n=\n \n(\n\u03c1\n,\n \n(\nminimum\n(\n\u03c1path\n),\n\n      \nmaximum\n(\n\u03c1path\n))),\n \nyaxis\n \n=\n \n(\n\u03b2\u0302(\u03c1)\n),\n \nlabel\n=\nlabels\n,\n \ncolor\n=\ncolors\n)\n\n\ntitle!\n(\nProstrate Data: Solution Path via Constrained Lasso\n)\n\n\n\n\n\n\n\n\nBelow, we solve the same problem using \nGLMNet.jl\n package. \n\n\nusing\n \nGLMNet\n;\n  \n\npath\n \n=\n \nglmnet\n(\nXz\n,\n \ny\n,\n \nintercept\n=\nfalse\n);\n\n\npath\n.\nbetas\n\n\n\n\n\n\n8\u00d770 GLMNet.CompressedPredictorMatrix:\n 0.0  0.075317  0.143943  0.206473  \u2026   0.660413   0.660823   0.661201 \n 0.0  0.0       0.0       0.0           0.264962   0.265099   0.265223 \n 0.0  0.0       0.0       0.0          -0.153231  -0.153657  -0.154055 \n 0.0  0.0       0.0       0.0           0.137859   0.138074   0.138272 \n 0.0  0.0       0.0       0.0           0.310435   0.310849   0.311241 \n 0.0  0.0       0.0       0.0       \u2026  -0.136715  -0.137682  -0.138598 \n 0.0  0.0       0.0       0.0           0.033943   0.034123   0.0342689\n 0.0  0.0       0.0       0.0           0.121233   0.121575   0.121916\n\n\n\n\n\nplot\n(\npath\n.\nlambda\n,\n \npath\n.\nbetas\n,\n \ncolor\n=\ncolors\n,\n \nlabel\n=\nlabels\n,\n \n        \nxaxis\n=\n(\n\u03bb\n),\n \nyaxis\n=\n \n(\n\u03b2\u0302(\u03bb)\n))", 
            "title": "Prostate data"
        }, 
        {
            "location": "/demo/prostate/#prostate-data", 
            "text": "This demonstration solves a regular, unconstrained lasso problem using the constrained lasso solution path ( lsq_classopath.jl ).  The  prostate  data come from a study that examined the correlation between the level of prostate specific antigen and a number of clinical measures in men who were about to receive a radical prostatectomy. ( Stamey et al. (1989) )  Let's load and organize the  prostate  data. Since we are interested in the following variables as predictors, we extract them and create a design matrix  Xz :   lcavol  : log(cancer volume)  lweight : log(prostate weight)  age     : age  lbph    : log(benign prostatic hyperplasia amount)  svi     : seminal vesicle invasion  lcp     : log(capsular penetration)  gleason : Gleason score  pgg45   : percentage Gleason scores 4 or 5   The response variable is  lpsa , which is log(prostate specific antigen).   using   ConstrainedLasso   prostate   =   readcsv ( data/prostate.csv ,   header = true )  tmp   =   []  labels   =   [ lcavol   lweight   age   lbph   svi   lcp   gleason   pgg45 ]  for   i   in   labels \n     push! ( tmp ,   find ( x   -   x   ==   i ,   prostate [ 2 ])[ 1 ])  end  Xz   =   Array { Float64 }( prostate [ 1 ][ : ,   tmp ])   97\u00d78 Array{Float64,2}:\n -0.579818  2.76946  50.0  -1.38629   0.0  -1.38629   6.0   0.0\n -0.994252  3.31963  58.0  -1.38629   0.0  -1.38629   6.0   0.0\n -0.510826  2.69124  74.0  -1.38629   0.0  -1.38629   7.0  20.0\n -1.20397   3.28279  58.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.751416  3.43237  62.0  -1.38629   0.0  -1.38629   6.0   0.0\n -1.04982   3.22883  50.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.737164  3.47352  64.0   0.615186  0.0  -1.38629   6.0   0.0\n  0.693147  3.53951  58.0   1.53687   0.0  -1.38629   6.0   0.0\n -0.776529  3.53951  47.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.223144  3.24454  63.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.254642  3.60414  65.0  -1.38629   0.0  -1.38629   6.0   0.0\n -1.34707   3.59868  63.0   1.26695   0.0  -1.38629   6.0   0.0\n  1.61343   3.02286  63.0  -1.38629   0.0  -0.597837  7.0  30.0\n  \u22ee                                         \u22ee                  \n  3.30285   3.51898  64.0  -1.38629   1.0   2.32728   7.0  60.0\n  2.02419   3.7317   58.0   1.639     0.0  -1.38629   6.0   0.0\n  1.73166   3.36902  62.0  -1.38629   1.0   0.300105  7.0  30.0\n  2.80759   4.71805  65.0  -1.38629   1.0   2.46385   7.0  60.0\n  1.56235   3.69511  76.0   0.936093  1.0   0.81093   7.0  75.0\n  3.24649   4.10182  68.0  -1.38629   0.0  -1.38629   6.0   0.0\n  2.5329    3.67757  61.0   1.34807   1.0  -1.38629   7.0  15.0\n  2.83027   3.8764   68.0  -1.38629   1.0   1.32176   7.0  60.0\n  3.821     3.89691  44.0  -1.38629   1.0   2.16905   7.0  40.0\n  2.90745   3.39619  52.0  -1.38629   1.0   2.46385   7.0  10.0\n  2.88256   3.77391  68.0   1.55814   1.0   1.55814   7.0  80.0\n  3.47197   3.975    68.0   0.438255  1.0   2.90417   7.0  20.0  y   =   Array { Float64 }( prostate [ 1 ][ : ,   end - 1 ])   97-element Array{Float64,1}:\n -0.430783\n -0.162519\n -0.162519\n -0.162519\n  0.371564\n  0.765468\n  0.765468\n  0.854415\n  1.04732 \n  1.04732 \n  1.26695 \n  1.26695 \n  1.26695 \n  \u22ee       \n  3.63099 \n  3.68009 \n  3.71235 \n  3.98434 \n  3.9936  \n  4.02981 \n  4.12955 \n  4.38515 \n  4.68444 \n  5.14312 \n  5.47751 \n  5.58293  First we standardize the data by subtracting its mean and dividing by its standard deviation.   n ,   p   =   size ( Xz )  for   i   in   1 : size ( Xz , 2 ) \n     Xz [ : ,   i ]   -=   mean ( Xz [ : ,   i ]) \n     Xz [ : ,   i ]   /=   std ( Xz [ : ,   i ])  end  Xz   97\u00d78 Array{Float64,2}:\n -1.63736   -2.00621    -1.86243    \u2026  -0.863171  -1.04216   -0.864467\n -1.98898   -0.722009   -0.787896      -0.863171  -1.04216   -0.864467\n -1.57882   -2.18878     1.36116       -0.863171   0.342627  -0.155348\n -2.16692   -0.807994   -0.787896      -0.863171  -1.04216   -0.864467\n -0.507874  -0.458834   -0.250631      -0.863171  -1.04216   -0.864467\n -2.03613   -0.933955   -1.86243    \u2026  -0.863171  -1.04216   -0.864467\n -0.519967  -0.362793    0.0180011     -0.863171  -1.04216   -0.864467\n -0.557313  -0.208757   -0.787896      -0.863171  -1.04216   -0.864467\n -1.80425   -0.208757   -2.26537       -0.863171  -1.04216   -0.864467\n -0.956085  -0.897266   -0.116315      -0.863171  -1.04216   -0.864467\n -0.92936   -0.0578992   0.152317   \u2026  -0.863171  -1.04216   -0.864467\n -2.28833   -0.0706369  -0.116315      -0.863171  -1.04216   -0.864467\n  0.223498  -1.41472    -0.116315      -0.299282   0.342627   0.199211\n  \u22ee                                 \u22f1   \u22ee                             \n  1.65688   -0.256675    0.0180011  \u2026   1.7927     0.342627   1.26289 \n  0.572009   0.239854   -0.787896      -0.863171  -1.04216   -0.864467\n  0.323806  -0.606718   -0.250631       0.342907   0.342627   0.199211\n  1.23668    2.54221     0.152317       1.89038    0.342627   1.26289 \n  0.180156   0.154448    1.6298         0.70824    0.342627   1.79473 \n  1.60906    1.10379     0.555266   \u2026  -0.863171  -1.04216   -0.864467\n  1.00362    0.113497   -0.384948      -0.863171   0.342627  -0.332628\n  1.25592    0.577607    0.555266       1.07357    0.342627   1.26289 \n  2.09651    0.625489   -2.66832        1.67954    0.342627   0.55377 \n  1.3214    -0.543304   -1.59379        1.89038    0.342627  -0.509907\n  1.30029    0.338384    0.555266   \u2026   1.24263    0.342627   1.97201 \n  1.80037    0.807764    0.555266       2.20528    0.342627  -0.155348  Now we solve the problem using solution path algorithm.   \u03b2path ,   \u03c1path ,   =   lsq_classopath ( Xz ,   y );  \u03b2path   8\u00d79 Array{Float64,2}:\n 0.0  0.421131  0.461588   0.557453  \u2026   0.597269    0.602923    0.665147 \n 0.0  0.0       0.0        0.171805      0.232686    0.246293    0.26648  \n 0.0  0.0       0.0        0.0          -0.0598626  -0.0937415  -0.158195 \n 0.0  0.0       0.0        0.0           0.088065    0.108124    0.140311 \n 0.0  0.0       0.0404563  0.182941      0.2436      0.252692    0.315329 \n 0.0  0.0       0.0        0.0       \u2026   0.0         0.0        -0.148286 \n 0.0  0.0       0.0        0.0           0.0         0.0123116   0.0355492\n 0.0  0.0       0.0        0.0           0.0645835   0.0700037   0.12572  We plot the solution path below.   using   Plots ;   pyplot ();   colors   =   [ : green   : orange   : black   : purple   : red   : grey   : brown   : blue ]   plot ( \u03c1path ,   \u03b2path ,   xaxis   =   ( \u03c1 ,   ( minimum ( \u03c1path ), \n       maximum ( \u03c1path ))),   yaxis   =   ( \u03b2\u0302(\u03c1) ),   label = labels ,   color = colors )  title! ( Prostrate Data: Solution Path via Constrained Lasso )    Below, we solve the same problem using  GLMNet.jl  package.   using   GLMNet ;    path   =   glmnet ( Xz ,   y ,   intercept = false );  path . betas   8\u00d770 GLMNet.CompressedPredictorMatrix:\n 0.0  0.075317  0.143943  0.206473  \u2026   0.660413   0.660823   0.661201 \n 0.0  0.0       0.0       0.0           0.264962   0.265099   0.265223 \n 0.0  0.0       0.0       0.0          -0.153231  -0.153657  -0.154055 \n 0.0  0.0       0.0       0.0           0.137859   0.138074   0.138272 \n 0.0  0.0       0.0       0.0           0.310435   0.310849   0.311241 \n 0.0  0.0       0.0       0.0       \u2026  -0.136715  -0.137682  -0.138598 \n 0.0  0.0       0.0       0.0           0.033943   0.034123   0.0342689\n 0.0  0.0       0.0       0.0           0.121233   0.121575   0.121916  plot ( path . lambda ,   path . betas ,   color = colors ,   label = labels ,  \n         xaxis = ( \u03bb ),   yaxis =   ( \u03b2\u0302(\u03bb) ))", 
            "title": "Prostate Data"
        }, 
        {
            "location": "/demo/warming/", 
            "text": "Global Warming Data\n\n\nHere we consider the annual data on temperature anomalies. As has been previously noted in the literature on isotonic regression, in general temperature appears to increase monotonically over the time period of 1850 to 2015 (\nWu et al., 2001\n; \nTibshirani et al., 2011\n). This monotonicity can be imposed on the coeffcient estimates using the constrained lasso with the inequality constraint matrix:\n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{1em} \\boldsymbol{C\\beta} \\leq \\boldsymbol{d} \n\\end{split}\n\n\n\n\n\nwhere \n\n\n\n\n\n\\boldsymbol{C} = \\begin{pmatrix} \n1 & -1 &     &        &       &     & \\\\\n  & 1  & -1  &        &         &   & \\\\\n  &    &  1  & -1     &         &   & \\\\\n  &     &       & \\ddots & \\ddots &  & \\\\\n  &     &       &        &          & 1 & -1 \\\\\n\\end{pmatrix}\n\n\n\n\n\nand $\\boldsymbol{d} = \\boldsymbol{0}.$\n\n\nusing\n \nConstrainedLasso\n \n\nusing\n \nECOS\n\n\n\n\n\n\nFirst we load and organize the data. \n\n\nwarming\n \n=\n \nreadcsv\n(\ndata/warming.csv\n,\n \nheader\n=\ntrue\n)[\n1\n]\n\n\nyear\n \n=\n \nwarming\n[\n:\n,\n \n1\n]\n\n\ny\n    \n=\n \nwarming\n[\n:\n,\n \n2\n]\n\n\n\n\n\n\n166-element Array{Float64,1}:\n -0.375\n -0.223\n -0.224\n -0.271\n -0.246\n -0.271\n -0.352\n -0.46 \n -0.466\n -0.286\n -0.346\n -0.409\n -0.522\n  \u22ee    \n  0.45 \n  0.544\n  0.505\n  0.493\n  0.395\n  0.506\n  0.559\n  0.422\n  0.47 \n  0.499\n  0.567\n  0.746\n\n\n\n\n\nn\n \n=\n \np\n \n=\n \nsize\n(\ny\n,\n \n1\n)\n\n\nX\n \n=\n \neye\n(\nn\n)\n\n\n\n\n\n\n166\u00d7166 Array{Float64,2}:\n 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n \u22ee                        \u22ee              \u22f1       \u22ee                        \u22ee  \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  1.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  1.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  1.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n\n\n\n\n\nNow we define inequality constraints as specified earlier. \n\n\nC\n \n=\n \n[\neye\n(\np\n-\n1\n)\n \nzeros\n(\np\n-\n1\n,\n \n1\n)]\n \n-\n \n[\nzeros\n(\np\n-\n1\n,\n \n1\n)\n \neye\n(\np\n-\n1\n)]\n\n\n\n\n\n\n165\u00d7166 Array{Float64,2}:\n 1.0  -1.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   1.0  -1.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   1.0  -1.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   1.0  -1.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   1.0  -1.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   1.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n \u22ee                             \u22ee    \u22f1   \u22ee                             \u22ee  \n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0     -1.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   1.0  -1.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   1.0  -1.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   1.0  -1.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   1.0  -1.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   1.0  -1.0\n\n\n\n\n\nd\n \n=\n \nzeros\n(\nsize\n(\nC\n,\n \n1\n))\n\n\n\n\n\n\n165-element Array{Float64,1}:\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n \u22ee  \n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n\n\n\n\n\nThen we estimate constrained lasso solution path. Here we use \nECOS\n solver rather than the default \nSCS\n solver. \n\n\n\u03b2\u0302path\n,\n \n\u03c1path\n,\n \n=\n \nlsq_classopath\n(\nX\n,\n \ny\n;\n \nAineq\n \n=\n \nC\n,\n \nbineq\n \n=\n \nd\n,\n \nsolver\n \n=\n \nECOSSolver\n(\nverbose\n=\n0\n,\n \nmaxit\n=\n1e8\n));\n \n\n\n\n\n\n\u03b2\u0302path\n\n\n\n\n\n\n166\u00d7198 Array{Float64,2}:\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026  -0.323225  -0.366     -0.375   \n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026  -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026  -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n \u22ee                        \u22ee         \u22f1   \u22ee                             \n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026   0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026   0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0           0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0           0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0           0.447225   0.49       0.499   \n 0.0  0.0  0.0  0.0  0.0  0.0           0.515225   0.558      0.567   \n 0.0  0.0  0.0  0.0  0.0  0.011639  \u2026   0.699138   0.737854   0.746\n\n\n\n\n\nIn this formulation, isotonic regression is a special case of the constrained lasso with $\\rho=0.$ Below, \nmonoreg\n is coefficient estimates obtained using isotonic regression. \n\n\nmonoreg\n \n=\n \nreaddlm\n(\ndata/monoreg.txt\n)\n\n\n\n\n\n\n166\u00d71 Array{Float64,2}:\n -0.375   \n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n  \u22ee       \n  0.484571\n  0.484571\n  0.484571\n  0.484571\n  0.484571\n  0.48925 \n  0.48925 \n  0.48925 \n  0.48925 \n  0.499   \n  0.567   \n  0.746\n\n\n\n\n\nNow let's compare estimates by obtaining the largest absolute difference between isotonic regression constrained lasso fit. \n\n\nmaximum\n(\nabs\n.\n(\nmonoreg\n \n-\n \n\u03b2\u0302path\n[\n:\n,\n \nend\n]))\n\n\n\n\n\n\n1.2212453270876722e-15\n\n\n\n\n\nBelow is a figure that plots the constrained lasso fit at $\\rho = 0$ with the estimates using isotonic regression.\n\n\nusing\n \nPlots\n;\n \npyplot\n();\n \n\nscatter\n(\nyear\n,\n \ny\n,\n \nlabel\n=\nObserved Data\n,\n \nmarkerstrokecolor\n=\ndarkblue\n,\n \n        \nmarkercolor\n=\nwhite\n)\n\n\nscatter!\n(\nyear\n,\n \n\u03b2\u0302path\n[\n:\n,\n \nend\n],\n \nlabel\n=\nClassopath (\u03c1=0)\n,\n \n        \nmarkerstrokecolor\n=\nblack\n,\n \nmarker\n=:\nrect\n,\n \nmarkercolor\n=\nwhite\n)\n\n\nscatter!\n(\nyear\n,\n \nmonoreg\n,\n \nlabel\n=\nIsotonic Regression\n,\n \nmarker\n=:\nx\n,\n\n        \nmarkercolor\n=\nred\n,\n \nmarkersize\n=\n2\n)\n\n\nxaxis!\n(\nYear\n)\n \n\nyaxis!\n(\nTemperature anomalies\n)\n\n\ntitle!\n(\nGlobal Warming Data\n)", 
            "title": "Global warming data"
        }, 
        {
            "location": "/demo/warming/#global-warming-data", 
            "text": "Here we consider the annual data on temperature anomalies. As has been previously noted in the literature on isotonic regression, in general temperature appears to increase monotonically over the time period of 1850 to 2015 ( Wu et al., 2001 ;  Tibshirani et al., 2011 ). This monotonicity can be imposed on the coeffcient estimates using the constrained lasso with the inequality constraint matrix:   \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{1em} \\boldsymbol{C\\beta} \\leq \\boldsymbol{d} \n\\end{split}   where    \n\\boldsymbol{C} = \\begin{pmatrix} \n1 & -1 &     &        &       &     & \\\\\n  & 1  & -1  &        &         &   & \\\\\n  &    &  1  & -1     &         &   & \\\\\n  &     &       & \\ddots & \\ddots &  & \\\\\n  &     &       &        &          & 1 & -1 \\\\\n\\end{pmatrix}   and $\\boldsymbol{d} = \\boldsymbol{0}.$  using   ConstrainedLasso   using   ECOS   First we load and organize the data.   warming   =   readcsv ( data/warming.csv ,   header = true )[ 1 ]  year   =   warming [ : ,   1 ]  y      =   warming [ : ,   2 ]   166-element Array{Float64,1}:\n -0.375\n -0.223\n -0.224\n -0.271\n -0.246\n -0.271\n -0.352\n -0.46 \n -0.466\n -0.286\n -0.346\n -0.409\n -0.522\n  \u22ee    \n  0.45 \n  0.544\n  0.505\n  0.493\n  0.395\n  0.506\n  0.559\n  0.422\n  0.47 \n  0.499\n  0.567\n  0.746  n   =   p   =   size ( y ,   1 )  X   =   eye ( n )   166\u00d7166 Array{Float64,2}:\n 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n \u22ee                        \u22ee              \u22f1       \u22ee                        \u22ee  \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  1.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  1.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  1.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  1.0  Now we define inequality constraints as specified earlier.   C   =   [ eye ( p - 1 )   zeros ( p - 1 ,   1 )]   -   [ zeros ( p - 1 ,   1 )   eye ( p - 1 )]   165\u00d7166 Array{Float64,2}:\n 1.0  -1.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   1.0  -1.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   1.0  -1.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   1.0  -1.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   1.0  -1.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   1.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n \u22ee                             \u22ee    \u22f1   \u22ee                             \u22ee  \n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0     -1.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   1.0  -1.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   1.0  -1.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   1.0  -1.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   1.0  -1.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   1.0  -1.0  d   =   zeros ( size ( C ,   1 ))   165-element Array{Float64,1}:\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n \u22ee  \n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0  Then we estimate constrained lasso solution path. Here we use  ECOS  solver rather than the default  SCS  solver.   \u03b2\u0302path ,   \u03c1path ,   =   lsq_classopath ( X ,   y ;   Aineq   =   C ,   bineq   =   d ,   solver   =   ECOSSolver ( verbose = 0 ,   maxit = 1e8 ));    \u03b2\u0302path   166\u00d7198 Array{Float64,2}:\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026  -0.323225  -0.366     -0.375   \n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026  -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026  -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n \u22ee                        \u22ee         \u22f1   \u22ee                             \n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026   0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026   0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0           0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0           0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0           0.447225   0.49       0.499   \n 0.0  0.0  0.0  0.0  0.0  0.0           0.515225   0.558      0.567   \n 0.0  0.0  0.0  0.0  0.0  0.011639  \u2026   0.699138   0.737854   0.746  In this formulation, isotonic regression is a special case of the constrained lasso with $\\rho=0.$ Below,  monoreg  is coefficient estimates obtained using isotonic regression.   monoreg   =   readdlm ( data/monoreg.txt )   166\u00d71 Array{Float64,2}:\n -0.375   \n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n  \u22ee       \n  0.484571\n  0.484571\n  0.484571\n  0.484571\n  0.484571\n  0.48925 \n  0.48925 \n  0.48925 \n  0.48925 \n  0.499   \n  0.567   \n  0.746  Now let's compare estimates by obtaining the largest absolute difference between isotonic regression constrained lasso fit.   maximum ( abs . ( monoreg   -   \u03b2\u0302path [ : ,   end ]))   1.2212453270876722e-15  Below is a figure that plots the constrained lasso fit at $\\rho = 0$ with the estimates using isotonic regression.  using   Plots ;   pyplot ();   scatter ( year ,   y ,   label = Observed Data ,   markerstrokecolor = darkblue ,  \n         markercolor = white )  scatter! ( year ,   \u03b2\u0302path [ : ,   end ],   label = Classopath (\u03c1=0) ,  \n         markerstrokecolor = black ,   marker =: rect ,   markercolor = white )  scatter! ( year ,   monoreg ,   label = Isotonic Regression ,   marker =: x , \n         markercolor = red ,   markersize = 2 )  xaxis! ( Year )   yaxis! ( Temperature anomalies )  title! ( Global Warming Data )", 
            "title": "Global Warming Data"
        }, 
        {
            "location": "/references/", 
            "text": "References\n\n\n\n\n[1] Altenbuchinger, M., Rehberg, T., Zacharias, H.U., St\u00e4mmler, F., Dettmer, K., Weber, D., Hiergeist, A., Gessner, A., Holler, E., Oefner, P.J. and Spang, R. (2016). Reference point insensitive molecular data analysis. \nBioinformatics\n, 33(2):219-226.\n\n\n[2] Bredel, M., Bredel, C., Juric, D., Harsh, G. R., Vogel, H., Recht, L. D., and Sikic, B. I. (2005). High-resolution genome-wide mapping of genetic alterations in human glial brain tumors. \nCancer Research\n, 65:4088\u20134096.\n\n\n[3] Gaines, B.R. and Zhou, H. (2016). Algorithms for fitting the constrained lasso. arXiv preprint arXiv:1611.01511.\n\n\n[4] Stamey, T.A., Kabalin, J.N., McNeal, J.E., Johnstone, I.M., Freiha, F., Redwine, E.A. and Yang, N. (1989). Prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate: II. radical prostatectomy treated patients. \nJournal of Urology\n 141(5):1076\u20131083.\n\n\n[5] Tibshirani, R. J., Hoefling, H., and Tibshirani, R. (2011). Nearly-isotonic regression. \nTechnometrics\n, 53:54\u201361.\n\n\n[6] Tibshirani, R. and Wang, P. (2008). Spatial smoothing and hot spot detection for CGH data using the fused lasso. \nBiostatistics\n, 9:18\u201329.\n\n\n[7] Weber, D., Oefner, P. J., Hiergeist, A., Koestler, J., Gessner, A., Weber, M., Hahn, J., Wolff, D., Sta \u0308mmler, F., Spang, R., Herr, W., Dettmer, K., and Holler, E. (2015). Low urinary undoxyl sulfate levels early after transplantation reflect a disrupted microbiome and are associated with poor outcome. \nBlood\n, 126:1723\u20131728.\n\n\n[8] Wu, W. B., Woodroofe, M., and Mentz, G. (2001). Isotonic regression: another look at the changepoint problem. \nBiometrika\n, 88:793\u2013804.", 
            "title": "References"
        }, 
        {
            "location": "/references/#references", 
            "text": "[1] Altenbuchinger, M., Rehberg, T., Zacharias, H.U., St\u00e4mmler, F., Dettmer, K., Weber, D., Hiergeist, A., Gessner, A., Holler, E., Oefner, P.J. and Spang, R. (2016). Reference point insensitive molecular data analysis.  Bioinformatics , 33(2):219-226.  [2] Bredel, M., Bredel, C., Juric, D., Harsh, G. R., Vogel, H., Recht, L. D., and Sikic, B. I. (2005). High-resolution genome-wide mapping of genetic alterations in human glial brain tumors.  Cancer Research , 65:4088\u20134096.  [3] Gaines, B.R. and Zhou, H. (2016). Algorithms for fitting the constrained lasso. arXiv preprint arXiv:1611.01511.  [4] Stamey, T.A., Kabalin, J.N., McNeal, J.E., Johnstone, I.M., Freiha, F., Redwine, E.A. and Yang, N. (1989). Prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate: II. radical prostatectomy treated patients.  Journal of Urology  141(5):1076\u20131083.  [5] Tibshirani, R. J., Hoefling, H., and Tibshirani, R. (2011). Nearly-isotonic regression.  Technometrics , 53:54\u201361.  [6] Tibshirani, R. and Wang, P. (2008). Spatial smoothing and hot spot detection for CGH data using the fused lasso.  Biostatistics , 9:18\u201329.  [7] Weber, D., Oefner, P. J., Hiergeist, A., Koestler, J., Gessner, A., Weber, M., Hahn, J., Wolff, D., Sta \u0308mmler, F., Spang, R., Herr, W., Dettmer, K., and Holler, E. (2015). Low urinary undoxyl sulfate levels early after transplantation reflect a disrupted microbiome and are associated with poor outcome.  Blood , 126:1723\u20131728.  [8] Wu, W. B., Woodroofe, M., and Mentz, G. (2001). Isotonic regression: another look at the changepoint problem.  Biometrika , 88:793\u2013804.", 
            "title": "References"
        }
    ]
}