{
    "docs": [
        {
            "location": "/", 
            "text": "ConstrainedLasso\n\n\nConstrainedLasso\n solves the following problem\n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\boldsymbol{\\beta}||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\boldsymbol{A\\beta}=\\boldsymbol{b} \\text{ and } \\boldsymbol{C\\beta} \\leq \\boldsymbol{d}\n\\end{split}\n\n\n\n\n\nwhere\n\n\n\n\n$\\boldsymbol{y} \\in \\mathbb{R}^n$: the response vector\n\n\n$\\boldsymbol{X}\\in \\mathbb{R}^{n\\times p}$: the design matrix of predictor or covariates\n\n\n$\\boldsymbol{\\beta} \\in \\mathbb{R}^p$: the vector of unknown regression coefficients,\n\n\n$\\rho \\geq 0$: a tuning parameter that controls the amount of regularization.\n\n\n\n\n\n\nInstallation\n\n\nWithin Julia, use the package manager to install \nConstrainedLasso\n:\n\n\nPkg\n.\nclone\n(\ngit://github.com/Hua-Zhou/ConstrainedLasso.git\n)\n\n\n\n\n\n\nThis package supports Julia v0.6.\n\n\n\n\nCitation\n\n\nOriginal method paper on the constrained lasso is\n\n\nG.M. James, C. Paulson and P. Rusmevichientong. (2013) Penalized and constrained regression. \nhttp://www-bcf.usc.edu/~gareth/research/PAC.pdf\n\n\nIf you use \nConstrainedLasso\n package in your research, please cite the following paper on the algorithms:\n\n\nB.R. Gaines, H. Zhou. (2016) Algorithms for Fitting the Constrained Lasso. \nhttps://arxiv.org/abs/1611.01511", 
            "title": "Home"
        }, 
        {
            "location": "/#constrainedlasso", 
            "text": "ConstrainedLasso  solves the following problem   \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\boldsymbol{\\beta}||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\boldsymbol{A\\beta}=\\boldsymbol{b} \\text{ and } \\boldsymbol{C\\beta} \\leq \\boldsymbol{d}\n\\end{split}   where   $\\boldsymbol{y} \\in \\mathbb{R}^n$: the response vector  $\\boldsymbol{X}\\in \\mathbb{R}^{n\\times p}$: the design matrix of predictor or covariates  $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$: the vector of unknown regression coefficients,  $\\rho \\geq 0$: a tuning parameter that controls the amount of regularization.", 
            "title": "ConstrainedLasso"
        }, 
        {
            "location": "/#installation", 
            "text": "Within Julia, use the package manager to install  ConstrainedLasso :  Pkg . clone ( git://github.com/Hua-Zhou/ConstrainedLasso.git )   This package supports Julia v0.6.", 
            "title": "Installation"
        }, 
        {
            "location": "/#citation", 
            "text": "Original method paper on the constrained lasso is  G.M. James, C. Paulson and P. Rusmevichientong. (2013) Penalized and constrained regression.  http://www-bcf.usc.edu/~gareth/research/PAC.pdf  If you use  ConstrainedLasso  package in your research, please cite the following paper on the algorithms:  B.R. Gaines, H. Zhou. (2016) Algorithms for Fitting the Constrained Lasso.  https://arxiv.org/abs/1611.01511", 
            "title": "Citation"
        }, 
        {
            "location": "/interface/", 
            "text": "Interface\n\n\nFunctions exported from \nConstrainedLasso\n:\n\n\n#\n\n\nConstrainedLasso.lsq_constrsparsereg\n \n \nFunction\n.\n\n\n  lsq_constrsparsereg(\n      X         :: AbstractMatrix{T},\n      y         :: AbstractVector{T},\n      \u03c1         :: Union{AbstractVector, Number} = zero(T);\n      Aeq       :: AbstractMatrix = zeros(T, 0, size(X, 2)),\n      beq       :: Union{AbstractVector, Number} = zeros(T, size(Aeq, 1)),\n      Aineq     :: AbstractMatrix = zeros(T, 0, size(X, 2)),\n      bineq     :: Union{AbstractVector, number} = zeros(T, size(Aineq, 1)),\n      obswt     :: AbstractVector = ones(T, length(y)),\n      penwt     :: AbstractVector = ones(T, size(X, 2)),\n      warmstart :: Bool = false,\n      solver = ECOSSolver(maxit=10e8, verbose=0)\n    )\n\n\n\n\n\nFit constrained lasso at fixed tuning parameter value(s) by minimizing     \n0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(penwt .* \u03b2)\n subject to linear constraints, using \nConvex.jl\n.\n\n\nArguments\n\n\n\n\nX\n       : predictor matrix.\n\n\ny\n       : response vector.\n\n\n\u03c1\n       : tuning parameter. Can be a number or a list of numbers. Default 0.\n\n\n\n\nOptional arguments\n\n\n\n\nAeq\n       : equality constraint matrix.\n\n\nbeq\n       : equality constraint vector.\n\n\nAineq\n     : inequality constraint matrix.\n\n\nbineq\n     : inequality constraint vector.\n\n\nobswt\n     : observation weights. Default is \n[1 1 1 ... 1]\n.\n\n\npenwt\n     : predictor penalty weights. Default is \n[1 1 1 ... 1]\n.\n\n\nwarmstart\n :\n\n\nsolver\n    : a solver Convex.jl supports. Default is ECOS.             Note that Mosek and Gurobi are more robust than ECOS. Unlike ECOS or             SCS, both Mosek and Gurobi require a license (free for academic             use). \nhttp://convexjl.readthedocs.io/en/latest/solvers.html\n\n\n\n\nReturns\n\n\n\n\n\u03b2\n       : estimated coefficents.\n\n\nobjval\n  : optimal objective value.\n\n\nproblem\n : Convex.jl problem.\n\n\n\n\nsource\n\n\n#\n\n\nConstrainedLasso.lsq_constrsparsereg_admm\n \n \nFunction\n.\n\n\n  lsq_constrsparsereg_admm(\n    X             :: AbstractMatrix{T},\n    y             :: AbstractVector{T},\n    \u03c1             :: Number = zero(T);\n    proj          :: Function = x -\n x,\n    obswt         :: Vector{T} = ones(T, length(y)),\n    penwt         :: Vector{T} = ones(T, size(X, 2)),\n    \u03b20            :: Vector{T} = zeros(T, size(X, 2)),\n    admmmaxite    :: Int = 10000,\n    admmabstol    :: Float64 = 1e-4,\n    admmreltol    :: Float64 = 1e-4,\n    admmscale     :: Float64 = 1 / length(y),\n    admmvaryscale :: Bool = false\n    )\n\n\n\n\n\nFit constrained lasso at a fixed tuning parameter value by applying the alternating direction method of multipliers (ADMM) algorithm.\n\n\nArguments\n\n\n\n\nX\n       : predictor matrix.\n\n\ny\n       : response vector.\n\n\n\u03c1\n       : tuning parameter. Default 0.\n\n\n\n\nOptional arguments\n\n\n\n\nproj\n         : projection onto the constraint set. Default is identity (no constraint).\n\n\nobswt\n        : observation weights. Default is \n[1 1 1 ... 1]\n.\n\n\npenwt\n        : predictor penalty weights. Default is \n[1 1 1 ... 1]\n.\n\n\n\u03b20\n           : starting point.\n\n\nadmmmaxite\n   : maximum number of iterations for ADMM. Default is \n10000\n.\n\n\nadmmabstol\n   : absolute tolerance for ADMM.\n\n\nadmmreltol\n   : relative tolerance for ADMM.\n\n\nadmmscale\n    : ADMM scale parameter. Default is \n1/n\n.\n\n\nadmmvaryscale\n: dynamically chance the ADMM scale parameter. Default is false.\n\n\n\n\nReturns\n\n\n\n\n\u03b2\n       : estimated coefficents.\n\n\n\n\nsource\n\n\n  lsq_constrsparsereg_admm(\n    X             :: AbstractMatrix{T},\n    y             :: AbstractVector{T},\n    \u03c1list         :: Vector;\n    proj          :: Function = x -\n x,\n    obswt         :: Vector{T} = ones(T, length(y)),\n    penwt         :: Vector{T} = ones(T, size(X, 2)),\n    admmmaxite    :: Int = 10000,\n    admmabstol    :: Float64 = 1e-4,\n    admmreltol    :: Float64 = 1e-4,\n    admmscale     :: Float64 = 1 / length(y),\n    admmvaryscale :: Bool = false\n    )\n\n\n\n\n\nFit constrained lasso at fixed tuning parameter values by applying the alternating direction method of multipliers (ADMM) algorithm.\n\n\nArguments\n\n\n\n\nX\n       : predictor matrix.\n\n\ny\n       : response vector.\n\n\n\u03c1list\n   : a vector of tuning parameter values.\n\n\n\n\nOptional arguments\n\n\n\n\nproj\n         : projection onto the constraint set. Default is identity (no constraint).\n\n\nobswt\n        : observation weights. Default is \n[1 1 1 ... 1]\n.\n\n\npenwt\n        : predictor penalty weights. Default is \n[1 1 1 ... 1]\n.\n\n\nadmmmaxite\n   : maximum number of iterations for ADMM. Default is \n10000\n.\n\n\nadmmabstol\n   : absolute tolerance for ADMM.\n\n\nadmmreltol\n   : relative tolerance for ADMM.\n\n\nadmmscale\n    : ADMM scale parameter. Default is \n1/n\n.\n\n\nadmmvaryscale\n: dynamically chance the ADMM scale parameter. Default is false.\n\n\n\n\nReturns\n\n\n\n\n\u03b2path\n       : estimated coefficents along the grid of \n\u03c1\n values.\n\n\n\n\nsource\n\n\n#\n\n\nConstrainedLasso.lsq_classopath\n \n \nFunction\n.\n\n\n  lsq_classopath(\n    X      :: AbstractMatrix{T},\n    y      :: AbstractVector{T};\n    Aeq    :: AbstractMatrix = zeros(T, 0, size(X, 2)),\n    beq    :: Union{AbstractVector, Number} = zeros(T, size(Aeq, 1)),\n    Aineq  :: AbstractMatrix = zeros(T, 0, size(X, 2)),\n    bineq  :: Union{AbstractVector, Number} = zeros(T, size(Aineq, 1)),\n    \u03c1ridge :: Number = zero(T),\n    penidx :: Array{Bool} = fill(true, size(X, 2)),\n    solver = ECOSSolver(maxit=10e8, verbose=0)\n    )\n\n\n\n\n\nCalculate the solution path of the constrained lasso problem that minimizes     \n0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(penwt .* \u03b2)\n subject to linear constraints.\n\n\nArguments\n\n\n\n\nX\n       : predictor matrix.\n\n\ny\n       : response vector.\n\n\n\n\nOptional arguments\n\n\n\n\nAeq\n     : equality constraint matrix.\n\n\nbeq\n     : equality constraint vector.\n\n\nAineq\n   : inequality constraint matrix.\n\n\nbineq\n   : inequality constraint vector.\n\n\n\u03c1ridge\n  : tuning parameter for ridge penalty. Default is 0.\n\n\npenidx\n  : a logical vector indicating penalized coefficients.\n\n\nsolver\n  : a solver Convex.jl supports. Default is ECOS.             Note that Mosek and Gurobi are more robust than ECOS. Unlike ECOS or             SCS, both Mosek and Gurobi require a license (free for academic             use). For details, see \nhttp://convexjl.readthedocs.io/en/latest/solvers.html\n.\n\n\n\n\nExamples\n\n\nSee tutorial examples at \nhttps://hua-zhou.github.io/ConstrainedLasso.jl/latest/demo/path/\n.\n\n\nsource\n\n\n#\n\n\nConstrainedLasso.genlasso\n \n \nFunction\n.\n\n\n  genlasso(\n      X    :: AbstractMatrix{T},\n      y    :: AbstractVector{T};\n      path :: Bool = true,\n      \u03c1    :: Union{AbstractVector, Number} = zero(T),\n      D    :: AbstractMatrix{T} = eye(size(X, 2)),\n      solver = ECOSSolver(maxit=10e8, verbose=0)\n    )\n\n\n\n\n\nSolve generalized lasso problem by reformulating it as constrained lasso problem. Note genralized lasso minimizes     \n0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(D * \u03b2)\n.\n\n\nArguments\n\n\n\n\nX\n       : predictor matrix.\n\n\ny\n       : response vector.\n\n\n\n\nOptional arguments\n\n\n\n\npath\n    : set \npath=false\n if user wishes to supply parameter value(s).             Default is true.\n\n\n\u03c1\n       : tuning parameter value(s). Default is 0.\n\n\nD\n       : penalty matrix. Default is identity matrix.\n\n\nsolver\n  : a solver Convex.jl supports. Default is ECOS.             Note that Mosek and Gurobi are more robust than ECOS. Unlike ECOS or             SCS, both Mosek and Gurobi require a license (free for academic             use). \nhttp://convexjl.readthedocs.io/en/latest/solvers.html\n\n\n\n\nReturns\n\n\n\n\n\u03b2\n       : estimated coefficents.\n\n\nobjval\n  : optimal objective value.\n\n\nproblem\n : Convex.jl problem.\n\n\n\n\nsource\n\n\nPrivate function in \nConstrainedLasso\n:\n\n\n#\n\n\nConstrainedLasso.find_\u03c1max\n \n \nFunction\n.\n\n\n  find_\u03c1max(\n    X      :: AbstractMatrix,\n    y      :: AbstractVector;\n    Aeq    :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n    beq    :: Union{AbstractVector, Number} = zeros(eltype(X), size(Aeq, 1)),\n    Aineq  :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n    bineq  :: Union{AbstractVector, Number} = zeros(eltype(X), size(Aineq, 1)),\n    penidx :: Array{Bool} = fill(true, size(X, 2)),\n    solver = ECOSSolver(maxit=10e8, verbose=0)\n    )\n\n\n\n\n\nFind the maximum tuning parameter value \n\u03c1max\n to kick-start the solution path.\n\n\nsource", 
            "title": "Interface"
        }, 
        {
            "location": "/interface/#interface", 
            "text": "Functions exported from  ConstrainedLasso :  #  ConstrainedLasso.lsq_constrsparsereg     Function .    lsq_constrsparsereg(\n      X         :: AbstractMatrix{T},\n      y         :: AbstractVector{T},\n      \u03c1         :: Union{AbstractVector, Number} = zero(T);\n      Aeq       :: AbstractMatrix = zeros(T, 0, size(X, 2)),\n      beq       :: Union{AbstractVector, Number} = zeros(T, size(Aeq, 1)),\n      Aineq     :: AbstractMatrix = zeros(T, 0, size(X, 2)),\n      bineq     :: Union{AbstractVector, number} = zeros(T, size(Aineq, 1)),\n      obswt     :: AbstractVector = ones(T, length(y)),\n      penwt     :: AbstractVector = ones(T, size(X, 2)),\n      warmstart :: Bool = false,\n      solver = ECOSSolver(maxit=10e8, verbose=0)\n    )  Fit constrained lasso at fixed tuning parameter value(s) by minimizing      0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(penwt .* \u03b2)  subject to linear constraints, using  Convex.jl .  Arguments   X        : predictor matrix.  y        : response vector.  \u03c1        : tuning parameter. Can be a number or a list of numbers. Default 0.   Optional arguments   Aeq        : equality constraint matrix.  beq        : equality constraint vector.  Aineq      : inequality constraint matrix.  bineq      : inequality constraint vector.  obswt      : observation weights. Default is  [1 1 1 ... 1] .  penwt      : predictor penalty weights. Default is  [1 1 1 ... 1] .  warmstart  :  solver     : a solver Convex.jl supports. Default is ECOS.             Note that Mosek and Gurobi are more robust than ECOS. Unlike ECOS or             SCS, both Mosek and Gurobi require a license (free for academic             use).  http://convexjl.readthedocs.io/en/latest/solvers.html   Returns   \u03b2        : estimated coefficents.  objval   : optimal objective value.  problem  : Convex.jl problem.   source  #  ConstrainedLasso.lsq_constrsparsereg_admm     Function .    lsq_constrsparsereg_admm(\n    X             :: AbstractMatrix{T},\n    y             :: AbstractVector{T},\n    \u03c1             :: Number = zero(T);\n    proj          :: Function = x -  x,\n    obswt         :: Vector{T} = ones(T, length(y)),\n    penwt         :: Vector{T} = ones(T, size(X, 2)),\n    \u03b20            :: Vector{T} = zeros(T, size(X, 2)),\n    admmmaxite    :: Int = 10000,\n    admmabstol    :: Float64 = 1e-4,\n    admmreltol    :: Float64 = 1e-4,\n    admmscale     :: Float64 = 1 / length(y),\n    admmvaryscale :: Bool = false\n    )  Fit constrained lasso at a fixed tuning parameter value by applying the alternating direction method of multipliers (ADMM) algorithm.  Arguments   X        : predictor matrix.  y        : response vector.  \u03c1        : tuning parameter. Default 0.   Optional arguments   proj          : projection onto the constraint set. Default is identity (no constraint).  obswt         : observation weights. Default is  [1 1 1 ... 1] .  penwt         : predictor penalty weights. Default is  [1 1 1 ... 1] .  \u03b20            : starting point.  admmmaxite    : maximum number of iterations for ADMM. Default is  10000 .  admmabstol    : absolute tolerance for ADMM.  admmreltol    : relative tolerance for ADMM.  admmscale     : ADMM scale parameter. Default is  1/n .  admmvaryscale : dynamically chance the ADMM scale parameter. Default is false.   Returns   \u03b2        : estimated coefficents.   source    lsq_constrsparsereg_admm(\n    X             :: AbstractMatrix{T},\n    y             :: AbstractVector{T},\n    \u03c1list         :: Vector;\n    proj          :: Function = x -  x,\n    obswt         :: Vector{T} = ones(T, length(y)),\n    penwt         :: Vector{T} = ones(T, size(X, 2)),\n    admmmaxite    :: Int = 10000,\n    admmabstol    :: Float64 = 1e-4,\n    admmreltol    :: Float64 = 1e-4,\n    admmscale     :: Float64 = 1 / length(y),\n    admmvaryscale :: Bool = false\n    )  Fit constrained lasso at fixed tuning parameter values by applying the alternating direction method of multipliers (ADMM) algorithm.  Arguments   X        : predictor matrix.  y        : response vector.  \u03c1list    : a vector of tuning parameter values.   Optional arguments   proj          : projection onto the constraint set. Default is identity (no constraint).  obswt         : observation weights. Default is  [1 1 1 ... 1] .  penwt         : predictor penalty weights. Default is  [1 1 1 ... 1] .  admmmaxite    : maximum number of iterations for ADMM. Default is  10000 .  admmabstol    : absolute tolerance for ADMM.  admmreltol    : relative tolerance for ADMM.  admmscale     : ADMM scale parameter. Default is  1/n .  admmvaryscale : dynamically chance the ADMM scale parameter. Default is false.   Returns   \u03b2path        : estimated coefficents along the grid of  \u03c1  values.   source  #  ConstrainedLasso.lsq_classopath     Function .    lsq_classopath(\n    X      :: AbstractMatrix{T},\n    y      :: AbstractVector{T};\n    Aeq    :: AbstractMatrix = zeros(T, 0, size(X, 2)),\n    beq    :: Union{AbstractVector, Number} = zeros(T, size(Aeq, 1)),\n    Aineq  :: AbstractMatrix = zeros(T, 0, size(X, 2)),\n    bineq  :: Union{AbstractVector, Number} = zeros(T, size(Aineq, 1)),\n    \u03c1ridge :: Number = zero(T),\n    penidx :: Array{Bool} = fill(true, size(X, 2)),\n    solver = ECOSSolver(maxit=10e8, verbose=0)\n    )  Calculate the solution path of the constrained lasso problem that minimizes      0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(penwt .* \u03b2)  subject to linear constraints.  Arguments   X        : predictor matrix.  y        : response vector.   Optional arguments   Aeq      : equality constraint matrix.  beq      : equality constraint vector.  Aineq    : inequality constraint matrix.  bineq    : inequality constraint vector.  \u03c1ridge   : tuning parameter for ridge penalty. Default is 0.  penidx   : a logical vector indicating penalized coefficients.  solver   : a solver Convex.jl supports. Default is ECOS.             Note that Mosek and Gurobi are more robust than ECOS. Unlike ECOS or             SCS, both Mosek and Gurobi require a license (free for academic             use). For details, see  http://convexjl.readthedocs.io/en/latest/solvers.html .   Examples  See tutorial examples at  https://hua-zhou.github.io/ConstrainedLasso.jl/latest/demo/path/ .  source  #  ConstrainedLasso.genlasso     Function .    genlasso(\n      X    :: AbstractMatrix{T},\n      y    :: AbstractVector{T};\n      path :: Bool = true,\n      \u03c1    :: Union{AbstractVector, Number} = zero(T),\n      D    :: AbstractMatrix{T} = eye(size(X, 2)),\n      solver = ECOSSolver(maxit=10e8, verbose=0)\n    )  Solve generalized lasso problem by reformulating it as constrained lasso problem. Note genralized lasso minimizes      0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(D * \u03b2) .  Arguments   X        : predictor matrix.  y        : response vector.   Optional arguments   path     : set  path=false  if user wishes to supply parameter value(s).             Default is true.  \u03c1        : tuning parameter value(s). Default is 0.  D        : penalty matrix. Default is identity matrix.  solver   : a solver Convex.jl supports. Default is ECOS.             Note that Mosek and Gurobi are more robust than ECOS. Unlike ECOS or             SCS, both Mosek and Gurobi require a license (free for academic             use).  http://convexjl.readthedocs.io/en/latest/solvers.html   Returns   \u03b2        : estimated coefficents.  objval   : optimal objective value.  problem  : Convex.jl problem.   source  Private function in  ConstrainedLasso :  #  ConstrainedLasso.find_\u03c1max     Function .    find_\u03c1max(\n    X      :: AbstractMatrix,\n    y      :: AbstractVector;\n    Aeq    :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n    beq    :: Union{AbstractVector, Number} = zeros(eltype(X), size(Aeq, 1)),\n    Aineq  :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n    bineq  :: Union{AbstractVector, Number} = zeros(eltype(X), size(Aineq, 1)),\n    penidx :: Array{Bool} = fill(true, size(X, 2)),\n    solver = ECOSSolver(maxit=10e8, verbose=0)\n    )  Find the maximum tuning parameter value  \u03c1max  to kick-start the solution path.  source", 
            "title": "Interface"
        }, 
        {
            "location": "/demo/fixedparam/", 
            "text": "Optimize at fixed tuning parameter value(s)\n\n\nlsq_constrsparsereg.jl\n fits constrained lasso\n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac{1}{2}||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\boldsymbol{A\\beta}=\\boldsymbol{b} \\text{ and } \\boldsymbol{C\\beta} \\leq \\boldsymbol{d}\n\\end{split}\n\n\n\n\n\nat a fixed tuning parameter value $\\rho$ or several tuning parameter values provided by user.\n\n\n\n\nSingle tuning parameter value\n\n\nWe demonstrate the usage of a single tuning parameter value with a sum-to-zero constraint example defined as \n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac{1}{2}||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\sum_j \\beta_j = 0.\n\\end{split}\n\n\n\n\n\nFirst, let's define a true parameter \n\u03b2\n such that \nsum(\u03b2) = 0\n.\n\n\nusing\n \nConstrainedLasso\n,\n \nBase\n.\nTest\n\n\n\n\n\n\nn\n,\n \np\n \n=\n \n50\n,\n \n100\n  \n\n\u03b2\n \n=\n \nzeros\n(\np\n)\n\n\n\u03b2\n[\n1\n:\nround\n(\nInt\n,\n \np\n \n/\n \n4\n)]\n \n=\n \n0\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \np\n \n/\n \n4\n)\n \n+\n \n1\n)\n:\nround\n(\nInt\n,\n \np\n \n/\n \n2\n)]\n \n=\n \n1\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \np\n \n/\n \n2\n)\n \n+\n \n1\n)\n:\nround\n(\nInt\n,\n \n3\np\n \n/\n \n4\n)]\n \n=\n \n0\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \n3\np\n \n/\n \n4\n)\n \n+\n \n1\n)\n:\np\n]\n \n=\n \n-\n1\n\n\n\u03b2\n\n\n\n\n\n\n100-element Array{Float64,1}:\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  \u22ee  \n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n\n\n\n\n\nNext we generate data based on the true parameter \n\u03b2\n.\n\n\nsrand\n(\n41\n)\n\n\nX\n \n=\n \nrandn\n(\nn\n,\n \np\n)\n\n\n\n\n\n\n50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627\n\n\n\n\n\ny\n \n=\n \nX\n \n*\n \n\u03b2\n \n+\n \nrandn\n(\nn\n)\n\n\n\n\n\n\n50-element Array{Float64,1}:\n  -9.90585 \n  -5.40562 \n   5.24289 \n  -6.29951 \n  -4.9586  \n  -6.1342  \n  -7.90981 \n   2.51009 \n  -5.79548 \n   1.61355 \n  -0.722766\n  10.4522  \n   4.03935 \n   \u22ee       \n   0.397781\n  -2.6661  \n   5.36896 \n  -3.56537 \n  -2.402   \n   0.11478 \n  -5.39248 \n   4.38391 \n   0.706801\n -10.1066  \n  -1.12558 \n  14.2473\n\n\n\n\n\nSince the equality constraint can be written as \n\n\n\n\n\n\\begin{split}\n\\begin{pmatrix} 1 & 1 & \\cdots & 1 \\end{pmatrix} \\beta = 0,\n\\end{split}\n\n\n\n\n\nwe define the constraint as below.\n\n\nbeq\n   \n=\n \n0.0\n\n\nAeq\n   \n=\n \nones\n(\n1\n,\n \np\n)\n\n\n\n\n\n\n1\u00d7100 Array{Float64,2}:\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \u2026  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n\n\n\n\n\nNow we are ready to fit the constrained lasso problem, say at \n\u03c1=10\n.\n\n\n\u03c1\n \n=\n \n10.0\n\n\n\u03b2\u0302\n,\n \n=\n \nlsq_constrsparsereg\n(\nX\n,\n \ny\n,\n \n\u03c1\n;\n \nAeq\n \n=\n \nAeq\n,\n \nbeq\n \n=\n \nbeq\n);\n\n\n\n\n\n\n\u03b2\u0302\n\n\n\n\n\n\n100\u00d71 Array{Float64,2}:\n  0.0332021  \n  3.10012e-10\n -4.6978e-10 \n -0.39502    \n -2.56803e-10\n -5.22472e-11\n -5.31822e-10\n -4.24518e-11\n  0.288476   \n -6.96174e-10\n -1.18309e-10\n -4.8233e-11 \n -6.46568e-11\n  \u22ee          \n -0.188338   \n -2.37826e-9 \n -1.60819e-9 \n  1.41678e-10\n -0.84392    \n -0.645577   \n -0.257303   \n -1.43023e-10\n -1.13316    \n  4.26293e-10\n -3.84653e-10\n  2.17566e-9\n\n\n\n\n\nWe see if the sum of estimated $\\beta$ coefficients is approximately equal to 0.\n\n\n@test\n \nsum\n(\n\u03b2\u0302\n)\n\u22480\n.\n0\n \natol\n=\n1e-6\n\n\n\n\n\n\n\u001b[1m\u001b[32mTest Passed\n\u001b[39m\u001b[22m\n\n\n\n\n\n\n\nMultiple tuning parameter values\n\n\nDefine \n\u03c1list\n to be a decreasing sequence of values from 152.0 to 2.0.\n\n\n\u03c1list\n \n=\n \n152.0\n:-\n15.0\n:\n2.0\n\n\n\n\n\n\n152.0\n:-\n15.0\n:\n2.0\n\n\n\n\n\n\nUsing the same equality constraints, we fit the constrained lasso.\n\n\n\u03b2\u0302\n,\n \n=\n \nlsq_constrsparsereg\n(\nX\n,\n \ny\n,\n \n\u03c1list\n;\n \nAeq\n \n=\n \nAeq\n,\n \nbeq\n \n=\n \nbeq\n);\n\n\n\n\n\n\n\u03b2\u0302\n\n\n\n\n\n\n100\u00d711 Array{Float64,2}:\n  5.7885e-8    9.72387e-9    2.81065e-10  \u2026   2.48441e-8    1.51451e-8 \n -5.87852e-8  -1.31342e-8   -3.79759e-10     -4.08278e-10   0.178717   \n -3.38004e-8  -7.55024e-9   -2.09421e-10     -5.52919e-9   -1.02944e-9 \n -8.00996e-8  -1.8626e-8    -6.07662e-10     -0.330981     -0.414043   \n -1.46534e-7  -4.13375e-8   -1.40985e-9      -5.87274e-9   -3.33221e-10\n  1.35681e-7   2.5494e-8     8.8654e-10   \u2026   1.77541e-9   -4.14188e-10\n -7.15503e-8  -1.62251e-8   -4.70464e-10     -9.09917e-9    3.28018e-11\n -3.43517e-8  -7.81034e-9   -2.37254e-10     -1.42302e-9    8.38037e-11\n  9.96051e-8   1.79582e-8    6.78349e-10      0.205806      0.335375   \n -9.85721e-9  -2.84963e-9   -6.60348e-11     -4.06107e-9   -0.157908   \n -5.23226e-8  -1.19088e-8   -3.45603e-10  \u2026  -5.93287e-10  -2.44739e-9 \n  7.14349e-9   4.03781e-10  -2.60167e-11     -4.83948e-10  -8.79003e-10\n -1.71432e-8  -4.23818e-9   -1.15154e-10     -3.84297e-10  -8.17787e-10\n  \u22ee                                       \u22f1                 \u22ee          \n -4.47748e-8  -9.81256e-9   -2.58362e-10     -0.137195     -0.391635   \n -1.48721e-8  -3.93589e-9   -1.06969e-10     -6.58309e-9   -0.33352    \n -2.15687e-8  -5.18704e-9   -1.69607e-10  \u2026  -1.82448e-8   -5.51459e-9 \n -1.74035e-8  -4.39562e-9   -1.63827e-10      2.06069e-9    1.09637e-9 \n -4.3585e-7   -0.0198435    -0.152322        -0.914601     -0.867639   \n -1.40812e-7  -3.82869e-8   -1.1339e-9       -0.218698     -0.999583   \n -7.48206e-8  -1.72154e-8   -4.88002e-10     -0.237392     -0.577743   \n -3.60962e-8  -8.21917e-9   -2.17684e-10  \u2026  -8.88674e-10  -8.93601e-10\n -2.73163e-7  -1.44347e-7   -1.31485e-8      -1.05679      -1.16862    \n  2.52674e-8   3.8825e-9     1.33482e-10      1.82138e-9    2.29367e-9 \n -4.49981e-8  -9.74654e-9   -2.59147e-10     -6.95863e-9   -1.52035e-9 \n  4.50404e-8   7.3066e-9     2.16334e-10      1.6277e-8     2.71896e-9\n\n\n\n\n\nusing\n \nPlots\n;\n \npyplot\n();\n\n\nplot\n(\n\u03c1list\n,\n \n\u03b2\u0302\n,\n \nlabel\n=\n,\n \nxaxis\n \n=\n \n(\n\u03c1\n,\n \n(\nminimum\n(\n\u03c1list\n),\n\n      \nmaximum\n(\n\u03c1list\n))),\n \nyaxis\n \n=\n \n(\n\u03b2\u0302(\u03c1)\n),\n \nwidth\n=\n0.5\n)\n \n\ntitle!\n(\nSolution Path at Fixed Parameter Values\n)\n \n\n\n\n\n\nsavefig\n(\nmisc/fixed.svg\n)\n\n\n\n\n\n\n\n\nFollow the \nlink\n to access the .ipynb file of this page.", 
            "title": "Fixed Tuning Parameter Value"
        }, 
        {
            "location": "/demo/fixedparam/#optimize-at-fixed-tuning-parameter-values", 
            "text": "lsq_constrsparsereg.jl  fits constrained lasso   \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac{1}{2}||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\boldsymbol{A\\beta}=\\boldsymbol{b} \\text{ and } \\boldsymbol{C\\beta} \\leq \\boldsymbol{d}\n\\end{split}   at a fixed tuning parameter value $\\rho$ or several tuning parameter values provided by user.", 
            "title": "Optimize at fixed tuning parameter value(s)"
        }, 
        {
            "location": "/demo/fixedparam/#single-tuning-parameter-value", 
            "text": "We demonstrate the usage of a single tuning parameter value with a sum-to-zero constraint example defined as    \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac{1}{2}||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\sum_j \\beta_j = 0.\n\\end{split}   First, let's define a true parameter  \u03b2  such that  sum(\u03b2) = 0 .  using   ConstrainedLasso ,   Base . Test   n ,   p   =   50 ,   100    \u03b2   =   zeros ( p )  \u03b2 [ 1 : round ( Int ,   p   /   4 )]   =   0  \u03b2 [( round ( Int ,   p   /   4 )   +   1 ) : round ( Int ,   p   /   2 )]   =   1  \u03b2 [( round ( Int ,   p   /   2 )   +   1 ) : round ( Int ,   3 p   /   4 )]   =   0  \u03b2 [( round ( Int ,   3 p   /   4 )   +   1 ) : p ]   =   - 1  \u03b2   100-element Array{Float64,1}:\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  \u22ee  \n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0  Next we generate data based on the true parameter  \u03b2 .  srand ( 41 )  X   =   randn ( n ,   p )   50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627  y   =   X   *   \u03b2   +   randn ( n )   50-element Array{Float64,1}:\n  -9.90585 \n  -5.40562 \n   5.24289 \n  -6.29951 \n  -4.9586  \n  -6.1342  \n  -7.90981 \n   2.51009 \n  -5.79548 \n   1.61355 \n  -0.722766\n  10.4522  \n   4.03935 \n   \u22ee       \n   0.397781\n  -2.6661  \n   5.36896 \n  -3.56537 \n  -2.402   \n   0.11478 \n  -5.39248 \n   4.38391 \n   0.706801\n -10.1066  \n  -1.12558 \n  14.2473  Since the equality constraint can be written as    \n\\begin{split}\n\\begin{pmatrix} 1 & 1 & \\cdots & 1 \\end{pmatrix} \\beta = 0,\n\\end{split}   we define the constraint as below.  beq     =   0.0  Aeq     =   ones ( 1 ,   p )   1\u00d7100 Array{Float64,2}:\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \u2026  1.0  1.0  1.0  1.0  1.0  1.0  1.0  Now we are ready to fit the constrained lasso problem, say at  \u03c1=10 .  \u03c1   =   10.0  \u03b2\u0302 ,   =   lsq_constrsparsereg ( X ,   y ,   \u03c1 ;   Aeq   =   Aeq ,   beq   =   beq );   \u03b2\u0302   100\u00d71 Array{Float64,2}:\n  0.0332021  \n  3.10012e-10\n -4.6978e-10 \n -0.39502    \n -2.56803e-10\n -5.22472e-11\n -5.31822e-10\n -4.24518e-11\n  0.288476   \n -6.96174e-10\n -1.18309e-10\n -4.8233e-11 \n -6.46568e-11\n  \u22ee          \n -0.188338   \n -2.37826e-9 \n -1.60819e-9 \n  1.41678e-10\n -0.84392    \n -0.645577   \n -0.257303   \n -1.43023e-10\n -1.13316    \n  4.26293e-10\n -3.84653e-10\n  2.17566e-9  We see if the sum of estimated $\\beta$ coefficients is approximately equal to 0.  @test   sum ( \u03b2\u0302 ) \u22480 . 0   atol = 1e-6   \u001b[1m\u001b[32mTest Passed\n\u001b[39m\u001b[22m", 
            "title": "Single tuning parameter value"
        }, 
        {
            "location": "/demo/fixedparam/#multiple-tuning-parameter-values", 
            "text": "Define  \u03c1list  to be a decreasing sequence of values from 152.0 to 2.0.  \u03c1list   =   152.0 :- 15.0 : 2.0   152.0 :- 15.0 : 2.0   Using the same equality constraints, we fit the constrained lasso.  \u03b2\u0302 ,   =   lsq_constrsparsereg ( X ,   y ,   \u03c1list ;   Aeq   =   Aeq ,   beq   =   beq );   \u03b2\u0302   100\u00d711 Array{Float64,2}:\n  5.7885e-8    9.72387e-9    2.81065e-10  \u2026   2.48441e-8    1.51451e-8 \n -5.87852e-8  -1.31342e-8   -3.79759e-10     -4.08278e-10   0.178717   \n -3.38004e-8  -7.55024e-9   -2.09421e-10     -5.52919e-9   -1.02944e-9 \n -8.00996e-8  -1.8626e-8    -6.07662e-10     -0.330981     -0.414043   \n -1.46534e-7  -4.13375e-8   -1.40985e-9      -5.87274e-9   -3.33221e-10\n  1.35681e-7   2.5494e-8     8.8654e-10   \u2026   1.77541e-9   -4.14188e-10\n -7.15503e-8  -1.62251e-8   -4.70464e-10     -9.09917e-9    3.28018e-11\n -3.43517e-8  -7.81034e-9   -2.37254e-10     -1.42302e-9    8.38037e-11\n  9.96051e-8   1.79582e-8    6.78349e-10      0.205806      0.335375   \n -9.85721e-9  -2.84963e-9   -6.60348e-11     -4.06107e-9   -0.157908   \n -5.23226e-8  -1.19088e-8   -3.45603e-10  \u2026  -5.93287e-10  -2.44739e-9 \n  7.14349e-9   4.03781e-10  -2.60167e-11     -4.83948e-10  -8.79003e-10\n -1.71432e-8  -4.23818e-9   -1.15154e-10     -3.84297e-10  -8.17787e-10\n  \u22ee                                       \u22f1                 \u22ee          \n -4.47748e-8  -9.81256e-9   -2.58362e-10     -0.137195     -0.391635   \n -1.48721e-8  -3.93589e-9   -1.06969e-10     -6.58309e-9   -0.33352    \n -2.15687e-8  -5.18704e-9   -1.69607e-10  \u2026  -1.82448e-8   -5.51459e-9 \n -1.74035e-8  -4.39562e-9   -1.63827e-10      2.06069e-9    1.09637e-9 \n -4.3585e-7   -0.0198435    -0.152322        -0.914601     -0.867639   \n -1.40812e-7  -3.82869e-8   -1.1339e-9       -0.218698     -0.999583   \n -7.48206e-8  -1.72154e-8   -4.88002e-10     -0.237392     -0.577743   \n -3.60962e-8  -8.21917e-9   -2.17684e-10  \u2026  -8.88674e-10  -8.93601e-10\n -2.73163e-7  -1.44347e-7   -1.31485e-8      -1.05679      -1.16862    \n  2.52674e-8   3.8825e-9     1.33482e-10      1.82138e-9    2.29367e-9 \n -4.49981e-8  -9.74654e-9   -2.59147e-10     -6.95863e-9   -1.52035e-9 \n  4.50404e-8   7.3066e-9     2.16334e-10      1.6277e-8     2.71896e-9  using   Plots ;   pyplot ();  plot ( \u03c1list ,   \u03b2\u0302 ,   label = ,   xaxis   =   ( \u03c1 ,   ( minimum ( \u03c1list ), \n       maximum ( \u03c1list ))),   yaxis   =   ( \u03b2\u0302(\u03c1) ),   width = 0.5 )   title! ( Solution Path at Fixed Parameter Values )    savefig ( misc/fixed.svg )    Follow the  link  to access the .ipynb file of this page.", 
            "title": "Multiple tuning parameter values"
        }, 
        {
            "location": "/demo/admm/", 
            "text": "ADMM\n\n\nIn this section, we solve the same constrained lasso problem using the alternating direction method of multipliers (ADMM) algorithm. ADMM algorithm is advantageous since it can scale to larger size problems and is not restricted to linear constraints. See \nGaines and Zhou (2016)\n for details. \n\n\nIn order to use the algorithm, user needs to supply a projection function onto the constraint set. Some simple examples are discussed below. \n\n\n\n\nsum-to-zero constraint\n\n\nWe demonstrate using a sum-to-zero constraint example\n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac{1}{2}||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\sum_j \\beta_j = 0\n\\end{split}\n\n\n\n\n\nFor this constraint, the appropriate projection function for $\\boldsymbol{x} = (x_1, \\cdots, x_p)^T$ would be \n\n\n\n\n\n\\text{proj}(\\boldsymbol{x}) = \\boldsymbol{x} - \\bar{\\boldsymbol{x}}= \\boldsymbol{x} - \\sum_{j=1}^p x_j.\n\n\n\n\n\nFirst, let's define a true parameter \n\u03b2\n such that \nsum(\u03b2) = 0\n.\n\n\nusing\n \nConstrainedLasso\n,\n \nBase\n.\nTest\n\n\n\n\n\n\nn\n,\n \np\n \n=\n \n50\n,\n \n100\n  \n\n\u03b2\n \n=\n \nzeros\n(\np\n);\n\n\n\u03b2\n[\n1\n:\nround\n(\nInt\n,\n \np\n \n/\n \n4\n)]\n \n=\n \n0\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \np\n \n/\n \n4\n)\n \n+\n \n1\n)\n:\nround\n(\nInt\n,\n \np\n \n/\n \n2\n)]\n \n=\n \n1\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \np\n \n/\n \n2\n)\n \n+\n \n1\n)\n:\nround\n(\nInt\n,\n \n3\np\n \n/\n \n4\n)]\n \n=\n \n0\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \n3\np\n \n/\n \n4\n)\n \n+\n \n1\n)\n:\np\n]\n \n=\n \n-\n1\n\n\n\u03b2\n\n\n\n\n\n\n100-element Array{Float64,1}:\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  \u22ee  \n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n\n\n\n\n\nNext we generate data based on the true parameter \u03b2.\n\n\nsrand\n(\n41\n)\n\n\nX\n \n=\n \nrandn\n(\nn\n,\n \np\n)\n\n\n\n\n\n\n50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627\n\n\n\n\n\ny\n \n=\n \nX\n \n*\n \n\u03b2\n \n+\n \nrandn\n(\nn\n)\n\n\n\n\n\n\n50-element Array{Float64,1}:\n  -9.90585 \n  -5.40562 \n   5.24289 \n  -6.29951 \n  -4.9586  \n  -6.1342  \n  -7.90981 \n   2.51009 \n  -5.79548 \n   1.61355 \n  -0.722766\n  10.4522  \n   4.03935 \n   \u22ee       \n   0.397781\n  -2.6661  \n   5.36896 \n  -3.56537 \n  -2.402   \n   0.11478 \n  -5.39248 \n   4.38391 \n   0.706801\n -10.1066  \n  -1.12558 \n  14.2473\n\n\n\n\n\nNow we estimate coefficients at fixed tuning parameter value using ADMM alogrithm. \n\n\n\u03c1\n \n=\n \n2.0\n\n\n\u03b2\u0302admm\n \n=\n \nlsq_constrsparsereg_admm\n(\nX\n,\n \ny\n,\n \n\u03c1\n;\n \nproj\n \n=\n \nx\n \n-\n \nx\n \n-\n \nmean\n(\nx\n))\n\n\n\n\n\n\n\u03b2\u0302admm\n\n\n\n\n\n\n100-element Array{Float64,1}:\n  0.0     \n  0.174344\n  0.0     \n -0.421288\n  0.0     \n  0.0     \n  0.0     \n  0.0     \n  0.324233\n -0.15384 \n  0.0     \n  0.0     \n  0.0     \n  \u22ee       \n -0.397027\n -0.32079 \n  0.0     \n  0.0     \n -0.868508\n -0.992272\n -0.571755\n  0.0     \n -1.16568 \n  0.0     \n  0.0     \n  0.0\n\n\n\n\n\nNow let's compare the estimated coefficients with those obtained using quadratic programming. \n\n\n\u03c1\n \n=\n \n2.0\n \n\nbeq\n \n=\n \n[\n0.0\n]\n\n\nAeq\n \n=\n \nones\n(\n1\n,\n \np\n)\n\n\n\u03b2\u0302\n,\n \n=\n \nlsq_constrsparsereg\n(\nX\n,\n \ny\n,\n \n\u03c1\n;\n \nAeq\n \n=\n \nAeq\n,\n \nbeq\n \n=\n \nbeq\n)\n \n\n\n\n\n\nhcat\n(\n\u03b2\u0302admm\n,\n \n\u03b2\u0302\n)\n\n\n\n\n\n\n100\u00d72 Array{Float64,2}:\n  0.0        1.51451e-8 \n  0.174344   0.178717   \n  0.0       -1.02944e-9 \n -0.421288  -0.414043   \n  0.0       -3.33221e-10\n  0.0       -4.14188e-10\n  0.0        3.28018e-11\n  0.0        8.38037e-11\n  0.324233   0.335375   \n -0.15384   -0.157908   \n  0.0       -2.44739e-9 \n  0.0       -8.79003e-10\n  0.0       -8.17787e-10\n  \u22ee                     \n -0.397027  -0.391635   \n -0.32079   -0.33352    \n  0.0       -5.51459e-9 \n  0.0        1.09637e-9 \n -0.868508  -0.867639   \n -0.992272  -0.999583   \n -0.571755  -0.577743   \n  0.0       -8.93601e-10\n -1.16568   -1.16862    \n  0.0        2.29367e-9 \n  0.0       -1.52035e-9 \n  0.0        2.71896e-9\n\n\n\n\n\n\n\nNon-negativity constraint\n\n\nHere we look at the non-negativity constraint. For the non-negativity constraint \n\n\n\n\n\n\\beta_j \\geq 0 \\hspace{0.8em} \\forall j,\n\n\n\n\n\nthe appropriate projection function for $\\boldsymbol{x} = (x_1, \\cdots, x_p)^T$ is \n\n\n\n\n\n\\text{proj}(\\boldsymbol{x})_j = \\begin{cases}\nx_j & \\text{if } x_j \\geq 0 \\\\ \n0 & \\text{else} \\end{cases} \\hspace{0.8em} \\text{where } j = 1, \\cdots, p.\n\n\n\n\n\nNow let's generate \nX\n and \ny\n.\n\n\nn\n,\n \np\n \n=\n \n50\n,\n \n100\n   \n\n\u03b2\n \n=\n \nzeros\n(\np\n)\n\n\n\u03b2\n[\n1\n:\n10\n]\n \n=\n \n1\n:\n10\n\n\nsrand\n(\n41\n)\n\n\nX\n \n=\n \nrandn\n(\nn\n,\n \np\n)\n\n\n\n\n\n\n50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627\n\n\n\n\n\ny\n \n=\n \nX\n \n*\n \n\u03b2\n \n+\n \nrandn\n(\nn\n)\n\n\n\n\n\n\n50-element Array{Float64,1}:\n  12.6173  \n  40.3776  \n   2.2169  \n  27.4631  \n  38.592   \n   7.82023 \n  22.7367  \n   7.88475 \n  -7.47037 \n   0.621035\n  -4.91899 \n -14.9363  \n   8.26901 \n   \u22ee       \n   7.83882 \n  -9.30699 \n -29.7205  \n  15.2482  \n -19.1784  \n  14.9865  \n   2.32728 \n  -9.11988 \n -15.3472  \n  22.9679  \n  -0.997964\n  42.6068\n\n\n\n\n\nThis time, we feed a sequence of tuning parameter values into the \nlsq_constrsparsereg_admm\n function. \n\n\n\u03c1\n \n=\n \nlinspace\n(\n1.0\n,\n \n70.0\n,\n \n10\n)\n\n\n\u03b2\u0302admm\n \n=\n \nlsq_constrsparsereg_admm\n(\nX\n,\n \ny\n,\n \n\u03c1\n;\n \nproj\n \n=\n \nx\n \n-\n \nclamp\n.\n(\nx\n,\n \n0\n,\n \nInf\n))\n\n\n\n\n\n\n\u03b2\u0302admm\n\n\n\n\n\n\n100\u00d710 Array{Float64,2}:\n  0.653843    0.490395    0.277812    \u2026  0.0      0.0      0.0      0.0    \n  2.21572     2.07051     1.96482        1.41607  1.26061  1.10519  0.94974\n  2.77145     2.52934     2.36727        1.77875  1.64644  1.51417  1.38186\n  4.13883     3.90272     3.70409        2.99234  2.82268  2.65297  2.4833 \n  4.84074     4.56198     4.29935        3.31819  3.06715  2.81613  2.5651 \n  5.8787      5.69235     5.49622     \u2026  4.46134  4.18054  3.89969  3.61888\n  6.7637      6.55971     6.34682        5.68324  5.54216  5.40096  5.25987\n  8.51065     8.03277     7.8351         6.87417  6.61797  6.36169  6.10549\n  8.7451      8.33258     8.02948        6.94937  6.68847  6.42749  6.16659\n  9.77876     9.76788     9.6326         9.12581  9.02118  8.91651  8.81187\n  1.65481e-6  2.65614e-5  5.75666e-5  \u2026  0.0      0.0      0.0      0.0    \n  0.0         0.0         0.0            0.0      0.0      0.0      0.0    \n  0.0         0.0         0.0            0.0      0.0      0.0      0.0    \n  \u22ee                                   \u22f1                                    \n  0.170999    1.06392e-5  0.0            0.0      0.0      0.0      0.0    \n  0.0         0.0         0.0            0.0      0.0      0.0      0.0    \n  0.0         0.0         0.0         \u2026  0.0      0.0      0.0      0.0    \n -5.9382e-7   0.0         0.0            0.0      0.0      0.0      0.0    \n  0.0         2.57367e-5  3.98409e-5     0.0      0.0      0.0      0.0    \n  0.0         0.0         0.0            0.0      0.0      0.0      0.0    \n  0.0         1.69146e-5  0.0            0.0      0.0      0.0      0.0    \n  0.0         0.0         0.0         \u2026  0.0      0.0      0.0      0.0    \n  0.0         0.0         3.45786e-5     0.0      0.0      0.0      0.0    \n  0.0141037   0.0         0.0            0.0      0.0      0.0      0.0    \n  0.0         2.39758e-5  3.62464e-5     0.0      0.0      0.0      0.0    \n  0.0503145   0.0553623   0.0339917      0.0      0.0      0.0      0.0\n\n\n\n\n\nAgain we compare with the estimates from quadratic programming.\n\n\nbineq\n \n=\n \nzeros\n(\np\n)\n\n\nAineq\n \n=\n \n-\n \neye\n(\np\n)\n\n\n\u03b2\u0302\n,\n \n=\n \nlsq_constrsparsereg\n(\nX\n,\n \ny\n,\n \n\u03c1\n;\n \nAineq\n \n=\n \nAineq\n,\n \nbineq\n \n=\n \nbineq\n)\n \n\n\n\n\n\n\u03b2\u0302\n\n\n\n\n\n\n100\u00d710 Array{Float64,2}:\n  0.652004      0.489105      0.277863     \u2026   2.00626e-9    8.58265e-9 \n  2.2179        2.07061       1.9648           1.105         0.949582   \n  2.77457       2.52826       2.36721          1.51391       1.38162    \n  4.13788       3.9009        3.70426          2.65333       2.48359    \n  4.84588       4.56136       4.29938          2.81596       2.56492    \n  5.87904       5.69159       5.49619      \u2026   3.89993       3.61905    \n  6.75881       6.55723       6.34731          5.40177       5.26053    \n  8.51202       8.03199       7.83528          6.36216       6.10583    \n  8.74756       8.32936       8.02991          6.42804       6.16702    \n  9.77692       9.76675       9.63278          8.91676       8.81211    \n -2.48584e-10  -2.61442e-10  -2.8615e-10   \u2026  -9.64613e-11  -7.2806e-10 \n -1.30999e-10  -3.24647e-10  -3.6769e-10      -1.54375e-10  -8.53117e-10\n -1.68134e-10   3.14662e-10   6.39206e-10      6.38157e-10   4.18343e-9 \n  \u22ee                                        \u22f1                            \n  0.172384      5.34512e-9    2.48272e-9       1.24417e-9    7.18092e-9 \n  8.77676e-9    2.44647e-9    2.84651e-9       2.01956e-9    1.09825e-8 \n  6.75785e-9    2.44624e-10   4.79165e-10  \u2026   2.79034e-10   1.6273e-9  \n -7.91733e-11   4.07951e-10   6.78723e-10      8.17503e-10   5.9523e-9  \n -1.49319e-10  -1.68014e-10  -6.97805e-11      2.31588e-10   1.52119e-9 \n  4.53248e-10   5.12964e-10   1.57423e-9       3.93055e-9    3.06738e-8 \n -1.56445e-10   1.21443e-9    1.45145e-9       1.89669e-9    1.22688e-8 \n  1.43742e-9    1.20569e-9    1.0833e-9    \u2026   9.42889e-10   6.01223e-9 \n  1.90626e-10  -1.41852e-10   3.12113e-11      1.8946e-10    1.17163e-9 \n  0.0162573     9.37889e-10   8.29365e-10      7.05427e-10   4.91352e-9 \n -1.94284e-10   1.46358e-10   4.19363e-10      6.81323e-10   4.51758e-9 \n  0.0522645     0.055839      0.0336149        3.45354e-9    1.57129e-8\n\n\n\n\n\nAs expected, estimated coefficient values are quite close. \n\n\nFollow the \nlink\n to access the .ipynb file of this page.", 
            "title": "ADMM"
        }, 
        {
            "location": "/demo/admm/#admm", 
            "text": "In this section, we solve the same constrained lasso problem using the alternating direction method of multipliers (ADMM) algorithm. ADMM algorithm is advantageous since it can scale to larger size problems and is not restricted to linear constraints. See  Gaines and Zhou (2016)  for details.   In order to use the algorithm, user needs to supply a projection function onto the constraint set. Some simple examples are discussed below.", 
            "title": "ADMM"
        }, 
        {
            "location": "/demo/admm/#sum-to-zero-constraint", 
            "text": "We demonstrate using a sum-to-zero constraint example   \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac{1}{2}||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\sum_j \\beta_j = 0\n\\end{split}   For this constraint, the appropriate projection function for $\\boldsymbol{x} = (x_1, \\cdots, x_p)^T$ would be    \n\\text{proj}(\\boldsymbol{x}) = \\boldsymbol{x} - \\bar{\\boldsymbol{x}}= \\boldsymbol{x} - \\sum_{j=1}^p x_j.   First, let's define a true parameter  \u03b2  such that  sum(\u03b2) = 0 .  using   ConstrainedLasso ,   Base . Test   n ,   p   =   50 ,   100    \u03b2   =   zeros ( p );  \u03b2 [ 1 : round ( Int ,   p   /   4 )]   =   0  \u03b2 [( round ( Int ,   p   /   4 )   +   1 ) : round ( Int ,   p   /   2 )]   =   1  \u03b2 [( round ( Int ,   p   /   2 )   +   1 ) : round ( Int ,   3 p   /   4 )]   =   0  \u03b2 [( round ( Int ,   3 p   /   4 )   +   1 ) : p ]   =   - 1  \u03b2   100-element Array{Float64,1}:\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  \u22ee  \n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0  Next we generate data based on the true parameter \u03b2.  srand ( 41 )  X   =   randn ( n ,   p )   50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627  y   =   X   *   \u03b2   +   randn ( n )   50-element Array{Float64,1}:\n  -9.90585 \n  -5.40562 \n   5.24289 \n  -6.29951 \n  -4.9586  \n  -6.1342  \n  -7.90981 \n   2.51009 \n  -5.79548 \n   1.61355 \n  -0.722766\n  10.4522  \n   4.03935 \n   \u22ee       \n   0.397781\n  -2.6661  \n   5.36896 \n  -3.56537 \n  -2.402   \n   0.11478 \n  -5.39248 \n   4.38391 \n   0.706801\n -10.1066  \n  -1.12558 \n  14.2473  Now we estimate coefficients at fixed tuning parameter value using ADMM alogrithm.   \u03c1   =   2.0  \u03b2\u0302admm   =   lsq_constrsparsereg_admm ( X ,   y ,   \u03c1 ;   proj   =   x   -   x   -   mean ( x ))   \u03b2\u0302admm   100-element Array{Float64,1}:\n  0.0     \n  0.174344\n  0.0     \n -0.421288\n  0.0     \n  0.0     \n  0.0     \n  0.0     \n  0.324233\n -0.15384 \n  0.0     \n  0.0     \n  0.0     \n  \u22ee       \n -0.397027\n -0.32079 \n  0.0     \n  0.0     \n -0.868508\n -0.992272\n -0.571755\n  0.0     \n -1.16568 \n  0.0     \n  0.0     \n  0.0  Now let's compare the estimated coefficients with those obtained using quadratic programming.   \u03c1   =   2.0   beq   =   [ 0.0 ]  Aeq   =   ones ( 1 ,   p )  \u03b2\u0302 ,   =   lsq_constrsparsereg ( X ,   y ,   \u03c1 ;   Aeq   =   Aeq ,   beq   =   beq )    hcat ( \u03b2\u0302admm ,   \u03b2\u0302 )   100\u00d72 Array{Float64,2}:\n  0.0        1.51451e-8 \n  0.174344   0.178717   \n  0.0       -1.02944e-9 \n -0.421288  -0.414043   \n  0.0       -3.33221e-10\n  0.0       -4.14188e-10\n  0.0        3.28018e-11\n  0.0        8.38037e-11\n  0.324233   0.335375   \n -0.15384   -0.157908   \n  0.0       -2.44739e-9 \n  0.0       -8.79003e-10\n  0.0       -8.17787e-10\n  \u22ee                     \n -0.397027  -0.391635   \n -0.32079   -0.33352    \n  0.0       -5.51459e-9 \n  0.0        1.09637e-9 \n -0.868508  -0.867639   \n -0.992272  -0.999583   \n -0.571755  -0.577743   \n  0.0       -8.93601e-10\n -1.16568   -1.16862    \n  0.0        2.29367e-9 \n  0.0       -1.52035e-9 \n  0.0        2.71896e-9", 
            "title": "sum-to-zero constraint"
        }, 
        {
            "location": "/demo/admm/#non-negativity-constraint", 
            "text": "Here we look at the non-negativity constraint. For the non-negativity constraint    \n\\beta_j \\geq 0 \\hspace{0.8em} \\forall j,   the appropriate projection function for $\\boldsymbol{x} = (x_1, \\cdots, x_p)^T$ is    \n\\text{proj}(\\boldsymbol{x})_j = \\begin{cases}\nx_j & \\text{if } x_j \\geq 0 \\\\ \n0 & \\text{else} \\end{cases} \\hspace{0.8em} \\text{where } j = 1, \\cdots, p.   Now let's generate  X  and  y .  n ,   p   =   50 ,   100     \u03b2   =   zeros ( p )  \u03b2 [ 1 : 10 ]   =   1 : 10  srand ( 41 )  X   =   randn ( n ,   p )   50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627  y   =   X   *   \u03b2   +   randn ( n )   50-element Array{Float64,1}:\n  12.6173  \n  40.3776  \n   2.2169  \n  27.4631  \n  38.592   \n   7.82023 \n  22.7367  \n   7.88475 \n  -7.47037 \n   0.621035\n  -4.91899 \n -14.9363  \n   8.26901 \n   \u22ee       \n   7.83882 \n  -9.30699 \n -29.7205  \n  15.2482  \n -19.1784  \n  14.9865  \n   2.32728 \n  -9.11988 \n -15.3472  \n  22.9679  \n  -0.997964\n  42.6068  This time, we feed a sequence of tuning parameter values into the  lsq_constrsparsereg_admm  function.   \u03c1   =   linspace ( 1.0 ,   70.0 ,   10 )  \u03b2\u0302admm   =   lsq_constrsparsereg_admm ( X ,   y ,   \u03c1 ;   proj   =   x   -   clamp . ( x ,   0 ,   Inf ))   \u03b2\u0302admm   100\u00d710 Array{Float64,2}:\n  0.653843    0.490395    0.277812    \u2026  0.0      0.0      0.0      0.0    \n  2.21572     2.07051     1.96482        1.41607  1.26061  1.10519  0.94974\n  2.77145     2.52934     2.36727        1.77875  1.64644  1.51417  1.38186\n  4.13883     3.90272     3.70409        2.99234  2.82268  2.65297  2.4833 \n  4.84074     4.56198     4.29935        3.31819  3.06715  2.81613  2.5651 \n  5.8787      5.69235     5.49622     \u2026  4.46134  4.18054  3.89969  3.61888\n  6.7637      6.55971     6.34682        5.68324  5.54216  5.40096  5.25987\n  8.51065     8.03277     7.8351         6.87417  6.61797  6.36169  6.10549\n  8.7451      8.33258     8.02948        6.94937  6.68847  6.42749  6.16659\n  9.77876     9.76788     9.6326         9.12581  9.02118  8.91651  8.81187\n  1.65481e-6  2.65614e-5  5.75666e-5  \u2026  0.0      0.0      0.0      0.0    \n  0.0         0.0         0.0            0.0      0.0      0.0      0.0    \n  0.0         0.0         0.0            0.0      0.0      0.0      0.0    \n  \u22ee                                   \u22f1                                    \n  0.170999    1.06392e-5  0.0            0.0      0.0      0.0      0.0    \n  0.0         0.0         0.0            0.0      0.0      0.0      0.0    \n  0.0         0.0         0.0         \u2026  0.0      0.0      0.0      0.0    \n -5.9382e-7   0.0         0.0            0.0      0.0      0.0      0.0    \n  0.0         2.57367e-5  3.98409e-5     0.0      0.0      0.0      0.0    \n  0.0         0.0         0.0            0.0      0.0      0.0      0.0    \n  0.0         1.69146e-5  0.0            0.0      0.0      0.0      0.0    \n  0.0         0.0         0.0         \u2026  0.0      0.0      0.0      0.0    \n  0.0         0.0         3.45786e-5     0.0      0.0      0.0      0.0    \n  0.0141037   0.0         0.0            0.0      0.0      0.0      0.0    \n  0.0         2.39758e-5  3.62464e-5     0.0      0.0      0.0      0.0    \n  0.0503145   0.0553623   0.0339917      0.0      0.0      0.0      0.0  Again we compare with the estimates from quadratic programming.  bineq   =   zeros ( p )  Aineq   =   -   eye ( p )  \u03b2\u0302 ,   =   lsq_constrsparsereg ( X ,   y ,   \u03c1 ;   Aineq   =   Aineq ,   bineq   =   bineq )    \u03b2\u0302   100\u00d710 Array{Float64,2}:\n  0.652004      0.489105      0.277863     \u2026   2.00626e-9    8.58265e-9 \n  2.2179        2.07061       1.9648           1.105         0.949582   \n  2.77457       2.52826       2.36721          1.51391       1.38162    \n  4.13788       3.9009        3.70426          2.65333       2.48359    \n  4.84588       4.56136       4.29938          2.81596       2.56492    \n  5.87904       5.69159       5.49619      \u2026   3.89993       3.61905    \n  6.75881       6.55723       6.34731          5.40177       5.26053    \n  8.51202       8.03199       7.83528          6.36216       6.10583    \n  8.74756       8.32936       8.02991          6.42804       6.16702    \n  9.77692       9.76675       9.63278          8.91676       8.81211    \n -2.48584e-10  -2.61442e-10  -2.8615e-10   \u2026  -9.64613e-11  -7.2806e-10 \n -1.30999e-10  -3.24647e-10  -3.6769e-10      -1.54375e-10  -8.53117e-10\n -1.68134e-10   3.14662e-10   6.39206e-10      6.38157e-10   4.18343e-9 \n  \u22ee                                        \u22f1                            \n  0.172384      5.34512e-9    2.48272e-9       1.24417e-9    7.18092e-9 \n  8.77676e-9    2.44647e-9    2.84651e-9       2.01956e-9    1.09825e-8 \n  6.75785e-9    2.44624e-10   4.79165e-10  \u2026   2.79034e-10   1.6273e-9  \n -7.91733e-11   4.07951e-10   6.78723e-10      8.17503e-10   5.9523e-9  \n -1.49319e-10  -1.68014e-10  -6.97805e-11      2.31588e-10   1.52119e-9 \n  4.53248e-10   5.12964e-10   1.57423e-9       3.93055e-9    3.06738e-8 \n -1.56445e-10   1.21443e-9    1.45145e-9       1.89669e-9    1.22688e-8 \n  1.43742e-9    1.20569e-9    1.0833e-9    \u2026   9.42889e-10   6.01223e-9 \n  1.90626e-10  -1.41852e-10   3.12113e-11      1.8946e-10    1.17163e-9 \n  0.0162573     9.37889e-10   8.29365e-10      7.05427e-10   4.91352e-9 \n -1.94284e-10   1.46358e-10   4.19363e-10      6.81323e-10   4.51758e-9 \n  0.0522645     0.055839      0.0336149        3.45354e-9    1.57129e-8  As expected, estimated coefficient values are quite close.   Follow the  link  to access the .ipynb file of this page.", 
            "title": "Non-negativity constraint"
        }, 
        {
            "location": "/demo/path/", 
            "text": "Path algorithm\n\n\nThe solution path algorithm is useful when the user does not have a pre-specified grid of tuning parameter values and the cofficient estimates at more than a handful of values of the tuning parameter are desired.  \n\n\n\n\nSum-to-zero constraint\n\n\nIn this example, we will solve a problem defined by \n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\boldsymbol{\\beta}||_1  \\\\\n& \\text{subject to} \\hspace{1em} \\sum_j \\beta_j = 0\n\\end{split}\n\n\n\n\n\nNote that we can re-write the constraint as $\\boldsymbol{A\\beta} = \\boldsymbol{b}$ where \n\n\n\n\n\n\\boldsymbol{A} = \\begin{pmatrix} 1 & 1 & \\cdots & 1 \\end{pmatrix} \\text{ and } \\boldsymbol{b} = 0.\n\n\n\n\n\nFirst let's generate the predictor matrix \nX\n and response vector \ny\n. To do so, we need a true parameter vector \n\u03b2\n whose sum equals to 0. Note \nn\n is the number of observations \nn\n and \np\n is the number of predictors. \n\n\nn\n,\n \np\n \n=\n \n50\n,\n \n100\n  \n\n\u03b2\n \n=\n \nzeros\n(\np\n)\n\n\n\u03b2\n[\n1\n:\nround\n(\nInt\n,\n \np\n \n/\n \n4\n)]\n \n=\n \n0\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \np\n \n/\n \n4\n)\n \n+\n \n1\n)\n:\nround\n(\nInt\n,\n \np\n \n/\n \n2\n)]\n \n=\n \n1\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \np\n \n/\n \n2\n)\n \n+\n \n1\n)\n:\nround\n(\nInt\n,\n \n3\np\n \n/\n \n4\n)]\n \n=\n \n0\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \n3\np\n \n/\n \n4\n)\n \n+\n \n1\n)\n:\np\n]\n \n=\n \n-\n1\n\n\nsrand\n(\n41\n)\n\n\nX\n \n=\n \nrandn\n(\nn\n,\n \np\n)\n\n\n\n\n\n\n50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627\n\n\n\n\n\ny\n \n=\n \nX\n \n*\n \n\u03b2\n \n+\n \nrandn\n(\nn\n)\n\n\n\n\n\n\n50-element Array{Float64,1}:\n  -9.90585 \n  -5.40562 \n   5.24289 \n  -6.29951 \n  -4.9586  \n  -6.1342  \n  -7.90981 \n   2.51009 \n  -5.79548 \n   1.61355 \n  -0.722766\n  10.4522  \n   4.03935 \n   \u22ee       \n   0.397781\n  -2.6661  \n   5.36896 \n  -3.56537 \n  -2.402   \n   0.11478 \n  -5.39248 \n   4.38391 \n   0.706801\n -10.1066  \n  -1.12558 \n  14.2473\n\n\n\n\n\nSince the problem has equality constraints only, we define the constraints as below. \n\n\nbeq\n \n=\n \n0.0\n\n\nAeq\n \n=\n \nones\n(\n1\n,\n \np\n)\n\n\n\n\n\n\n1\u00d7100 Array{Float64,2}:\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \u2026  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n\n\n\n\n\nusing\n \nConstrainedLasso\n\n\n\u03b2\u0302path1\n,\n \n\u03c1path1\n,\n \nobjpath\n,\n \n=\n \nlsq_classopath\n(\nX\n,\n \ny\n;\n \nAeq\n \n=\n \nAeq\n,\n \nbeq\n \n=\n \nbeq\n);\n\n\n\n\n\n\nNow we are ready to obtain the solution path using the path algorithm. \n\n\n\u03b2\u0302path1\n\n\n\n\n\n\n100\u00d764 Array{Float64,2}:\n 0.0  0.0   0.0         0.0        \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0            0.2093     0.215544   0.222576\n 0.0  0.0   0.0         0.0            0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           -0.375203  -0.411589  -0.41253 \n 0.0  0.0   0.0         0.0            0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0        \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0            0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0            0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0            0.326206   0.349339   0.355867\n 0.0  0.0   0.0         0.0           -0.199751  -0.18084   -0.180896\n 0.0  0.0   0.0         0.0        \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0            0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0            0.0        0.0        0.0     \n \u22ee                                 \u22f1                                 \n 0.0  0.0   0.0         0.0           -0.466409  -0.455344  -0.458507\n 0.0  0.0   0.0         0.0           -0.40513   -0.358767  -0.358448\n 0.0  0.0   0.0         0.0        \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0            0.0        0.0        0.0     \n 0.0  0.0  -0.0558231  -0.0953907     -0.852347  -0.877741  -0.88212 \n 0.0  0.0   0.0         0.0           -1.08109   -1.05576   -1.06659 \n 0.0  0.0   0.0         0.0           -0.679306  -0.621053  -0.621533\n 0.0  0.0   0.0         0.0        \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           -1.25803   -1.20411   -1.20696 \n 0.0  0.0   0.0         0.0            0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0            0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0            0.0        0.0        0.0\n\n\n\n\n\nLet's see if sums of coefficients at all $\\rho$ values are approximately 0. \n\n\nall\n(\nabs\n.\n(\nsum\n(\n\u03b2\u0302path1\n,\n \n1\n))\n \n.\n \n1e-8\n)\n\n\n\n\n\n\ntrue\n\n\n\n\n\nWe plot the solution path below. \n\n\nusing\n \nPlots\n;\n \npyplot\n();\n\n\nplot\n(\n\u03c1path1\n,\n \n\u03b2\u0302path1\n,\n \nlabel\n=\n,\n \nxaxis\n \n=\n \n(\n\u03c1\n,\n \n(\nminimum\n(\n\u03c1path1\n),\n\n      \nmaximum\n(\n\u03c1path1\n))),\n \nyaxis\n \n=\n \n(\n\u03b2\u0302(\u03c1)\n),\n \nwidth\n=\n0.5\n)\n \n\ntitle!\n(\nSimulation 1: Solution Path via Constrained Lasso\n)\n \n\n\n\n\n\n\n\nsavefig\n(\nmisc/sumtozero.svg\n)\n\n\n\n\n\n\nNote the figure above is markedly smoother than in the \nfigure\n obtained from passing in a sequence of tuning parameter values. This is because the solution path algorithm captures all events. \n\n\n\n\nNon-negativity constraint\n\n\nIn this example, the problem is defined by \n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\boldsymbol{\\beta}||_1  \\\\\n& \\text{subject to} \\hspace{1em} \\beta_j \\geq 0 \\forall j\n\\end{split}\n\n\n\n\n\nWe can re-write the inequality constraint as $\\boldsymbol{C\\beta} \\leq \\boldsymbol{d}$ where \n\n\n\n\n\n\\boldsymbol{C} = \\begin{pmatrix} \n-1 & & & \\\\\n    & -1 & & \\\\\n    &   & \\ddots & \\\\\n    &   &   & -1\n\\end{pmatrix}\n\\text{ and } \\boldsymbol{d} = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}\n\n\n\n\n\nFirst we define a true parameter vector \n\u03b2\n that is sparse with a few non-zero coefficients. Let \nn\n and \np\n be the number of observations and predictors, respectively. \n\n\nn\n,\n \np\n \n=\n \n50\n,\n \n100\n   \n\n\u03b2\n \n=\n \nzeros\n(\np\n)\n\n\n\u03b2\n[\n1\n:\n10\n]\n \n=\n \n1\n:\n10\n\n\nsrand\n(\n41\n)\n\n\nX\n \n=\n \nrandn\n(\nn\n,\n \np\n)\n\n\n\n\n\n\n50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627\n\n\n\n\n\ny\n \n=\n \nX\n \n*\n \n\u03b2\n \n+\n \nrandn\n(\nn\n)\n\n\n\n\n\n\n50-element Array{Float64,1}:\n  12.6173  \n  40.3776  \n   2.2169  \n  27.4631  \n  38.592   \n   7.82023 \n  22.7367  \n   7.88475 \n  -7.47037 \n   0.621035\n  -4.91899 \n -14.9363  \n   8.26901 \n   \u22ee       \n   7.83882 \n  -9.30699 \n -29.7205  \n  15.2482  \n -19.1784  \n  14.9865  \n   2.32728 \n  -9.11988 \n -15.3472  \n  22.9679  \n  -0.997964\n  42.6068\n\n\n\n\n\nNow set up the inequality constraint for the problem.\n\n\nbineq\n \n=\n \nzeros\n(\np\n)\n\n\nAineq\n \n=\n \n-\n \neye\n(\np\n)\n\n\n\n\n\n\n100\u00d7100 Array{Float64,2}:\n -1.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -1.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -1.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -1.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -1.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -1.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n  \u22ee                             \u22ee    \u22f1         \u22ee                          \n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -1.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -1.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -1.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -1.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -1.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -1.0\n\n\n\n\n\nNow we are ready to obtain the solution path using the path algorithm.\n\n\n\u03b2\u0302path2\n,\n \n\u03c1path2\n,\n \n=\n \nlsq_classopath\n(\nX\n,\n \ny\n;\n \nAineq\n \n=\n \nAineq\n,\n \nbineq\n \n=\n \nbineq\n)\n \n\n\n\n\n\n\u03b2\u0302path2\n\n\n\n\n\n\n100\u00d7183 Array{Float64,2}:\n 0.0         0.0      0.0      0.0      \u2026  0.783939   0.791708   0.796529 \n 0.0         0.0      0.0      0.0         2.17561    2.18099    2.18875  \n 0.0         0.0      0.0      0.0         2.99935    3.008      3.01471  \n 0.0         0.0      0.0      0.0         4.30984    4.31056    4.30849  \n 0.0         0.0      0.0      0.0         4.98995    4.99358    4.9955   \n 0.0         0.0      0.0      0.0      \u2026  6.18666    6.18814    6.18596  \n 0.0         0.0      0.0      0.0         6.92076    6.92371    6.92749  \n 0.0         0.0      0.0      0.0         8.56963    8.55907    8.54642  \n 0.0         0.0      0.0      0.0         8.86323    8.864      8.86137  \n 0.00616069  2.01444  2.41323  2.42264     9.8864     9.89486    9.90491  \n 0.0         0.0      0.0      0.0      \u2026  0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n \u22ee                                      \u22f1  \u22ee                              \n 0.0         0.0      0.0      0.0         0.127693   0.122633   0.114126 \n 0.0         0.0      0.0      0.0         0.257807   0.261246   0.265255 \n 0.0         0.0      0.0      0.0      \u2026  0.294213   0.285664   0.272772 \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0      \u2026  0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0838146  0.0914735  0.0978112\n 0.0         0.0      0.0      0.0         0.200482   0.202642   0.201151 \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0\n\n\n\n\n\nWe plot the solution path below. \n\n\nplot\n(\n\u03c1path2\n,\n \n\u03b2\u0302path2\n,\n \nlabel\n=\n,\n \nxaxis\n \n=\n \n(\n\u03c1\n,\n \n(\nminimum\n(\n\u03c1path2\n),\n\n      \nmaximum\n(\n\u03c1path2\n))),\n \nyaxis\n \n=\n \n(\n\u03b2\u0302(\u03c1)\n),\n \nwidth\n=\n0.5\n)\n \n\ntitle!\n(\nSimulation 2: Solution Path via Constrained Lasso\n)\n \n\nsavefig\n(\nmisc/nonneg.svg\n)\n\n\n\n\n\n\n\n\nFollow the \nlink\n to access the .ipynb file of this page.", 
            "title": "Path Algorithm"
        }, 
        {
            "location": "/demo/path/#path-algorithm", 
            "text": "The solution path algorithm is useful when the user does not have a pre-specified grid of tuning parameter values and the cofficient estimates at more than a handful of values of the tuning parameter are desired.", 
            "title": "Path algorithm"
        }, 
        {
            "location": "/demo/path/#sum-to-zero-constraint", 
            "text": "In this example, we will solve a problem defined by    \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\boldsymbol{\\beta}||_1  \\\\\n& \\text{subject to} \\hspace{1em} \\sum_j \\beta_j = 0\n\\end{split}   Note that we can re-write the constraint as $\\boldsymbol{A\\beta} = \\boldsymbol{b}$ where    \n\\boldsymbol{A} = \\begin{pmatrix} 1 & 1 & \\cdots & 1 \\end{pmatrix} \\text{ and } \\boldsymbol{b} = 0.   First let's generate the predictor matrix  X  and response vector  y . To do so, we need a true parameter vector  \u03b2  whose sum equals to 0. Note  n  is the number of observations  n  and  p  is the number of predictors.   n ,   p   =   50 ,   100    \u03b2   =   zeros ( p )  \u03b2 [ 1 : round ( Int ,   p   /   4 )]   =   0  \u03b2 [( round ( Int ,   p   /   4 )   +   1 ) : round ( Int ,   p   /   2 )]   =   1  \u03b2 [( round ( Int ,   p   /   2 )   +   1 ) : round ( Int ,   3 p   /   4 )]   =   0  \u03b2 [( round ( Int ,   3 p   /   4 )   +   1 ) : p ]   =   - 1  srand ( 41 )  X   =   randn ( n ,   p )   50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627  y   =   X   *   \u03b2   +   randn ( n )   50-element Array{Float64,1}:\n  -9.90585 \n  -5.40562 \n   5.24289 \n  -6.29951 \n  -4.9586  \n  -6.1342  \n  -7.90981 \n   2.51009 \n  -5.79548 \n   1.61355 \n  -0.722766\n  10.4522  \n   4.03935 \n   \u22ee       \n   0.397781\n  -2.6661  \n   5.36896 \n  -3.56537 \n  -2.402   \n   0.11478 \n  -5.39248 \n   4.38391 \n   0.706801\n -10.1066  \n  -1.12558 \n  14.2473  Since the problem has equality constraints only, we define the constraints as below.   beq   =   0.0  Aeq   =   ones ( 1 ,   p )   1\u00d7100 Array{Float64,2}:\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \u2026  1.0  1.0  1.0  1.0  1.0  1.0  1.0  using   ConstrainedLasso  \u03b2\u0302path1 ,   \u03c1path1 ,   objpath ,   =   lsq_classopath ( X ,   y ;   Aeq   =   Aeq ,   beq   =   beq );   Now we are ready to obtain the solution path using the path algorithm.   \u03b2\u0302path1   100\u00d764 Array{Float64,2}:\n 0.0  0.0   0.0         0.0        \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0            0.2093     0.215544   0.222576\n 0.0  0.0   0.0         0.0            0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           -0.375203  -0.411589  -0.41253 \n 0.0  0.0   0.0         0.0            0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0        \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0            0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0            0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0            0.326206   0.349339   0.355867\n 0.0  0.0   0.0         0.0           -0.199751  -0.18084   -0.180896\n 0.0  0.0   0.0         0.0        \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0            0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0            0.0        0.0        0.0     \n \u22ee                                 \u22f1                                 \n 0.0  0.0   0.0         0.0           -0.466409  -0.455344  -0.458507\n 0.0  0.0   0.0         0.0           -0.40513   -0.358767  -0.358448\n 0.0  0.0   0.0         0.0        \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0            0.0        0.0        0.0     \n 0.0  0.0  -0.0558231  -0.0953907     -0.852347  -0.877741  -0.88212 \n 0.0  0.0   0.0         0.0           -1.08109   -1.05576   -1.06659 \n 0.0  0.0   0.0         0.0           -0.679306  -0.621053  -0.621533\n 0.0  0.0   0.0         0.0        \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           -1.25803   -1.20411   -1.20696 \n 0.0  0.0   0.0         0.0            0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0            0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0            0.0        0.0        0.0  Let's see if sums of coefficients at all $\\rho$ values are approximately 0.   all ( abs . ( sum ( \u03b2\u0302path1 ,   1 ))   .   1e-8 )   true  We plot the solution path below.   using   Plots ;   pyplot ();  plot ( \u03c1path1 ,   \u03b2\u0302path1 ,   label = ,   xaxis   =   ( \u03c1 ,   ( minimum ( \u03c1path1 ), \n       maximum ( \u03c1path1 ))),   yaxis   =   ( \u03b2\u0302(\u03c1) ),   width = 0.5 )   title! ( Simulation 1: Solution Path via Constrained Lasso )     savefig ( misc/sumtozero.svg )   Note the figure above is markedly smoother than in the  figure  obtained from passing in a sequence of tuning parameter values. This is because the solution path algorithm captures all events.", 
            "title": "Sum-to-zero constraint"
        }, 
        {
            "location": "/demo/path/#non-negativity-constraint", 
            "text": "In this example, the problem is defined by    \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\boldsymbol{\\beta}||_1  \\\\\n& \\text{subject to} \\hspace{1em} \\beta_j \\geq 0 \\forall j\n\\end{split}   We can re-write the inequality constraint as $\\boldsymbol{C\\beta} \\leq \\boldsymbol{d}$ where    \n\\boldsymbol{C} = \\begin{pmatrix} \n-1 & & & \\\\\n    & -1 & & \\\\\n    &   & \\ddots & \\\\\n    &   &   & -1\n\\end{pmatrix}\n\\text{ and } \\boldsymbol{d} = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}   First we define a true parameter vector  \u03b2  that is sparse with a few non-zero coefficients. Let  n  and  p  be the number of observations and predictors, respectively.   n ,   p   =   50 ,   100     \u03b2   =   zeros ( p )  \u03b2 [ 1 : 10 ]   =   1 : 10  srand ( 41 )  X   =   randn ( n ,   p )   50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627  y   =   X   *   \u03b2   +   randn ( n )   50-element Array{Float64,1}:\n  12.6173  \n  40.3776  \n   2.2169  \n  27.4631  \n  38.592   \n   7.82023 \n  22.7367  \n   7.88475 \n  -7.47037 \n   0.621035\n  -4.91899 \n -14.9363  \n   8.26901 \n   \u22ee       \n   7.83882 \n  -9.30699 \n -29.7205  \n  15.2482  \n -19.1784  \n  14.9865  \n   2.32728 \n  -9.11988 \n -15.3472  \n  22.9679  \n  -0.997964\n  42.6068  Now set up the inequality constraint for the problem.  bineq   =   zeros ( p )  Aineq   =   -   eye ( p )   100\u00d7100 Array{Float64,2}:\n -1.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -1.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -1.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -1.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -1.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -1.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n  \u22ee                             \u22ee    \u22f1         \u22ee                          \n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -1.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -1.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -1.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -1.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -1.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -1.0  Now we are ready to obtain the solution path using the path algorithm.  \u03b2\u0302path2 ,   \u03c1path2 ,   =   lsq_classopath ( X ,   y ;   Aineq   =   Aineq ,   bineq   =   bineq )    \u03b2\u0302path2   100\u00d7183 Array{Float64,2}:\n 0.0         0.0      0.0      0.0      \u2026  0.783939   0.791708   0.796529 \n 0.0         0.0      0.0      0.0         2.17561    2.18099    2.18875  \n 0.0         0.0      0.0      0.0         2.99935    3.008      3.01471  \n 0.0         0.0      0.0      0.0         4.30984    4.31056    4.30849  \n 0.0         0.0      0.0      0.0         4.98995    4.99358    4.9955   \n 0.0         0.0      0.0      0.0      \u2026  6.18666    6.18814    6.18596  \n 0.0         0.0      0.0      0.0         6.92076    6.92371    6.92749  \n 0.0         0.0      0.0      0.0         8.56963    8.55907    8.54642  \n 0.0         0.0      0.0      0.0         8.86323    8.864      8.86137  \n 0.00616069  2.01444  2.41323  2.42264     9.8864     9.89486    9.90491  \n 0.0         0.0      0.0      0.0      \u2026  0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n \u22ee                                      \u22f1  \u22ee                              \n 0.0         0.0      0.0      0.0         0.127693   0.122633   0.114126 \n 0.0         0.0      0.0      0.0         0.257807   0.261246   0.265255 \n 0.0         0.0      0.0      0.0      \u2026  0.294213   0.285664   0.272772 \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0      \u2026  0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0838146  0.0914735  0.0978112\n 0.0         0.0      0.0      0.0         0.200482   0.202642   0.201151 \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0  We plot the solution path below.   plot ( \u03c1path2 ,   \u03b2\u0302path2 ,   label = ,   xaxis   =   ( \u03c1 ,   ( minimum ( \u03c1path2 ), \n       maximum ( \u03c1path2 ))),   yaxis   =   ( \u03b2\u0302(\u03c1) ),   width = 0.5 )   title! ( Simulation 2: Solution Path via Constrained Lasso )   savefig ( misc/nonneg.svg )    Follow the  link  to access the .ipynb file of this page.", 
            "title": "Non-negativity constraint"
        }, 
        {
            "location": "/demo/prostate/", 
            "text": "Prostate Data\n\n\nThis demonstration solves a regular, unconstrained lasso problem using the constrained lasso solution path (\nlsq_classopath.jl\n).\n\n\nThe \nprostate\n data come from a study that examined the correlation between the level of prostate specific antigen and a number of clinical measures in men who were about to receive a radical prostatectomy. (\nStamey et al. (1989)\n)\n\n\nLet's load and organize the \nprostate\n data. Since we are interested in the following variables as predictors, we extract them and create a design matrix \nXz\n:\n\n\n\n\nlcavol\n : log(cancer volume)\n\n\nlweight\n: log(prostate weight)\n\n\nage\n    : age\n\n\nlbph\n   : log(benign prostatic hyperplasia amount)\n\n\nsvi\n    : seminal vesicle invasion\n\n\nlcp\n    : log(capsular penetration)\n\n\ngleason\n: Gleason score\n\n\npgg45\n  : percentage Gleason scores 4 or 5\n\n\n\n\nThe response variable is \nlpsa\n, which is log(prostate specific antigen). \n\n\nusing\n \nConstrainedLasso\n \n\n\n\n\n\nprostate\n \n=\n \nreadcsv\n(\njoinpath\n(\nPkg\n.\ndir\n(\nConstrainedLasso\n),\n \ndocs/src/demo/misc/prostate.csv\n),\n \nheader\n=\ntrue\n)\n\n\ntmp\n \n=\n \nInt\n[]\n\n\nlabels\n \n=\n \n[\nlcavol\n \nlweight\n \nage\n \nlbph\n \nsvi\n \nlcp\n \ngleason\n \npgg45\n]\n\n\nfor\n \ni\n \nin\n \nlabels\n\n    \npush!\n(\ntmp\n,\n \nfind\n(\nx\n \n-\n \nx\n \n==\n \ni\n,\n \nprostate\n[\n2\n])[\n1\n])\n\n\nend\n\n\nXz\n \n=\n \nArray\n{\nFloat64\n}(\nprostate\n[\n1\n][\n:\n,\n \ntmp\n])\n\n\n\n\n\n\n97\u00d78 Array{Float64,2}:\n -0.579818  2.76946  50.0  -1.38629   0.0  -1.38629   6.0   0.0\n -0.994252  3.31963  58.0  -1.38629   0.0  -1.38629   6.0   0.0\n -0.510826  2.69124  74.0  -1.38629   0.0  -1.38629   7.0  20.0\n -1.20397   3.28279  58.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.751416  3.43237  62.0  -1.38629   0.0  -1.38629   6.0   0.0\n -1.04982   3.22883  50.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.737164  3.47352  64.0   0.615186  0.0  -1.38629   6.0   0.0\n  0.693147  3.53951  58.0   1.53687   0.0  -1.38629   6.0   0.0\n -0.776529  3.53951  47.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.223144  3.24454  63.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.254642  3.60414  65.0  -1.38629   0.0  -1.38629   6.0   0.0\n -1.34707   3.59868  63.0   1.26695   0.0  -1.38629   6.0   0.0\n  1.61343   3.02286  63.0  -1.38629   0.0  -0.597837  7.0  30.0\n  \u22ee                                         \u22ee                  \n  3.30285   3.51898  64.0  -1.38629   1.0   2.32728   7.0  60.0\n  2.02419   3.7317   58.0   1.639     0.0  -1.38629   6.0   0.0\n  1.73166   3.36902  62.0  -1.38629   1.0   0.300105  7.0  30.0\n  2.80759   4.71805  65.0  -1.38629   1.0   2.46385   7.0  60.0\n  1.56235   3.69511  76.0   0.936093  1.0   0.81093   7.0  75.0\n  3.24649   4.10182  68.0  -1.38629   0.0  -1.38629   6.0   0.0\n  2.5329    3.67757  61.0   1.34807   1.0  -1.38629   7.0  15.0\n  2.83027   3.8764   68.0  -1.38629   1.0   1.32176   7.0  60.0\n  3.821     3.89691  44.0  -1.38629   1.0   2.16905   7.0  40.0\n  2.90745   3.39619  52.0  -1.38629   1.0   2.46385   7.0  10.0\n  2.88256   3.77391  68.0   1.55814   1.0   1.55814   7.0  80.0\n  3.47197   3.975    68.0   0.438255  1.0   2.90417   7.0  20.0\n\n\n\n\n\ny\n \n=\n \nArray\n{\nFloat64\n}(\nprostate\n[\n1\n][\n:\n,\n \nend\n-\n1\n])\n\n\n\n\n\n\n97-element Array{Float64,1}:\n -0.430783\n -0.162519\n -0.162519\n -0.162519\n  0.371564\n  0.765468\n  0.765468\n  0.854415\n  1.04732 \n  1.04732 \n  1.26695 \n  1.26695 \n  1.26695 \n  \u22ee       \n  3.63099 \n  3.68009 \n  3.71235 \n  3.98434 \n  3.9936  \n  4.02981 \n  4.12955 \n  4.38515 \n  4.68444 \n  5.14312 \n  5.47751 \n  5.58293\n\n\n\n\n\nFirst we standardize the data by subtracting its mean and dividing by its standard deviation. \n\n\nn\n,\n \np\n \n=\n \nsize\n(\nXz\n)\n\n\nfor\n \ni\n \nin\n \n1\n:\nsize\n(\nXz\n,\n2\n)\n\n    \nXz\n[\n:\n,\n \ni\n]\n \n-=\n \nmean\n(\nXz\n[\n:\n,\n \ni\n])\n\n    \nXz\n[\n:\n,\n \ni\n]\n \n/=\n \nstd\n(\nXz\n[\n:\n,\n \ni\n])\n\n\nend\n\n\nXz\n\n\n\n\n\n\n97\u00d78 Array{Float64,2}:\n -1.63736   -2.00621    -1.86243    \u2026  -0.863171  -1.04216   -0.864467\n -1.98898   -0.722009   -0.787896      -0.863171  -1.04216   -0.864467\n -1.57882   -2.18878     1.36116       -0.863171   0.342627  -0.155348\n -2.16692   -0.807994   -0.787896      -0.863171  -1.04216   -0.864467\n -0.507874  -0.458834   -0.250631      -0.863171  -1.04216   -0.864467\n -2.03613   -0.933955   -1.86243    \u2026  -0.863171  -1.04216   -0.864467\n -0.519967  -0.362793    0.0180011     -0.863171  -1.04216   -0.864467\n -0.557313  -0.208757   -0.787896      -0.863171  -1.04216   -0.864467\n -1.80425   -0.208757   -2.26537       -0.863171  -1.04216   -0.864467\n -0.956085  -0.897266   -0.116315      -0.863171  -1.04216   -0.864467\n -0.92936   -0.0578992   0.152317   \u2026  -0.863171  -1.04216   -0.864467\n -2.28833   -0.0706369  -0.116315      -0.863171  -1.04216   -0.864467\n  0.223498  -1.41472    -0.116315      -0.299282   0.342627   0.199211\n  \u22ee                                 \u22f1   \u22ee                             \n  1.65688   -0.256675    0.0180011  \u2026   1.7927     0.342627   1.26289 \n  0.572009   0.239854   -0.787896      -0.863171  -1.04216   -0.864467\n  0.323806  -0.606718   -0.250631       0.342907   0.342627   0.199211\n  1.23668    2.54221     0.152317       1.89038    0.342627   1.26289 \n  0.180156   0.154448    1.6298         0.70824    0.342627   1.79473 \n  1.60906    1.10379     0.555266   \u2026  -0.863171  -1.04216   -0.864467\n  1.00362    0.113497   -0.384948      -0.863171   0.342627  -0.332628\n  1.25592    0.577607    0.555266       1.07357    0.342627   1.26289 \n  2.09651    0.625489   -2.66832        1.67954    0.342627   0.55377 \n  1.3214    -0.543304   -1.59379        1.89038    0.342627  -0.509907\n  1.30029    0.338384    0.555266   \u2026   1.24263    0.342627   1.97201 \n  1.80037    0.807764    0.555266       2.20528    0.342627  -0.155348\n\n\n\n\n\nNow we solve the problem using solution path algorithm. \n\n\n\u03b2path\n,\n \n\u03c1path\n,\n \n=\n \nlsq_classopath\n(\nXz\n,\n \ny\n);\n\n\n\n\n\n\n\u03b2path\n\n\n\n\n\n\n8\u00d79 Array{Float64,2}:\n 0.000197119  0.421559  0.461915   \u2026   0.597645    0.603245    0.665561 \n 0.0          0.0       0.0            0.232715    0.246191    0.266408 \n 0.0          0.0       0.0           -0.0601318  -0.0936838  -0.158234 \n 0.0          0.0       0.0            0.0882392   0.108105    0.14034  \n 0.0          0.0       0.0403562      0.243534    0.252539    0.315269 \n 0.0          0.0       0.0        \u2026   0.0         0.0        -0.148508 \n 0.0          0.0       0.0            0.0         0.0121929   0.0354652\n 0.0          0.0       0.0            0.0646193   0.0699873   0.125787\n\n\n\n\n\nWe plot the solution path below. \n\n\nusing\n \nPlots\n;\n \npyplot\n();\n \n\ncolors\n \n=\n \n[\n:\ngreen\n \n:\norange\n \n:\nblack\n \n:\npurple\n \n:\nred\n \n:\ngrey\n \n:\nbrown\n \n:\nblue\n]\n \n\nplot\n(\n\u03c1path\n,\n \n\u03b2path\n,\n \nxaxis\n \n=\n \n(\n\u03c1\n,\n \n(\nminimum\n(\n\u03c1path\n),\n\n      \nmaximum\n(\n\u03c1path\n))),\n \nyaxis\n \n=\n \n(\n\u03b2\u0302(\u03c1)\n),\n \nlabel\n=\nlabels\n,\n \ncolor\n=\ncolors\n)\n\n\ntitle!\n(\nProstate Data: Solution Path via Constrained Lasso\n)\n\n\n\n\n\n\n\n\nBelow, we solve the same problem using \nGLMNet.jl\n package. \n\n\nusing\n \nGLMNet\n;\n  \n\npath\n \n=\n \nglmnet\n(\nXz\n,\n \ny\n,\n \nintercept\n=\nfalse\n);\n\n\npath\n.\nbetas\n\n\n\n\n\n\nplot\n(\npath\n.\nlambda\n,\n \npath\n.\nbetas\n,\n \ncolor\n=\ncolors\n,\n \nlabel\n=\nlabels\n,\n \n        \nxaxis\n=\n(\n\u03bb\n),\n \nyaxis\n=\n \n(\n\u03b2\u0302(\u03bb)\n))\n\n\ntitle!\n(\nProstate Data: Solution Path via GLMNet.jl\n)\n\n\n\n\n\n\n\n\nFollow the \nlink\n to access the .ipynb file of this page.", 
            "title": "Prostate Data"
        }, 
        {
            "location": "/demo/prostate/#prostate-data", 
            "text": "This demonstration solves a regular, unconstrained lasso problem using the constrained lasso solution path ( lsq_classopath.jl ).  The  prostate  data come from a study that examined the correlation between the level of prostate specific antigen and a number of clinical measures in men who were about to receive a radical prostatectomy. ( Stamey et al. (1989) )  Let's load and organize the  prostate  data. Since we are interested in the following variables as predictors, we extract them and create a design matrix  Xz :   lcavol  : log(cancer volume)  lweight : log(prostate weight)  age     : age  lbph    : log(benign prostatic hyperplasia amount)  svi     : seminal vesicle invasion  lcp     : log(capsular penetration)  gleason : Gleason score  pgg45   : percentage Gleason scores 4 or 5   The response variable is  lpsa , which is log(prostate specific antigen).   using   ConstrainedLasso    prostate   =   readcsv ( joinpath ( Pkg . dir ( ConstrainedLasso ),   docs/src/demo/misc/prostate.csv ),   header = true )  tmp   =   Int []  labels   =   [ lcavol   lweight   age   lbph   svi   lcp   gleason   pgg45 ]  for   i   in   labels \n     push! ( tmp ,   find ( x   -   x   ==   i ,   prostate [ 2 ])[ 1 ])  end  Xz   =   Array { Float64 }( prostate [ 1 ][ : ,   tmp ])   97\u00d78 Array{Float64,2}:\n -0.579818  2.76946  50.0  -1.38629   0.0  -1.38629   6.0   0.0\n -0.994252  3.31963  58.0  -1.38629   0.0  -1.38629   6.0   0.0\n -0.510826  2.69124  74.0  -1.38629   0.0  -1.38629   7.0  20.0\n -1.20397   3.28279  58.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.751416  3.43237  62.0  -1.38629   0.0  -1.38629   6.0   0.0\n -1.04982   3.22883  50.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.737164  3.47352  64.0   0.615186  0.0  -1.38629   6.0   0.0\n  0.693147  3.53951  58.0   1.53687   0.0  -1.38629   6.0   0.0\n -0.776529  3.53951  47.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.223144  3.24454  63.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.254642  3.60414  65.0  -1.38629   0.0  -1.38629   6.0   0.0\n -1.34707   3.59868  63.0   1.26695   0.0  -1.38629   6.0   0.0\n  1.61343   3.02286  63.0  -1.38629   0.0  -0.597837  7.0  30.0\n  \u22ee                                         \u22ee                  \n  3.30285   3.51898  64.0  -1.38629   1.0   2.32728   7.0  60.0\n  2.02419   3.7317   58.0   1.639     0.0  -1.38629   6.0   0.0\n  1.73166   3.36902  62.0  -1.38629   1.0   0.300105  7.0  30.0\n  2.80759   4.71805  65.0  -1.38629   1.0   2.46385   7.0  60.0\n  1.56235   3.69511  76.0   0.936093  1.0   0.81093   7.0  75.0\n  3.24649   4.10182  68.0  -1.38629   0.0  -1.38629   6.0   0.0\n  2.5329    3.67757  61.0   1.34807   1.0  -1.38629   7.0  15.0\n  2.83027   3.8764   68.0  -1.38629   1.0   1.32176   7.0  60.0\n  3.821     3.89691  44.0  -1.38629   1.0   2.16905   7.0  40.0\n  2.90745   3.39619  52.0  -1.38629   1.0   2.46385   7.0  10.0\n  2.88256   3.77391  68.0   1.55814   1.0   1.55814   7.0  80.0\n  3.47197   3.975    68.0   0.438255  1.0   2.90417   7.0  20.0  y   =   Array { Float64 }( prostate [ 1 ][ : ,   end - 1 ])   97-element Array{Float64,1}:\n -0.430783\n -0.162519\n -0.162519\n -0.162519\n  0.371564\n  0.765468\n  0.765468\n  0.854415\n  1.04732 \n  1.04732 \n  1.26695 \n  1.26695 \n  1.26695 \n  \u22ee       \n  3.63099 \n  3.68009 \n  3.71235 \n  3.98434 \n  3.9936  \n  4.02981 \n  4.12955 \n  4.38515 \n  4.68444 \n  5.14312 \n  5.47751 \n  5.58293  First we standardize the data by subtracting its mean and dividing by its standard deviation.   n ,   p   =   size ( Xz )  for   i   in   1 : size ( Xz , 2 ) \n     Xz [ : ,   i ]   -=   mean ( Xz [ : ,   i ]) \n     Xz [ : ,   i ]   /=   std ( Xz [ : ,   i ])  end  Xz   97\u00d78 Array{Float64,2}:\n -1.63736   -2.00621    -1.86243    \u2026  -0.863171  -1.04216   -0.864467\n -1.98898   -0.722009   -0.787896      -0.863171  -1.04216   -0.864467\n -1.57882   -2.18878     1.36116       -0.863171   0.342627  -0.155348\n -2.16692   -0.807994   -0.787896      -0.863171  -1.04216   -0.864467\n -0.507874  -0.458834   -0.250631      -0.863171  -1.04216   -0.864467\n -2.03613   -0.933955   -1.86243    \u2026  -0.863171  -1.04216   -0.864467\n -0.519967  -0.362793    0.0180011     -0.863171  -1.04216   -0.864467\n -0.557313  -0.208757   -0.787896      -0.863171  -1.04216   -0.864467\n -1.80425   -0.208757   -2.26537       -0.863171  -1.04216   -0.864467\n -0.956085  -0.897266   -0.116315      -0.863171  -1.04216   -0.864467\n -0.92936   -0.0578992   0.152317   \u2026  -0.863171  -1.04216   -0.864467\n -2.28833   -0.0706369  -0.116315      -0.863171  -1.04216   -0.864467\n  0.223498  -1.41472    -0.116315      -0.299282   0.342627   0.199211\n  \u22ee                                 \u22f1   \u22ee                             \n  1.65688   -0.256675    0.0180011  \u2026   1.7927     0.342627   1.26289 \n  0.572009   0.239854   -0.787896      -0.863171  -1.04216   -0.864467\n  0.323806  -0.606718   -0.250631       0.342907   0.342627   0.199211\n  1.23668    2.54221     0.152317       1.89038    0.342627   1.26289 \n  0.180156   0.154448    1.6298         0.70824    0.342627   1.79473 \n  1.60906    1.10379     0.555266   \u2026  -0.863171  -1.04216   -0.864467\n  1.00362    0.113497   -0.384948      -0.863171   0.342627  -0.332628\n  1.25592    0.577607    0.555266       1.07357    0.342627   1.26289 \n  2.09651    0.625489   -2.66832        1.67954    0.342627   0.55377 \n  1.3214    -0.543304   -1.59379        1.89038    0.342627  -0.509907\n  1.30029    0.338384    0.555266   \u2026   1.24263    0.342627   1.97201 \n  1.80037    0.807764    0.555266       2.20528    0.342627  -0.155348  Now we solve the problem using solution path algorithm.   \u03b2path ,   \u03c1path ,   =   lsq_classopath ( Xz ,   y );   \u03b2path   8\u00d79 Array{Float64,2}:\n 0.000197119  0.421559  0.461915   \u2026   0.597645    0.603245    0.665561 \n 0.0          0.0       0.0            0.232715    0.246191    0.266408 \n 0.0          0.0       0.0           -0.0601318  -0.0936838  -0.158234 \n 0.0          0.0       0.0            0.0882392   0.108105    0.14034  \n 0.0          0.0       0.0403562      0.243534    0.252539    0.315269 \n 0.0          0.0       0.0        \u2026   0.0         0.0        -0.148508 \n 0.0          0.0       0.0            0.0         0.0121929   0.0354652\n 0.0          0.0       0.0            0.0646193   0.0699873   0.125787  We plot the solution path below.   using   Plots ;   pyplot ();   colors   =   [ : green   : orange   : black   : purple   : red   : grey   : brown   : blue ]   plot ( \u03c1path ,   \u03b2path ,   xaxis   =   ( \u03c1 ,   ( minimum ( \u03c1path ), \n       maximum ( \u03c1path ))),   yaxis   =   ( \u03b2\u0302(\u03c1) ),   label = labels ,   color = colors )  title! ( Prostate Data: Solution Path via Constrained Lasso )    Below, we solve the same problem using  GLMNet.jl  package.   using   GLMNet ;    path   =   glmnet ( Xz ,   y ,   intercept = false );  path . betas   plot ( path . lambda ,   path . betas ,   color = colors ,   label = labels ,  \n         xaxis = ( \u03bb ),   yaxis =   ( \u03b2\u0302(\u03bb) ))  title! ( Prostate Data: Solution Path via GLMNet.jl )    Follow the  link  to access the .ipynb file of this page.", 
            "title": "Prostate Data"
        }, 
        {
            "location": "/demo/warming/", 
            "text": "Global Warming Data\n\n\nHere we consider the annual data on temperature anomalies. In general, temperature appears to increase monotonically over the time period of 1850 to 2015 (\nWu et al., 2001\n; \nTibshirani et al., 2011\n). This monotonicity can be imposed on the coeffcient estimates using the constrained lasso with the inequality constraint matrix:\n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac{1}{2} ||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{1em} \\boldsymbol{C\\beta} \\leq \\boldsymbol{d} \n\\end{split}\n\n\n\n\n\nwhere \n\n\n\n\n\n\\boldsymbol{C} = \\begin{pmatrix} \n1 & -1 &     &        &       &     & \\\\\n  & 1  & -1  &        &         &   & \\\\\n  &    &  1  & -1     &         &   & \\\\\n  &     &       & \\ddots & \\ddots &  & \\\\\n  &     &       &        &          & 1 & -1 \\\\\n\\end{pmatrix}\n\n\n\n\n\nand $\\boldsymbol{d} = \\boldsymbol{0}.$\n\n\nusing\n \nConstrainedLasso\n \n\n\n\n\n\nFirst we load and organize the data. \n\n\nwarming\n \n=\n \nreadcsv\n(\njoinpath\n(\nPkg\n.\ndir\n(\nConstrainedLasso\n),\ndocs/src/demo/misc/warming.csv\n),\n \nheader\n=\ntrue\n)[\n1\n]\n\n\nyear\n \n=\n \nwarming\n[\n:\n,\n \n1\n]\n\n\ny\n    \n=\n \nwarming\n[\n:\n,\n \n2\n]\n\n\nhcat\n(\nyear\n,\n \ny\n)\n\n\n\n\n\n\n166\u00d72 Array{Float64,2}:\n 1850.0  -0.375\n 1851.0  -0.223\n 1852.0  -0.224\n 1853.0  -0.271\n 1854.0  -0.246\n 1855.0  -0.271\n 1856.0  -0.352\n 1857.0  -0.46 \n 1858.0  -0.466\n 1859.0  -0.286\n 1860.0  -0.346\n 1861.0  -0.409\n 1862.0  -0.522\n    \u22ee          \n 2004.0   0.45 \n 2005.0   0.544\n 2006.0   0.505\n 2007.0   0.493\n 2008.0   0.395\n 2009.0   0.506\n 2010.0   0.559\n 2011.0   0.422\n 2012.0   0.47 \n 2013.0   0.499\n 2014.0   0.567\n 2015.0   0.746\n\n\n\n\n\nn\n \n=\n \np\n \n=\n \nsize\n(\ny\n,\n \n1\n)\n\n\nX\n \n=\n \neye\n(\nn\n)\n\n\n\n\n\n\n166\u00d7166 Array{Float64,2}:\n 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n \u22ee                        \u22ee              \u22f1       \u22ee                        \u22ee  \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  1.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  1.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  1.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n\n\n\n\n\nNow we define inequality constraints as specified earlier. \n\n\nC\n \n=\n \n[\neye\n(\np\n-\n1\n)\n \nzeros\n(\np\n-\n1\n,\n \n1\n)]\n \n-\n \n[\nzeros\n(\np\n-\n1\n,\n \n1\n)\n \neye\n(\np\n-\n1\n)]\n\n\n\n\n\n\n165\u00d7166 Array{Float64,2}:\n 1.0  -1.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   1.0  -1.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   1.0  -1.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   1.0  -1.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   1.0  -1.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   1.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n \u22ee                             \u22ee    \u22f1   \u22ee                             \u22ee  \n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0     -1.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   1.0  -1.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   1.0  -1.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   1.0  -1.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   1.0  -1.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   1.0  -1.0\n\n\n\n\n\nd\n \n=\n \nzeros\n(\nsize\n(\nC\n,\n \n1\n))\n\n\n\n\n\n\n165-element Array{Float64,1}:\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n \u22ee  \n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n\n\n\n\n\nThen we estimate constrained lasso solution path. \n\n\n\u03b2\u0302path\n,\n \n\u03c1path\n,\n \n=\n \nlsq_classopath\n(\nX\n,\n \ny\n;\n \nAineq\n \n=\n \nC\n,\n \nbineq\n \n=\n \nd\n);\n \n\n\n\n\n\n\u03b2\u0302path\n\n\n\n\n\n\n166\u00d7198 Array{Float64,2}:\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026  -0.323225  -0.366     -0.375   \n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026  -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026  -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n \u22ee                        \u22ee         \u22f1   \u22ee                             \n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026   0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026   0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0           0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0           0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0           0.447225   0.49       0.499   \n 0.0  0.0  0.0  0.0  0.0  0.0           0.515225   0.558      0.567   \n 0.0  0.0  0.0  0.0  0.0  0.011639  \u2026   0.699138   0.737854   0.746\n\n\n\n\n\nIn this formulation, isotonic regression is a special case of the constrained lasso with $\\rho=0.$ Below, \nmonoreg\n is coefficient estimates obtained using isotonic regression. \n\n\nmonoreg\n \n=\n \nreaddlm\n(\njoinpath\n(\nPkg\n.\ndir\n(\nConstrainedLasso\n),\ndocs/src/demo/misc/monoreg.txt\n))\n\n\n\n\n\n\n166\u00d71 Array{Float64,2}:\n -0.375   \n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n  \u22ee       \n  0.484571\n  0.484571\n  0.484571\n  0.484571\n  0.484571\n  0.48925 \n  0.48925 \n  0.48925 \n  0.48925 \n  0.499   \n  0.567   \n  0.746\n\n\n\n\n\nNow let's compare estimates by obtaining the largest absolute difference between isotonic regression constrained lasso fit. \n\n\nmaximum\n(\nabs\n.\n(\nmonoreg\n \n-\n \n\u03b2\u0302path\n[\n:\n,\n \nend\n]))\n\n\n\n\n\n\n1.2212453270876722e-15\n\n\n\n\n\nBelow is a figure that plots the constrained lasso fit at $\\rho = 0$ with the estimates using isotonic regression.\n\n\nusing\n \nPlots\n;\n \npyplot\n();\n \n\nscatter\n(\nyear\n,\n \ny\n,\n \nlabel\n=\nObserved Data\n,\n \nmarkerstrokecolor\n=\ndarkblue\n,\n \n        \nmarkercolor\n=\nwhite\n)\n\n\nscatter!\n(\nyear\n,\n \n\u03b2\u0302path\n[\n:\n,\n \nend\n],\n \nlabel\n=\nClassopath (\u03c1=0)\n,\n \n        \nmarkerstrokecolor\n=\nblack\n,\n \nmarker\n=:\nrect\n,\n \nmarkercolor\n=\nwhite\n)\n\n\nscatter!\n(\nyear\n,\n \nmonoreg\n,\n \nlabel\n=\nIsotonic Regression\n,\n \nmarker\n=:\nx\n,\n\n        \nmarkercolor\n=\nred\n,\n \nmarkersize\n=\n2\n)\n\n\nxaxis!\n(\nYear\n)\n \n\nyaxis!\n(\nTemperature anomalies\n)\n\n\ntitle!\n(\nGlobal Warming Data\n)\n\n\n\n\n\n\n\n\nFollow the \nlink\n to access the .ipynb file of this page.", 
            "title": "Global Warming Data"
        }, 
        {
            "location": "/demo/warming/#global-warming-data", 
            "text": "Here we consider the annual data on temperature anomalies. In general, temperature appears to increase monotonically over the time period of 1850 to 2015 ( Wu et al., 2001 ;  Tibshirani et al., 2011 ). This monotonicity can be imposed on the coeffcient estimates using the constrained lasso with the inequality constraint matrix:   \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac{1}{2} ||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{1em} \\boldsymbol{C\\beta} \\leq \\boldsymbol{d} \n\\end{split}   where    \n\\boldsymbol{C} = \\begin{pmatrix} \n1 & -1 &     &        &       &     & \\\\\n  & 1  & -1  &        &         &   & \\\\\n  &    &  1  & -1     &         &   & \\\\\n  &     &       & \\ddots & \\ddots &  & \\\\\n  &     &       &        &          & 1 & -1 \\\\\n\\end{pmatrix}   and $\\boldsymbol{d} = \\boldsymbol{0}.$  using   ConstrainedLasso    First we load and organize the data.   warming   =   readcsv ( joinpath ( Pkg . dir ( ConstrainedLasso ), docs/src/demo/misc/warming.csv ),   header = true )[ 1 ]  year   =   warming [ : ,   1 ]  y      =   warming [ : ,   2 ]  hcat ( year ,   y )   166\u00d72 Array{Float64,2}:\n 1850.0  -0.375\n 1851.0  -0.223\n 1852.0  -0.224\n 1853.0  -0.271\n 1854.0  -0.246\n 1855.0  -0.271\n 1856.0  -0.352\n 1857.0  -0.46 \n 1858.0  -0.466\n 1859.0  -0.286\n 1860.0  -0.346\n 1861.0  -0.409\n 1862.0  -0.522\n    \u22ee          \n 2004.0   0.45 \n 2005.0   0.544\n 2006.0   0.505\n 2007.0   0.493\n 2008.0   0.395\n 2009.0   0.506\n 2010.0   0.559\n 2011.0   0.422\n 2012.0   0.47 \n 2013.0   0.499\n 2014.0   0.567\n 2015.0   0.746  n   =   p   =   size ( y ,   1 )  X   =   eye ( n )   166\u00d7166 Array{Float64,2}:\n 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n \u22ee                        \u22ee              \u22f1       \u22ee                        \u22ee  \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  1.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  1.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  1.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  1.0  Now we define inequality constraints as specified earlier.   C   =   [ eye ( p - 1 )   zeros ( p - 1 ,   1 )]   -   [ zeros ( p - 1 ,   1 )   eye ( p - 1 )]   165\u00d7166 Array{Float64,2}:\n 1.0  -1.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   1.0  -1.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   1.0  -1.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   1.0  -1.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   1.0  -1.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   1.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n \u22ee                             \u22ee    \u22f1   \u22ee                             \u22ee  \n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0     -1.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   1.0  -1.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   1.0  -1.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   1.0  -1.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   1.0  -1.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   1.0  -1.0  d   =   zeros ( size ( C ,   1 ))   165-element Array{Float64,1}:\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n \u22ee  \n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0  Then we estimate constrained lasso solution path.   \u03b2\u0302path ,   \u03c1path ,   =   lsq_classopath ( X ,   y ;   Aineq   =   C ,   bineq   =   d );    \u03b2\u0302path   166\u00d7198 Array{Float64,2}:\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026  -0.323225  -0.366     -0.375   \n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026  -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026  -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n \u22ee                        \u22ee         \u22f1   \u22ee                             \n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026   0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026   0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0           0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0           0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0           0.447225   0.49       0.499   \n 0.0  0.0  0.0  0.0  0.0  0.0           0.515225   0.558      0.567   \n 0.0  0.0  0.0  0.0  0.0  0.011639  \u2026   0.699138   0.737854   0.746  In this formulation, isotonic regression is a special case of the constrained lasso with $\\rho=0.$ Below,  monoreg  is coefficient estimates obtained using isotonic regression.   monoreg   =   readdlm ( joinpath ( Pkg . dir ( ConstrainedLasso ), docs/src/demo/misc/monoreg.txt ))   166\u00d71 Array{Float64,2}:\n -0.375   \n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n  \u22ee       \n  0.484571\n  0.484571\n  0.484571\n  0.484571\n  0.484571\n  0.48925 \n  0.48925 \n  0.48925 \n  0.48925 \n  0.499   \n  0.567   \n  0.746  Now let's compare estimates by obtaining the largest absolute difference between isotonic regression constrained lasso fit.   maximum ( abs . ( monoreg   -   \u03b2\u0302path [ : ,   end ]))   1.2212453270876722e-15  Below is a figure that plots the constrained lasso fit at $\\rho = 0$ with the estimates using isotonic regression.  using   Plots ;   pyplot ();   scatter ( year ,   y ,   label = Observed Data ,   markerstrokecolor = darkblue ,  \n         markercolor = white )  scatter! ( year ,   \u03b2\u0302path [ : ,   end ],   label = Classopath (\u03c1=0) ,  \n         markerstrokecolor = black ,   marker =: rect ,   markercolor = white )  scatter! ( year ,   monoreg ,   label = Isotonic Regression ,   marker =: x , \n         markercolor = red ,   markersize = 2 )  xaxis! ( Year )   yaxis! ( Temperature anomalies )  title! ( Global Warming Data )    Follow the  link  to access the .ipynb file of this page.", 
            "title": "Global Warming Data"
        }, 
        {
            "location": "/demo/tumor/", 
            "text": "Brain Tumor Data\n\n\nHere we estimate a generalized lasso model (sparse fused lasso) via the constrained lasso. \n\n\nIn this example, we use a version of the comparative genomic hybridization (CGH) data from \nBredel et al. (2005)\n that was modified and studied by \nTibshirani and Wang (2008)\n.\n\n\nThe dataset here contains CGH measurements from 2 glioblastoma multiforme (GBM) brain tumors. \nTibshirani and Wang (2008)\n proposed using the sparse fused lasso to approximate the CGH signal by a sparse, piecewise constant function in order to determine the areas with non-zero values, as positive (negative) CGH values correspond to possible gains (losses). The sparse fused lasso (\nTibshirani et al., 2005\n) is given by\n\n\n\n\n\n\\begin{split}\n\\text{minimize} \\hspace{1em} \\frac{1}{2} ||\\boldsymbol{y}-\\boldsymbol{\\beta}||_2^2 + \\rho_1||\\boldsymbol{\\beta}||_1 + \\rho_2\\sum_{j=2}^p |\\beta_j - \\beta_{j-1}| \\hspace{5em} (1)\n\\end{split}\n\n\n\n\n\nThe sparse fused lasso is a special case of the generalized lasso with the penalty matrix. Therefore, the problem $(1)$ is equivalent to the following: \n\n\n\n\n\n\\begin{split} \n\\text{minimize} \\hspace{1em} \\frac{1}{2} ||\\boldsymbol{y}-\\boldsymbol{X\\beta}||_2^2 + \\rho ||\\boldsymbol{D\\beta}||_1 \\hspace{5em} (2)\n\\end{split}\n\n\n\n\n\nwhere \n\n\n\n\n\n\\boldsymbol{D} = \\begin{pmatrix} \n1 & -1 &     &        &       &     & \\\\\n  & 1  & -1  &        &         &   & \\\\\n  &    &  1  & -1     &         &   & \\\\\n  &     &       & \\ddots & \\ddots &  & \\\\\n  &     &       &        &          & 1 & -1 \\\\\n1 &  &     &          &       &     & \\\\\n  & 1  &   &          &         &   & \\\\\n  &    &  \\ddots  &       &         &   & \\\\\n  &     &          & & \\ddots & & \\\\\n  &     &            &      &       & 1 & \\\\\n  &     &       &        &              &  & 1\\\\  \n\\end{pmatrix} \\in \\mathbb{R}^{(2P-1)\\times p}.\n\n\n\n\n\nAs discussed in \nGaines, B.R. and Zhou, H., (2016)\n, the sparse fused lasso can be reformulated and solved as a constrained lasso problem. (For details of the reformulation, see Section 2 of [\n3\n]). Here, we demonstrate solving the generalized lasso problem as constrained lasso, using \ngenlasso.jl\n in \nConstrainedLasso\n package. Note that the performance of \ngenlasso.jl\n will be slow with large size problems because it involves the singular value decomposition (SVD).  \n\n\nusing\n \nConstrainedLasso\n\n\n\n\n\n\nWe load and organize the data first. Here, \ny\n is the response vector. The design matrix \nX\n is an identity matrix since the objective function in $(1)$ does not involve \nX\n. \n\n\ny\n \n=\n \nreaddlm\n(\njoinpath\n(\nPkg\n.\ndir\n(\nConstrainedLasso\n),\n \ndocs/src/demo/misc/tumor.txt\n))\n\n\n\n\n\n\n990\u00d71 Array{Float64,2}:\n  0.333661 \n -0.152838 \n  0.101485 \n -0.0342123\n  0.344761 \n  0.151108 \n  0.798318 \n  0.282754 \n  0.116233 \n -0.232173 \n -0.754577 \n  1.06762  \n -0.017392 \n  \u22ee        \n -0.170825 \n -0.161826 \n -0.348987 \n -0.001227 \n -0.221422 \n  0.552795 \n -0.603429 \n -0.447907 \n -0.317569 \n -0.728202 \n -0.505593 \n -0.147661\n\n\n\n\n\nn\n \n=\n \np\n \n=\n \nsize\n(\ny\n,\n \n1\n)\n\n\nX\n \n=\n \neye\n(\nn\n)\n\n\n\n\n\n\n990\u00d7990 Array{Float64,2}:\n 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n \u22ee                        \u22ee              \u22f1            \u22ee                      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  1.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  1.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  1.0\n\n\n\n\n\nFirst we create a penalty matrix \nD\n. \n\n\nD\n \n=\n \n[\neye\n(\np\n-\n1\n)\n \nzeros\n(\np\n-\n1\n,\n \n1\n)]\n \n-\n \n[\nzeros\n(\np\n-\n1\n,\n \n1\n)\n \neye\n(\np\n-\n1\n)]\n\n\n\n\n\n\n989\u00d7990 Array{Float64,2}:\n 1.0  -1.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   1.0  -1.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   1.0  -1.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   1.0  -1.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   1.0  -1.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   1.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n \u22ee                             \u22ee    \u22f1         \u22ee                          \n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0     -1.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      1.0  -1.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   1.0  -1.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   1.0  -1.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   1.0  -1.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   1.0  -1.0\n\n\n\n\n\n\u03b2\u0302path\n,\n \n\u03c1path\n \n=\n \ngenlasso\n(\nX\n,\n \ny\n;\n \nD\n \n=\n \nD\n)\n\n\n\n\n\n\n\u03b2\u0302path\n\n\n\n\n\n\n990\u00d7989 Array{Float64,2}:\n -0.0178926  -0.0046696   0.118695    0.258754   \u2026   0.333516     0.333644  \n -0.0178926  -0.0046696   0.118695    0.258754      -0.152548    -0.152803  \n -0.0178926  -0.0046696   0.118695    0.258754       0.101194     0.10145   \n -0.0178926  -0.0046696   0.118695    0.258754      -0.0339217   -0.0341772 \n -0.0178926  -0.0046696   0.118695    0.258754       0.34447      0.344726  \n -0.0178926  -0.0046696   0.118695    0.258754   \u2026   0.151399     0.151143  \n -0.0178926  -0.0046696   0.118695    0.258754       0.798027     0.798283  \n -0.0178926  -0.0046696   0.118695    0.258754       0.282754     0.282754  \n -0.0178926  -0.0046696   0.118695    0.258754       0.116233     0.116233  \n -0.0178926  -0.0046696   0.118695    0.258754      -0.232173    -0.232173  \n -0.0178926  -0.0046696   0.118695    0.258754   \u2026  -0.754287    -0.754542  \n -0.0178926  -0.0046696   0.118695    0.258754       1.06733      1.06759   \n -0.0178926  -0.0046696   0.118695    0.258754      -0.0171014   -0.0173569 \n  \u22ee                                              \u22f1                          \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.170534    -0.17079   \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.162116    -0.161861  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215  \u2026  -0.348696    -0.348952  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.00151764  -0.00126215\n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.221131    -0.221386  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215      0.552504     0.55276   \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.603138    -0.603394  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215  \u2026  -0.447907    -0.447907  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.31786     -0.317604  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.727912    -0.728167  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.505593    -0.505593  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.147807    -0.147679\n\n\n\n\n\n\u03c1path\n\n\n\n\n\n\n989-element Array{Float64,1}:\n 138.641      \n 135.983      \n 112.914      \n  86.7231     \n  38.6934     \n  33.6742     \n  33.4223     \n  29.0274     \n  16.4677     \n  14.8989     \n  13.7994     \n  11.5303     \n   7.45317    \n   \u22ee          \n   0.00336078 \n   0.00319116 \n   0.00312939 \n   0.0022499  \n   0.00129357 \n   0.00118782 \n   0.000983542\n   0.000623544\n   0.000246343\n   0.00017132 \n   0.000145322\n   0.0\n\n\n\n\n\nWe plot the constrained lasso solution path below. \n\n\nusing\n \nPlots\n;\n \npyplot\n();\n \n\n\u03c1newpath\n \n=\n \n\u03c1path\n[\n1\n:\nend\n-\n1\n]\n \n# exclude \u03c1=0\n\n\n\u03b2newpath\n \n=\n \n\u03b2\u0302path\n[\n:\n,\n \n1\n:\nend\n-\n1\n]\n\n\nplot\n(\nlog\n.\n(\n\u03c1newpath\n),\n \n\u03b2newpath\n,\n \nlabel\n=\n,\n \nxaxis\n \n=\n \n(\nlog(\u03c1)\n),\n \nyaxis\n \n=\n \n(\n\u03b2\u0302(\u03c1)\n),\n \nwidth\n=\n0.5\n)\n\n\ntitle!\n(\nBrain Tumor Data: Solution Path via Constrained Lasso\n)\n\n\n\n\n\n\n\n\nsavefig\n(\nmisc/tumor1.svg\n);\n\n\n\n\n\n\nNow let's compare our estimates with those from generalized lasso.  \n\n\nVariables \nlambda_path\n and \nbeta_path_fused\n are lambda values and estimated beta coefficients, respectively, obtained from \ngenlasso\n package in \nR\n. \n\n\nlambda_path\n \n=\n \nreaddlm\n(\njoinpath\n(\nPkg\n.\ndir\n(\nConstrainedLasso\n),\ndocs/src/demo/misc/lambda_path.txt\n))\n\n\n\n\n\n\n989\u00d71 Array{Float64,2}:\n 138.641      \n 135.983      \n 112.914      \n  86.7231     \n  38.6934     \n  33.6742     \n  33.4223     \n  29.0274     \n  16.4677     \n  14.8989     \n  13.7994     \n  11.5303     \n   7.45317    \n   \u22ee          \n   0.00336078 \n   0.00319116 \n   0.00312939 \n   0.0022499  \n   0.00129357 \n   0.00118782 \n   0.000983542\n   0.000623544\n   0.000246343\n   0.00017132 \n   0.000145321\n   1.7577e-5\n\n\n\n\n\nbeta_path_fused\n \n=\n \nreaddlm\n(\njoinpath\n(\nPkg\n.\ndir\n(\nConstrainedLasso\n),\ndocs/src/demo/misc/beta_path_fused.txt\n))[\n2\n:\nend\n,\n \n:\n]\n\n\n\n\n\n\n990\u00d7989 Array{Float64,2}:\n -0.0178926  -0.0046696   0.118694    0.258754   \u2026   0.333516     0.333644  \n -0.0178926  -0.0046696   0.118694    0.258754      -0.152548    -0.152803  \n -0.0178926  -0.0046696   0.118694    0.258754       0.101194     0.10145   \n -0.0178926  -0.0046696   0.118694    0.258754      -0.0339217   -0.0341772 \n -0.0178926  -0.0046696   0.118694    0.258754       0.344471     0.344726  \n -0.0178926  -0.0046696   0.118694    0.258754   \u2026   0.151399     0.151143  \n -0.0178926  -0.0046696   0.118694    0.258754       0.798027     0.798283  \n -0.0178926  -0.0046696   0.118694    0.258754       0.282754     0.282754  \n -0.0178926  -0.0046696   0.118694    0.258754       0.116233     0.116233  \n -0.0178926  -0.0046696   0.118694    0.258754      -0.232173    -0.232173  \n -0.0178926  -0.0046696   0.118694    0.258754   \u2026  -0.754287    -0.754542  \n -0.0178926  -0.0046696   0.118694    0.258754       1.06733      1.06759   \n -0.0178926  -0.0046696   0.118694    0.258754      -0.0171014   -0.0173569 \n  \u22ee                                              \u22f1                          \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.170535    -0.17079   \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.162116    -0.161861  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215  \u2026  -0.348696    -0.348952  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.00151764  -0.00126215\n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.221131    -0.221386  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215      0.552504     0.55276   \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.603138    -0.603394  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215  \u2026  -0.447907    -0.447907  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.31786     -0.317604  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.727912    -0.728167  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.505593    -0.505593  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.147807    -0.147679\n\n\n\n\n\nThe following figure plots generalized lasso solution path. \n\n\nplot\n(\nlog\n.\n(\nlambda_path\n),\n \nbeta_path_fused\n,\n \nlabel\n=\n,\n \nxaxis\n \n=\n \n(\nlog(\u03bb)\n),\n \nyaxis\n \n=\n \n(\n\u03b2\u0302(\u03bb)\n),\n \nwidth\n=\n0.5\n)\n\n\ntitle!\n(\nBrain Tumor Data: Generalized Lasso Solution Path\n)\n\n\n\n\n\n\n\n\nsavefig\n(\nmisc/tumor2.svg\n)\n\n\n\n\n\n\nNow we extract common values of $\\rho$ and compare estimates at those values. \n\n\nsame\u03c1\n \n=\n \nintersect\n(\nround\n.\n(\n\u03c1path\n,\n \n4\n),\n \nround\n.\n(\nlambda_path\n,\n \n4\n))\n\n\nsame\u03c1_err\n \n=\n \nFloat64\n[]\n\n\nfor\n \ni\n \nin\n \neachindex\n(\nsame\u03c1\n)\n\n \ncur\u03c1\n \n=\n \nsame\u03c1\n[\ni\n]\n\n \nidx1\n \n=\n \nfindmin\n(\nabs\n.\n(\n\u03c1path\n \n-\n \ncur\u03c1\n))[\n2\n]\n\n \nidx2\n \n=\n \nfindmin\n(\nabs\n.\n(\nlambda_path\n \n-\n \ncur\u03c1\n))[\n2\n]\n\n \npush!\n(\nsame\u03c1_err\n,\n \nmaximum\n(\nabs\n.\n(\n\u03b2\u0302path\n[\n:\n,\n \nidx1\n]\n \n-\n \nbeta_path_fused\n[\n:\n,\n \nidx2\n])))\n\n\nend\n\n\nsame\u03c1_err\n\n\n\n\n\n\n988-element Array{Any,1}:\n 1.22121e-9\n 2.47148e-9\n 4.33354e-9\n 4.3335e-9 \n 3.64469e-8\n 4.41366e-8\n 3.35e-8   \n 4.53911e-8\n 4.29568e-7\n 3.18004e-7\n 4.94452e-7\n 2.62003e-7\n 3.8775e-7 \n \u22ee         \n 5.00008e-7\n 4.89495e-7\n 4.87999e-7\n 4.95e-7   \n 4.78989e-7\n 4.86001e-7\n 4.96003e-7\n 4.99498e-7\n 4.86994e-7\n 4.86994e-7\n 4.89997e-7\n 4.83994e-7\n\n\n\n\n\nBelow are the mean, median, and maximum of the errors between estimated coefficients at common $\\rho$ values. \n\n\nprintln\n([\nmean\n(\nsame\u03c1_err\n);\n \nmedian\n(\nsame\u03c1_err\n);\n \nmaximum\n(\nsame\u03c1_err\n)])\n\n\n\n\n\n\n[4.77914e-7, 4.80866e-7, 5.00013e-7]\n\n\n\n\n\nFollow the \nlink\n to access the .ipynb file of this page.", 
            "title": "Brain Tumor Data"
        }, 
        {
            "location": "/demo/tumor/#brain-tumor-data", 
            "text": "Here we estimate a generalized lasso model (sparse fused lasso) via the constrained lasso.   In this example, we use a version of the comparative genomic hybridization (CGH) data from  Bredel et al. (2005)  that was modified and studied by  Tibshirani and Wang (2008) .  The dataset here contains CGH measurements from 2 glioblastoma multiforme (GBM) brain tumors.  Tibshirani and Wang (2008)  proposed using the sparse fused lasso to approximate the CGH signal by a sparse, piecewise constant function in order to determine the areas with non-zero values, as positive (negative) CGH values correspond to possible gains (losses). The sparse fused lasso ( Tibshirani et al., 2005 ) is given by   \n\\begin{split}\n\\text{minimize} \\hspace{1em} \\frac{1}{2} ||\\boldsymbol{y}-\\boldsymbol{\\beta}||_2^2 + \\rho_1||\\boldsymbol{\\beta}||_1 + \\rho_2\\sum_{j=2}^p |\\beta_j - \\beta_{j-1}| \\hspace{5em} (1)\n\\end{split}   The sparse fused lasso is a special case of the generalized lasso with the penalty matrix. Therefore, the problem $(1)$ is equivalent to the following:    \n\\begin{split} \n\\text{minimize} \\hspace{1em} \\frac{1}{2} ||\\boldsymbol{y}-\\boldsymbol{X\\beta}||_2^2 + \\rho ||\\boldsymbol{D\\beta}||_1 \\hspace{5em} (2)\n\\end{split}   where    \n\\boldsymbol{D} = \\begin{pmatrix} \n1 & -1 &     &        &       &     & \\\\\n  & 1  & -1  &        &         &   & \\\\\n  &    &  1  & -1     &         &   & \\\\\n  &     &       & \\ddots & \\ddots &  & \\\\\n  &     &       &        &          & 1 & -1 \\\\\n1 &  &     &          &       &     & \\\\\n  & 1  &   &          &         &   & \\\\\n  &    &  \\ddots  &       &         &   & \\\\\n  &     &          & & \\ddots & & \\\\\n  &     &            &      &       & 1 & \\\\\n  &     &       &        &              &  & 1\\\\  \n\\end{pmatrix} \\in \\mathbb{R}^{(2P-1)\\times p}.   As discussed in  Gaines, B.R. and Zhou, H., (2016) , the sparse fused lasso can be reformulated and solved as a constrained lasso problem. (For details of the reformulation, see Section 2 of [ 3 ]). Here, we demonstrate solving the generalized lasso problem as constrained lasso, using  genlasso.jl  in  ConstrainedLasso  package. Note that the performance of  genlasso.jl  will be slow with large size problems because it involves the singular value decomposition (SVD).    using   ConstrainedLasso   We load and organize the data first. Here,  y  is the response vector. The design matrix  X  is an identity matrix since the objective function in $(1)$ does not involve  X .   y   =   readdlm ( joinpath ( Pkg . dir ( ConstrainedLasso ),   docs/src/demo/misc/tumor.txt ))   990\u00d71 Array{Float64,2}:\n  0.333661 \n -0.152838 \n  0.101485 \n -0.0342123\n  0.344761 \n  0.151108 \n  0.798318 \n  0.282754 \n  0.116233 \n -0.232173 \n -0.754577 \n  1.06762  \n -0.017392 \n  \u22ee        \n -0.170825 \n -0.161826 \n -0.348987 \n -0.001227 \n -0.221422 \n  0.552795 \n -0.603429 \n -0.447907 \n -0.317569 \n -0.728202 \n -0.505593 \n -0.147661  n   =   p   =   size ( y ,   1 )  X   =   eye ( n )   990\u00d7990 Array{Float64,2}:\n 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n \u22ee                        \u22ee              \u22f1            \u22ee                      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  1.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  1.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  1.0  First we create a penalty matrix  D .   D   =   [ eye ( p - 1 )   zeros ( p - 1 ,   1 )]   -   [ zeros ( p - 1 ,   1 )   eye ( p - 1 )]   989\u00d7990 Array{Float64,2}:\n 1.0  -1.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   1.0  -1.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   1.0  -1.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   1.0  -1.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   1.0  -1.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   1.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n \u22ee                             \u22ee    \u22f1         \u22ee                          \n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0     -1.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      1.0  -1.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   1.0  -1.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   1.0  -1.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   1.0  -1.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   1.0  -1.0  \u03b2\u0302path ,   \u03c1path   =   genlasso ( X ,   y ;   D   =   D )   \u03b2\u0302path   990\u00d7989 Array{Float64,2}:\n -0.0178926  -0.0046696   0.118695    0.258754   \u2026   0.333516     0.333644  \n -0.0178926  -0.0046696   0.118695    0.258754      -0.152548    -0.152803  \n -0.0178926  -0.0046696   0.118695    0.258754       0.101194     0.10145   \n -0.0178926  -0.0046696   0.118695    0.258754      -0.0339217   -0.0341772 \n -0.0178926  -0.0046696   0.118695    0.258754       0.34447      0.344726  \n -0.0178926  -0.0046696   0.118695    0.258754   \u2026   0.151399     0.151143  \n -0.0178926  -0.0046696   0.118695    0.258754       0.798027     0.798283  \n -0.0178926  -0.0046696   0.118695    0.258754       0.282754     0.282754  \n -0.0178926  -0.0046696   0.118695    0.258754       0.116233     0.116233  \n -0.0178926  -0.0046696   0.118695    0.258754      -0.232173    -0.232173  \n -0.0178926  -0.0046696   0.118695    0.258754   \u2026  -0.754287    -0.754542  \n -0.0178926  -0.0046696   0.118695    0.258754       1.06733      1.06759   \n -0.0178926  -0.0046696   0.118695    0.258754      -0.0171014   -0.0173569 \n  \u22ee                                              \u22f1                          \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.170534    -0.17079   \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.162116    -0.161861  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215  \u2026  -0.348696    -0.348952  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.00151764  -0.00126215\n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.221131    -0.221386  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215      0.552504     0.55276   \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.603138    -0.603394  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215  \u2026  -0.447907    -0.447907  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.31786     -0.317604  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.727912    -0.728167  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.505593    -0.505593  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.147807    -0.147679  \u03c1path   989-element Array{Float64,1}:\n 138.641      \n 135.983      \n 112.914      \n  86.7231     \n  38.6934     \n  33.6742     \n  33.4223     \n  29.0274     \n  16.4677     \n  14.8989     \n  13.7994     \n  11.5303     \n   7.45317    \n   \u22ee          \n   0.00336078 \n   0.00319116 \n   0.00312939 \n   0.0022499  \n   0.00129357 \n   0.00118782 \n   0.000983542\n   0.000623544\n   0.000246343\n   0.00017132 \n   0.000145322\n   0.0  We plot the constrained lasso solution path below.   using   Plots ;   pyplot ();   \u03c1newpath   =   \u03c1path [ 1 : end - 1 ]   # exclude \u03c1=0  \u03b2newpath   =   \u03b2\u0302path [ : ,   1 : end - 1 ]  plot ( log . ( \u03c1newpath ),   \u03b2newpath ,   label = ,   xaxis   =   ( log(\u03c1) ),   yaxis   =   ( \u03b2\u0302(\u03c1) ),   width = 0.5 )  title! ( Brain Tumor Data: Solution Path via Constrained Lasso )    savefig ( misc/tumor1.svg );   Now let's compare our estimates with those from generalized lasso.    Variables  lambda_path  and  beta_path_fused  are lambda values and estimated beta coefficients, respectively, obtained from  genlasso  package in  R .   lambda_path   =   readdlm ( joinpath ( Pkg . dir ( ConstrainedLasso ), docs/src/demo/misc/lambda_path.txt ))   989\u00d71 Array{Float64,2}:\n 138.641      \n 135.983      \n 112.914      \n  86.7231     \n  38.6934     \n  33.6742     \n  33.4223     \n  29.0274     \n  16.4677     \n  14.8989     \n  13.7994     \n  11.5303     \n   7.45317    \n   \u22ee          \n   0.00336078 \n   0.00319116 \n   0.00312939 \n   0.0022499  \n   0.00129357 \n   0.00118782 \n   0.000983542\n   0.000623544\n   0.000246343\n   0.00017132 \n   0.000145321\n   1.7577e-5  beta_path_fused   =   readdlm ( joinpath ( Pkg . dir ( ConstrainedLasso ), docs/src/demo/misc/beta_path_fused.txt ))[ 2 : end ,   : ]   990\u00d7989 Array{Float64,2}:\n -0.0178926  -0.0046696   0.118694    0.258754   \u2026   0.333516     0.333644  \n -0.0178926  -0.0046696   0.118694    0.258754      -0.152548    -0.152803  \n -0.0178926  -0.0046696   0.118694    0.258754       0.101194     0.10145   \n -0.0178926  -0.0046696   0.118694    0.258754      -0.0339217   -0.0341772 \n -0.0178926  -0.0046696   0.118694    0.258754       0.344471     0.344726  \n -0.0178926  -0.0046696   0.118694    0.258754   \u2026   0.151399     0.151143  \n -0.0178926  -0.0046696   0.118694    0.258754       0.798027     0.798283  \n -0.0178926  -0.0046696   0.118694    0.258754       0.282754     0.282754  \n -0.0178926  -0.0046696   0.118694    0.258754       0.116233     0.116233  \n -0.0178926  -0.0046696   0.118694    0.258754      -0.232173    -0.232173  \n -0.0178926  -0.0046696   0.118694    0.258754   \u2026  -0.754287    -0.754542  \n -0.0178926  -0.0046696   0.118694    0.258754       1.06733      1.06759   \n -0.0178926  -0.0046696   0.118694    0.258754      -0.0171014   -0.0173569 \n  \u22ee                                              \u22f1                          \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.170535    -0.17079   \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.162116    -0.161861  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215  \u2026  -0.348696    -0.348952  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.00151764  -0.00126215\n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.221131    -0.221386  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215      0.552504     0.55276   \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.603138    -0.603394  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215  \u2026  -0.447907    -0.447907  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.31786     -0.317604  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.727912    -0.728167  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.505593    -0.505593  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.147807    -0.147679  The following figure plots generalized lasso solution path.   plot ( log . ( lambda_path ),   beta_path_fused ,   label = ,   xaxis   =   ( log(\u03bb) ),   yaxis   =   ( \u03b2\u0302(\u03bb) ),   width = 0.5 )  title! ( Brain Tumor Data: Generalized Lasso Solution Path )    savefig ( misc/tumor2.svg )   Now we extract common values of $\\rho$ and compare estimates at those values.   same\u03c1   =   intersect ( round . ( \u03c1path ,   4 ),   round . ( lambda_path ,   4 ))  same\u03c1_err   =   Float64 []  for   i   in   eachindex ( same\u03c1 ) \n  cur\u03c1   =   same\u03c1 [ i ] \n  idx1   =   findmin ( abs . ( \u03c1path   -   cur\u03c1 ))[ 2 ] \n  idx2   =   findmin ( abs . ( lambda_path   -   cur\u03c1 ))[ 2 ] \n  push! ( same\u03c1_err ,   maximum ( abs . ( \u03b2\u0302path [ : ,   idx1 ]   -   beta_path_fused [ : ,   idx2 ])))  end  same\u03c1_err   988-element Array{Any,1}:\n 1.22121e-9\n 2.47148e-9\n 4.33354e-9\n 4.3335e-9 \n 3.64469e-8\n 4.41366e-8\n 3.35e-8   \n 4.53911e-8\n 4.29568e-7\n 3.18004e-7\n 4.94452e-7\n 2.62003e-7\n 3.8775e-7 \n \u22ee         \n 5.00008e-7\n 4.89495e-7\n 4.87999e-7\n 4.95e-7   \n 4.78989e-7\n 4.86001e-7\n 4.96003e-7\n 4.99498e-7\n 4.86994e-7\n 4.86994e-7\n 4.89997e-7\n 4.83994e-7  Below are the mean, median, and maximum of the errors between estimated coefficients at common $\\rho$ values.   println ([ mean ( same\u03c1_err );   median ( same\u03c1_err );   maximum ( same\u03c1_err )])   [4.77914e-7, 4.80866e-7, 5.00013e-7]  Follow the  link  to access the .ipynb file of this page.", 
            "title": "Brain Tumor Data"
        }, 
        {
            "location": "/demo/micro/", 
            "text": "Microbiome Data\n\n\nThis real data application uses microbiome data [\n8\n]. The dataset itself contains information on 160 bacteria genera from 37 patients. The bacteria counts were $\\log_2$-transformed and normalized to have a constant average across samples.\n\n\nFirst, let's load and organize data.\n\n\nzerosum\n \n=\n \nreadcsv\n(\njoinpath\n(\nPkg\n.\ndir\n(\nConstrainedLasso\n),\n \ndocs/src/demo/misc/zerosum.csv\n),\n \nheader\n=\ntrue\n)[\n1\n]\n\n\ny\n \n=\n \nzerosum\n[\n:\n,\n \n1\n]\n\n\n\n\n\n\n37-element Array{Float64,1}:\n   3.1158 \n   3.21448\n -11.1341 \n  -5.13988\n  -4.8247 \n  -4.79219\n -11.5719 \n  -5.77868\n   4.97972\n  -3.38806\n  -9.90973\n   3.07384\n   2.77814\n   \u22ee      \n  -7.41032\n   4.70871\n   1.49355\n   3.93736\n  -3.29476\n -10.9239 \n -11.021  \n   3.18789\n  -8.73771\n -11.499  \n   3.66284\n  -8.88277\n\n\n\n\n\nX\n \n=\n \nzerosum\n[\n:\n,\n \n2\n:\nend\n]\n\n\n\n\n\n\n37\u00d7160 Array{Float64,2}:\n 0.0  3.32193  0.0  1.0      10.5304   \u2026  0.0      0.0  0.0  0.0  12.513  \n 0.0  4.08746  0.0  0.0       7.35755     0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  3.32193  0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  1.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   1.0    \n 0.0  3.16993  0.0  0.0       0.0      \u2026  0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       1.0         1.58496  0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       4.64386     0.0      0.0  0.0  0.0   1.58496\n 0.0  1.0      0.0  0.0       0.0         0.0      0.0  0.0  1.0   0.0    \n 0.0  0.0      0.0  0.0       0.0      \u2026  0.0      0.0  0.0  0.0   1.0    \n 0.0  5.45943  0.0  0.0       3.90689     0.0      0.0  0.0  0.0   1.58496\n 0.0  4.70044  0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0    \n \u22ee                                     \u22f1  \u22ee                               \n 0.0  0.0      0.0  0.0       0.0      \u2026  0.0      0.0  0.0  0.0   0.0    \n 0.0  3.58496  0.0  0.0      12.2886      0.0      0.0  0.0  0.0   7.39232\n 0.0  4.80735  0.0  6.87036   8.01123     0.0      0.0  0.0  0.0   3.0    \n 0.0  0.0      0.0  0.0       5.93074     0.0      0.0  0.0  0.0   0.0    \n 0.0  7.7211   0.0  0.0       1.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       0.0      \u2026  0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      1.0  0.0       1.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  4.08746  0.0  0.0       3.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   1.0    \n 0.0  5.04439  0.0  0.0       0.0      \u2026  0.0      0.0  0.0  0.0  10.444  \n 0.0  0.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0\n\n\n\n\n\nAltenbuchinger et al.\n demonstrated that a sum-to-zero constraint is useful anytime the normalization of data relative to some reference point results in proportional data, as is often the case in biological applications, since the analysis using the constraint is insensitive to the choice of the reference. \nAltenbuchinger et al.\n derived a coordinate descent algorithm for the elastic net with a zero-sum constraint, \n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac{1}{2} ||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho\\Big(||\\boldsymbol{\\beta}||_1 + \\frac{1-\\alpha}{2}||\\boldsymbol{\\beta}||_2^2\\Big) \\\\\n& \\text{subject to} \\hspace{1em} \\sum_j \\beta_j = 0\n\\end{split}\n\n\n\n\n\nbut the focus of their analysis corresponds to $\\alpha = 1$. Hence the problem is reduced to the constrained lasso.\n\n\nWe set up the zero-sum constraint.\n\n\nn\n,\n \np\n \n=\n \nsize\n(\nX\n)\n\n\nAeq\n \n=\n \nones\n(\n1\n,\n \np\n)\n\n\nbeq\n \n=\n \n0.0\n\n\nm1\n \n=\n \nsize\n(\nAeq\n,\n \n1\n);\n\n\n\n\n\n\nNow we estimate the constrained lasso solution path using path algorithm.\n\n\nusing\n \nConstrainedLasso\n\n\n\u03b2\u0302path\n,\n \n\u03c1path\n,\n \n=\n \nlsq_classopath\n(\nX\n,\n \ny\n;\n \nAeq\n \n=\n \nAeq\n,\n \nbeq\n \n=\n \nbeq\n)\n\n\n\n\n\n\n\u03b2\u0302path\n\n\n\n\n\n\n160\u00d779 Array{Float64,2}:\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026   0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.484646   0.488903   0.490802 \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     -0.209662  -0.248845  -0.253282 \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026   0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.897273   0.888551   0.889344 \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026   0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n \u22ee                        \u22ee              \u22f1                                  \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026   0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0       -0.0220978\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026   0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.659693   0.659783   0.658592\n\n\n\n\n\nThen we calculate $L_1$ norm of coefficients at each $\\rho$.\n\n\nnorm1path\n \n=\n \nzeros\n(\nsize\n(\n\u03b2\u0302path\n,\n \n2\n))\n\n\nfor\n \ni\n \nin\n \neachindex\n(\nnorm1path\n)\n\n    \nnorm1path\n[\ni\n]\n \n=\n \nnorm\n(\n\u03b2\u0302path\n[\n:\n,\n \ni\n],\n \n1\n)\n\n\nend\n\n\nnorm1path\n\n\n\n\n\n\n79-element Array{Float64,1}:\n  0.0     \n  0.0     \n  0.1462  \n  0.236204\n  0.368326\n  0.75722 \n  1.08009 \n  1.43022 \n  1.45122 \n  1.6103  \n  1.95359 \n  1.9595  \n  1.96205 \n  \u22ee       \n 11.2506  \n 11.5025  \n 11.8854  \n 11.9934  \n 12.0591  \n 12.2261  \n 12.2836  \n 12.7327  \n 12.9722  \n 13.0533  \n 13.221   \n 13.2708\n\n\n\n\n\nNow, let's plot the solution path, $\\widehat{\\boldsymbol{\\beta}}(\\rho)$, as a function of $||\\widehat{\\boldsymbol{\\beta}}(\\rho)||_1$ using constrained lasso.\n\n\nusing\n \nPlots\n;\n \npyplot\n();\n\n\nplot\n(\nnorm1path\n,\n \n\u03b2\u0302path\n,\n \nxaxis\n \n=\n \n(\n||\u03b2\u0302||\u2081\n),\n \nyaxis\n=\n(\n\u03b2\u0302\n),\n \nlabel\n=\n)\n\n\ntitle!\n(\nMicrobiome Data: Solution Path via Constrained Lasso\n)\n\n\n\n\n\n\n\n\nsavefig\n(\nmisc/micro.svg\n);\n \n\n\n\n\n\nFollow the \nlink\n to access the .ipynb file of this page.", 
            "title": "Microbiome Data"
        }, 
        {
            "location": "/demo/micro/#microbiome-data", 
            "text": "This real data application uses microbiome data [ 8 ]. The dataset itself contains information on 160 bacteria genera from 37 patients. The bacteria counts were $\\log_2$-transformed and normalized to have a constant average across samples.  First, let's load and organize data.  zerosum   =   readcsv ( joinpath ( Pkg . dir ( ConstrainedLasso ),   docs/src/demo/misc/zerosum.csv ),   header = true )[ 1 ]  y   =   zerosum [ : ,   1 ]   37-element Array{Float64,1}:\n   3.1158 \n   3.21448\n -11.1341 \n  -5.13988\n  -4.8247 \n  -4.79219\n -11.5719 \n  -5.77868\n   4.97972\n  -3.38806\n  -9.90973\n   3.07384\n   2.77814\n   \u22ee      \n  -7.41032\n   4.70871\n   1.49355\n   3.93736\n  -3.29476\n -10.9239 \n -11.021  \n   3.18789\n  -8.73771\n -11.499  \n   3.66284\n  -8.88277  X   =   zerosum [ : ,   2 : end ]   37\u00d7160 Array{Float64,2}:\n 0.0  3.32193  0.0  1.0      10.5304   \u2026  0.0      0.0  0.0  0.0  12.513  \n 0.0  4.08746  0.0  0.0       7.35755     0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  3.32193  0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  1.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   1.0    \n 0.0  3.16993  0.0  0.0       0.0      \u2026  0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       1.0         1.58496  0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       4.64386     0.0      0.0  0.0  0.0   1.58496\n 0.0  1.0      0.0  0.0       0.0         0.0      0.0  0.0  1.0   0.0    \n 0.0  0.0      0.0  0.0       0.0      \u2026  0.0      0.0  0.0  0.0   1.0    \n 0.0  5.45943  0.0  0.0       3.90689     0.0      0.0  0.0  0.0   1.58496\n 0.0  4.70044  0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0    \n \u22ee                                     \u22f1  \u22ee                               \n 0.0  0.0      0.0  0.0       0.0      \u2026  0.0      0.0  0.0  0.0   0.0    \n 0.0  3.58496  0.0  0.0      12.2886      0.0      0.0  0.0  0.0   7.39232\n 0.0  4.80735  0.0  6.87036   8.01123     0.0      0.0  0.0  0.0   3.0    \n 0.0  0.0      0.0  0.0       5.93074     0.0      0.0  0.0  0.0   0.0    \n 0.0  7.7211   0.0  0.0       1.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       0.0      \u2026  0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      1.0  0.0       1.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  4.08746  0.0  0.0       3.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   1.0    \n 0.0  5.04439  0.0  0.0       0.0      \u2026  0.0      0.0  0.0  0.0  10.444  \n 0.0  0.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0  Altenbuchinger et al.  demonstrated that a sum-to-zero constraint is useful anytime the normalization of data relative to some reference point results in proportional data, as is often the case in biological applications, since the analysis using the constraint is insensitive to the choice of the reference.  Altenbuchinger et al.  derived a coordinate descent algorithm for the elastic net with a zero-sum constraint,    \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac{1}{2} ||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho\\Big(||\\boldsymbol{\\beta}||_1 + \\frac{1-\\alpha}{2}||\\boldsymbol{\\beta}||_2^2\\Big) \\\\\n& \\text{subject to} \\hspace{1em} \\sum_j \\beta_j = 0\n\\end{split}   but the focus of their analysis corresponds to $\\alpha = 1$. Hence the problem is reduced to the constrained lasso.  We set up the zero-sum constraint.  n ,   p   =   size ( X )  Aeq   =   ones ( 1 ,   p )  beq   =   0.0  m1   =   size ( Aeq ,   1 );   Now we estimate the constrained lasso solution path using path algorithm.  using   ConstrainedLasso  \u03b2\u0302path ,   \u03c1path ,   =   lsq_classopath ( X ,   y ;   Aeq   =   Aeq ,   beq   =   beq )   \u03b2\u0302path   160\u00d779 Array{Float64,2}:\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026   0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.484646   0.488903   0.490802 \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     -0.209662  -0.248845  -0.253282 \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026   0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.897273   0.888551   0.889344 \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026   0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n \u22ee                        \u22ee              \u22f1                                  \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026   0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0       -0.0220978\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026   0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.659693   0.659783   0.658592  Then we calculate $L_1$ norm of coefficients at each $\\rho$.  norm1path   =   zeros ( size ( \u03b2\u0302path ,   2 ))  for   i   in   eachindex ( norm1path ) \n     norm1path [ i ]   =   norm ( \u03b2\u0302path [ : ,   i ],   1 )  end  norm1path   79-element Array{Float64,1}:\n  0.0     \n  0.0     \n  0.1462  \n  0.236204\n  0.368326\n  0.75722 \n  1.08009 \n  1.43022 \n  1.45122 \n  1.6103  \n  1.95359 \n  1.9595  \n  1.96205 \n  \u22ee       \n 11.2506  \n 11.5025  \n 11.8854  \n 11.9934  \n 12.0591  \n 12.2261  \n 12.2836  \n 12.7327  \n 12.9722  \n 13.0533  \n 13.221   \n 13.2708  Now, let's plot the solution path, $\\widehat{\\boldsymbol{\\beta}}(\\rho)$, as a function of $||\\widehat{\\boldsymbol{\\beta}}(\\rho)||_1$ using constrained lasso.  using   Plots ;   pyplot ();  plot ( norm1path ,   \u03b2\u0302path ,   xaxis   =   ( ||\u03b2\u0302||\u2081 ),   yaxis = ( \u03b2\u0302 ),   label = )  title! ( Microbiome Data: Solution Path via Constrained Lasso )    savefig ( misc/micro.svg );    Follow the  link  to access the .ipynb file of this page.", 
            "title": "Microbiome Data"
        }, 
        {
            "location": "/references/", 
            "text": "References\n\n\n\n\n[1] Altenbuchinger, M., Rehberg, T., Zacharias, H.U., St\u00e4mmler, F., Dettmer, K., Weber, D., Hiergeist, A., Gessner, A., Holler, E., Oefner, P.J. and Spang, R. (2016). Reference point insensitive molecular data analysis. \nBioinformatics\n, 33(2):219-226.\n\n\n[2] Bredel, M., Bredel, C., Juric, D., Harsh, G. R., Vogel, H., Recht, L. D., and Sikic, B. I. (2005). High-resolution genome-wide mapping of genetic alterations in human glial brain tumors. \nCancer Research\n, 65:4088\u20134096.\n\n\n[3] Gaines, B.R. and Zhou, H. (2016). Algorithms for fitting the constrained lasso. arXiv preprint arXiv:1611.01511.\n\n\n[4] Stamey, T.A., Kabalin, J.N., McNeal, J.E., Johnstone, I.M., Freiha, F., Redwine, E.A. and Yang, N. (1989). Prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate: II. radical prostatectomy treated patients. \nJournal of Urology\n 141(5):1076\u20131083.\n\n\n[5] Tibshirani, R., Saunders, M., Rosset, S., Zhu, J., and Knight, K. (2005), \u201cSparsity and Smoothness via the Fused Lasso,\u201d Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67, 91\u2013108.\n\n\n[6] Tibshirani, R. and Wang, P. (2008). Spatial smoothing and hot spot detection for CGH data using the fused lasso. \nBiostatistics\n, 9:18\u201329.\n\n\n[7] Tibshirani, R. J., Hoefling, H., and Tibshirani, R. (2011). Nearly-isotonic regression. \nTechnometrics\n, 53:54\u201361.\n\n\n[8] Weber, D., Oefner, P. J., Hiergeist, A., Koestler, J., Gessner, A., Weber, M., Hahn, J., Wolff, D., Sta \u0308mmler, F., Spang, R., Herr, W., Dettmer, K., and Holler, E. (2015). Low urinary undoxyl sulfate levels early after transplantation reflect a disrupted microbiome and are associated with poor outcome. \nBlood\n, 126:1723\u20131728.\n\n\n[9] Wu, W. B., Woodroofe, M., and Mentz, G. (2001). Isotonic regression: another look at the changepoint problem. \nBiometrika\n, 88:793\u2013804.", 
            "title": "References"
        }, 
        {
            "location": "/references/#references", 
            "text": "[1] Altenbuchinger, M., Rehberg, T., Zacharias, H.U., St\u00e4mmler, F., Dettmer, K., Weber, D., Hiergeist, A., Gessner, A., Holler, E., Oefner, P.J. and Spang, R. (2016). Reference point insensitive molecular data analysis.  Bioinformatics , 33(2):219-226.  [2] Bredel, M., Bredel, C., Juric, D., Harsh, G. R., Vogel, H., Recht, L. D., and Sikic, B. I. (2005). High-resolution genome-wide mapping of genetic alterations in human glial brain tumors.  Cancer Research , 65:4088\u20134096.  [3] Gaines, B.R. and Zhou, H. (2016). Algorithms for fitting the constrained lasso. arXiv preprint arXiv:1611.01511.  [4] Stamey, T.A., Kabalin, J.N., McNeal, J.E., Johnstone, I.M., Freiha, F., Redwine, E.A. and Yang, N. (1989). Prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate: II. radical prostatectomy treated patients.  Journal of Urology  141(5):1076\u20131083.  [5] Tibshirani, R., Saunders, M., Rosset, S., Zhu, J., and Knight, K. (2005), \u201cSparsity and Smoothness via the Fused Lasso,\u201d Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67, 91\u2013108.  [6] Tibshirani, R. and Wang, P. (2008). Spatial smoothing and hot spot detection for CGH data using the fused lasso.  Biostatistics , 9:18\u201329.  [7] Tibshirani, R. J., Hoefling, H., and Tibshirani, R. (2011). Nearly-isotonic regression.  Technometrics , 53:54\u201361.  [8] Weber, D., Oefner, P. J., Hiergeist, A., Koestler, J., Gessner, A., Weber, M., Hahn, J., Wolff, D., Sta \u0308mmler, F., Spang, R., Herr, W., Dettmer, K., and Holler, E. (2015). Low urinary undoxyl sulfate levels early after transplantation reflect a disrupted microbiome and are associated with poor outcome.  Blood , 126:1723\u20131728.  [9] Wu, W. B., Woodroofe, M., and Mentz, G. (2001). Isotonic regression: another look at the changepoint problem.  Biometrika , 88:793\u2013804.", 
            "title": "References"
        }
    ]
}