{
    "docs": [
        {
            "location": "/", 
            "text": "ConstrainedLasso\n\n\nConstrainedLasso\n solves the following problem\n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\boldsymbol{\\beta}||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\boldsymbol{A\\beta}=\\boldsymbol{b} \\text{ and } \\boldsymbol{C\\beta} \\leq \\boldsymbol{d}\n\\end{split}\n\n\n\n\n\nwhere\n\n\n\n\n$\\boldsymbol{y} \\in \\mathbb{R}^n$: the response vector\n\n\n$\\boldsymbol{X}\\in \\mathbb{R}^{n\\times p}$: the design matrix of predictor or covariates\n\n\n$\\boldsymbol{\\beta} \\in \\mathbb{R}^p$: the vector of unknown regression coefficients,\n\n\n$\\rho \\geq 0$: a tuning parameter that controls the amount of regularization.\n\n\n\n\n\n\nInstallation\n\n\nWithin Julia, use the package manager to install \nConstrainedLasso\n:\n\n\nPkg\n.\nclone\n(\ngit://github.com/Hua-Zhou/ConstrainedLasso.git\n)\n\n\n\n\n\n\nThis package supports Julia v0.6.\n\n\n\n\nCitation\n\n\nOriginal method paper on the constrained lasso is\n\n\nG.M. James, C. Paulson and P. Rusmevichientong. (2013) Penalized and constrained regression. \nhttp://www-bcf.usc.edu/~gareth/research/PAC.pdf\n\n\nIf you use \nConstrainedLasso\n package in your research, please cite the following paper on the algorithms:\n\n\nB.R. Gaines, H. Zhou. (2016) Algorithms for Fitting the Constrained Lasso. \nhttps://arxiv.org/abs/1611.01511", 
            "title": "Home"
        }, 
        {
            "location": "/#constrainedlasso", 
            "text": "ConstrainedLasso  solves the following problem   \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\boldsymbol{\\beta}||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\boldsymbol{A\\beta}=\\boldsymbol{b} \\text{ and } \\boldsymbol{C\\beta} \\leq \\boldsymbol{d}\n\\end{split}   where   $\\boldsymbol{y} \\in \\mathbb{R}^n$: the response vector  $\\boldsymbol{X}\\in \\mathbb{R}^{n\\times p}$: the design matrix of predictor or covariates  $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$: the vector of unknown regression coefficients,  $\\rho \\geq 0$: a tuning parameter that controls the amount of regularization.", 
            "title": "ConstrainedLasso"
        }, 
        {
            "location": "/#installation", 
            "text": "Within Julia, use the package manager to install  ConstrainedLasso :  Pkg . clone ( git://github.com/Hua-Zhou/ConstrainedLasso.git )   This package supports Julia v0.6.", 
            "title": "Installation"
        }, 
        {
            "location": "/#citation", 
            "text": "Original method paper on the constrained lasso is  G.M. James, C. Paulson and P. Rusmevichientong. (2013) Penalized and constrained regression.  http://www-bcf.usc.edu/~gareth/research/PAC.pdf  If you use  ConstrainedLasso  package in your research, please cite the following paper on the algorithms:  B.R. Gaines, H. Zhou. (2016) Algorithms for Fitting the Constrained Lasso.  https://arxiv.org/abs/1611.01511", 
            "title": "Citation"
        }, 
        {
            "location": "/interface/", 
            "text": "Interface\n\n\nFunctions exported from \nConstrainedLasso\n:\n\n\n#\n\n\nConstrainedLasso.lsq_constrsparsereg\n \n \nFunction\n.\n\n\n  \nlsq_constrsparsereg\n(\nX\n::\nAbstractMatrix\n{\nT\n}\n,\n \ny\n::\nAbstractVector\n{\nT\n}\n,\n \n\u03c1\n \n=\n \nzero\n(\nT\n);\n\n      \nAeq\n       \n::\n \nAbstractMatrix\n \n=\n \nzeros\n(\nT\n,\n \n0\n,\n \nsize\n(\nX\n,\n \n2\n)),\n\n      \nbeq\n       \n::\n \nUnion\n{\nAbstractVector,\n \nT\n}\n \n=\n \nzeros\n(\nT\n,\n \nsize\n(\nAeq\n,\n \n1\n)),\n\n      \nAineq\n     \n::\n \nAbstractMatrix\n \n=\n \nzeros\n(\nT\n,\n \n0\n,\n \nsize\n(\nX\n,\n \n2\n)),\n\n      \nbineq\n     \n::\n \nAbstractVector\n \n=\n \nzeros\n(\nT\n,\n \nsize\n(\nAineq\n,\n \n1\n)),\n\n      \nobswt\n     \n::\n \nAbstractVector\n \n=\n \nones\n(\nT\n,\n \nlength\n(\ny\n)),\n\n      \npenwt\n     \n::\n \nAbstractVector\n \n=\n \nones\n(\nT\n,\n \nsize\n(\nX\n,\n \n2\n)),\n\n      \nwarmstart\n \n::\n \nBool\n \n=\n \nfalse\n,\n\n      \nsolver\n \n=\n \nECOSSolver\n(\nmaxit\n=\n10e8\n,\n \nverbose\n=\n0\n)\n\n    \n)\n\n\n\n\n\n\nFit constrained lasso at fixed tuning parameter value(s) by minimizing     \n0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(penwt .* \u03b2)\n subject to linear constraints, using \nConvex.jl\n.\n\n\nArguments\n\n\n\n\nX\n       : predictor matrix.\n\n\ny\n       : response vector.\n\n\n\u03c1\n       : tuning parameter. Can be a number or a list of numbers. Default 0.\n\n\n\n\nOptional arguments\n\n\n\n\nAeq\n     : equality constraint matrix.\n\n\nbeq\n     : equality constraint vector.\n\n\nAineq\n   : inequality constraint matrix.\n\n\nbineq\n   : inequality constraint vector.\n\n\nobswt\n   : observation weights. Default is \n[1 1 1 ... 1]\n.\n\n\npenwt\n   : predictor penalty weights. Default is \n[1 1 1 ... 1]\n.\n\n\nsolver\n  : a solver Convex.jl supports. Default is ECOS.             Note that Mosek and Gurobi are more robust than ECOS. Unlike ECOS or             SCS, both Mosek and Gurobi require a license (free for academic             use). \nhttp://convexjl.readthedocs.io/en/latest/solvers.html\n\n\n\n\nReturns\n\n\n\n\n\u03b2\n       : estimated coefficents.\n\n\nobjval\n  : optimal objective value.\n\n\nproblem\n : Convex.jl problem.\n\n\n\n\nsource\n\n\n#\n\n\nConstrainedLasso.lsq_constrsparsereg_admm\n \n \nFunction\n.\n\n\n  lsq_constrsparsereg_admm(\n    X             :: AbstractMatrix{T},\n    y             :: AbstractVector{T},\n    \u03c1             :: Number = zero(T);\n    proj          :: Function = x -\n x,\n    obswt         :: Vector{T} = ones(T, length(y)),\n    penwt         :: Vector{T} = ones(T, size(X, 2)),\n    \u03b20            :: Vector{T} = zeros(T, size(X, 2)),\n    admmmaxite    :: Int = 10000,\n    admmabstol    :: Float64 = 1e-4,\n    admmreltol    :: Float64 = 1e-4,\n    admmscale     :: Float64 = 1 / length(y),\n    admmvaryscale :: Bool = false\n    )\n\n\n\n\n\nFit constrained lasso at a fixed tuning parameter value by applying the alternating direction method of multipliers (ADMM) algorithm.\n\n\nArguments\n\n\n\n\nX\n       : predictor matrix.\n\n\ny\n       : response vector.\n\n\n\u03c1\n       : tuning parameter. Default 0.\n\n\n\n\nOptional arguments\n\n\n\n\nproj\n         : projection onto the constraint set. Default is identity (no constraint).\n\n\nobswt\n        : observation weights. Default is \n[1 1 1 ... 1]\n.\n\n\npenwt\n        : predictor penalty weights. Default is \n[1 1 1 ... 1]\n.\n\n\n\u03b20\n           : starting point.\n\n\nadmmmaxite\n   : maximum number of iterations for ADMM. Default is \n10000\n.\n\n\nadmmabstol\n   : absolute tolerance for ADMM.\n\n\nadmmreltol\n   : relative tolerance for ADMM.\n\n\nadmmscale\n    : ADMM scale parameter. Default is \n1/n\n.\n\n\nadmmvaryscale\n: dynamically chance the ADMM scale parameter. Default is false.\n\n\n\n\nReturns\n\n\n\n\n\u03b2\n       : estimated coefficents.\n\n\n\n\nsource\n\n\n  lsq_constrsparsereg_admm(\n    X             :: AbstractMatrix{T},\n    y             :: AbstractVector{T},\n    \u03c1list         :: Vector;\n    proj          :: Function = x -\n x,\n    obswt         :: Vector{T} = ones(T, length(y)),\n    penwt         :: Vector{T} = ones(T, size(X, 2)),\n    admmmaxite    :: Int = 10000,\n    admmabstol    :: Float64 = 1e-4,\n    admmreltol    :: Float64 = 1e-4,\n    admmscale     :: Float64 = 1 / length(y),\n    admmvaryscale :: Bool = false\n    )\n\n\n\n\n\nFit constrained lasso at fixed tuning parameter values by applying the alternating direction method of multipliers (ADMM) algorithm.\n\n\nArguments\n\n\n\n\nX\n       : predictor matrix.\n\n\ny\n       : response vector.\n\n\n\u03c1list\n   : a vector of tuning parameter values.\n\n\n\n\nOptional arguments\n\n\n\n\nproj\n         : projection onto the constraint set. Default is identity (no constraint).\n\n\nobswt\n        : observation weights. Default is \n[1 1 1 ... 1]\n.\n\n\npenwt\n        : predictor penalty weights. Default is \n[1 1 1 ... 1]\n.\n\n\nadmmmaxite\n   : maximum number of iterations for ADMM. Default is \n10000\n.\n\n\nadmmabstol\n   : absolute tolerance for ADMM.\n\n\nadmmreltol\n   : relative tolerance for ADMM.\n\n\nadmmscale\n    : ADMM scale parameter. Default is \n1/n\n.\n\n\nadmmvaryscale\n: dynamically chance the ADMM scale parameter. Default is false.\n\n\n\n\nReturns\n\n\n\n\n\u03b2path\n       : estimated coefficents along the grid of \n\u03c1\n values.\n\n\n\n\nsource\n\n\n#\n\n\nConstrainedLasso.lsq_classopath\n \n \nFunction\n.\n\n\n  lsq_classopath(X, y;\n      Aeq    :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq    :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq  :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq  :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      \u03c1ridge :: Number = zero(eltype(X)),\n      penidx ::Array{Bool} = fill(true, size(X, 2)),\n      solver = ECOSSolver(maxit=10e8, verbose=0)\n      )\n\nCalculate the solution path of the constrained lasso problem that minimizes\n    `0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(penwt .* \u03b2)`\nsubject to linear constraints.\n\n\n\n\n\nArguments\n\n\n\n\nX\n       : predictor matrix.\n\n\ny\n       : response vector.\n\n\n\n\nOptional arguments\n\n\n\n\nAeq\n     : equality constraint matrix.\n\n\nbeq\n     : equality constraint vector.\n\n\nAineq\n   : inequality constraint matrix.\n\n\nbineq\n   : inequality constraint vector.\n\n\n\u03c1ridge\n  : tuning parameter for ridge penalty. Default is 0.\n\n\npenidx\n  : a logical vector indicating penalized coefficients.\n\n\nsolver\n  : a solver Convex.jl supports. Default is ECOS.             Note that Mosek and Gurobi are more robust than ECOS. Unlike ECOS or             SCS, both Mosek and Gurobi require a license (free for academic             use). \nhttp://convexjl.readthedocs.io/en/latest/solvers.html\n\n\n\n\nExamples\n\n\nSee tutorial examples at https://github.com/Hua-Zhou/ConstrainedLasso\n\n\nsource\n\n\nPrivate function in \nConstrainedLasso\n:\n\n\n#\n\n\nConstrainedLasso.find_\u03c1max\n \n \nFunction\n.\n\n\n  find_\u03c1max(X, y;\n      Aeq    :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq    :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq  :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq  :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      penidx :: Array{Bool} = fill(true, size(X, 2)),\n      solver = SCSSolver(verbose=0, max_iters=10e8)\n\n\n\n\n\nFind the maximum tuning parameter value \n\u03c1max\n to kick-start the solution path.\n\n\nsource", 
            "title": "Interface"
        }, 
        {
            "location": "/interface/#interface", 
            "text": "Functions exported from  ConstrainedLasso :  #  ConstrainedLasso.lsq_constrsparsereg     Function .     lsq_constrsparsereg ( X :: AbstractMatrix { T } ,   y :: AbstractVector { T } ,   \u03c1   =   zero ( T ); \n       Aeq         ::   AbstractMatrix   =   zeros ( T ,   0 ,   size ( X ,   2 )), \n       beq         ::   Union { AbstractVector,   T }   =   zeros ( T ,   size ( Aeq ,   1 )), \n       Aineq       ::   AbstractMatrix   =   zeros ( T ,   0 ,   size ( X ,   2 )), \n       bineq       ::   AbstractVector   =   zeros ( T ,   size ( Aineq ,   1 )), \n       obswt       ::   AbstractVector   =   ones ( T ,   length ( y )), \n       penwt       ::   AbstractVector   =   ones ( T ,   size ( X ,   2 )), \n       warmstart   ::   Bool   =   false , \n       solver   =   ECOSSolver ( maxit = 10e8 ,   verbose = 0 ) \n     )   Fit constrained lasso at fixed tuning parameter value(s) by minimizing      0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(penwt .* \u03b2)  subject to linear constraints, using  Convex.jl .  Arguments   X        : predictor matrix.  y        : response vector.  \u03c1        : tuning parameter. Can be a number or a list of numbers. Default 0.   Optional arguments   Aeq      : equality constraint matrix.  beq      : equality constraint vector.  Aineq    : inequality constraint matrix.  bineq    : inequality constraint vector.  obswt    : observation weights. Default is  [1 1 1 ... 1] .  penwt    : predictor penalty weights. Default is  [1 1 1 ... 1] .  solver   : a solver Convex.jl supports. Default is ECOS.             Note that Mosek and Gurobi are more robust than ECOS. Unlike ECOS or             SCS, both Mosek and Gurobi require a license (free for academic             use).  http://convexjl.readthedocs.io/en/latest/solvers.html   Returns   \u03b2        : estimated coefficents.  objval   : optimal objective value.  problem  : Convex.jl problem.   source  #  ConstrainedLasso.lsq_constrsparsereg_admm     Function .    lsq_constrsparsereg_admm(\n    X             :: AbstractMatrix{T},\n    y             :: AbstractVector{T},\n    \u03c1             :: Number = zero(T);\n    proj          :: Function = x -  x,\n    obswt         :: Vector{T} = ones(T, length(y)),\n    penwt         :: Vector{T} = ones(T, size(X, 2)),\n    \u03b20            :: Vector{T} = zeros(T, size(X, 2)),\n    admmmaxite    :: Int = 10000,\n    admmabstol    :: Float64 = 1e-4,\n    admmreltol    :: Float64 = 1e-4,\n    admmscale     :: Float64 = 1 / length(y),\n    admmvaryscale :: Bool = false\n    )  Fit constrained lasso at a fixed tuning parameter value by applying the alternating direction method of multipliers (ADMM) algorithm.  Arguments   X        : predictor matrix.  y        : response vector.  \u03c1        : tuning parameter. Default 0.   Optional arguments   proj          : projection onto the constraint set. Default is identity (no constraint).  obswt         : observation weights. Default is  [1 1 1 ... 1] .  penwt         : predictor penalty weights. Default is  [1 1 1 ... 1] .  \u03b20            : starting point.  admmmaxite    : maximum number of iterations for ADMM. Default is  10000 .  admmabstol    : absolute tolerance for ADMM.  admmreltol    : relative tolerance for ADMM.  admmscale     : ADMM scale parameter. Default is  1/n .  admmvaryscale : dynamically chance the ADMM scale parameter. Default is false.   Returns   \u03b2        : estimated coefficents.   source    lsq_constrsparsereg_admm(\n    X             :: AbstractMatrix{T},\n    y             :: AbstractVector{T},\n    \u03c1list         :: Vector;\n    proj          :: Function = x -  x,\n    obswt         :: Vector{T} = ones(T, length(y)),\n    penwt         :: Vector{T} = ones(T, size(X, 2)),\n    admmmaxite    :: Int = 10000,\n    admmabstol    :: Float64 = 1e-4,\n    admmreltol    :: Float64 = 1e-4,\n    admmscale     :: Float64 = 1 / length(y),\n    admmvaryscale :: Bool = false\n    )  Fit constrained lasso at fixed tuning parameter values by applying the alternating direction method of multipliers (ADMM) algorithm.  Arguments   X        : predictor matrix.  y        : response vector.  \u03c1list    : a vector of tuning parameter values.   Optional arguments   proj          : projection onto the constraint set. Default is identity (no constraint).  obswt         : observation weights. Default is  [1 1 1 ... 1] .  penwt         : predictor penalty weights. Default is  [1 1 1 ... 1] .  admmmaxite    : maximum number of iterations for ADMM. Default is  10000 .  admmabstol    : absolute tolerance for ADMM.  admmreltol    : relative tolerance for ADMM.  admmscale     : ADMM scale parameter. Default is  1/n .  admmvaryscale : dynamically chance the ADMM scale parameter. Default is false.   Returns   \u03b2path        : estimated coefficents along the grid of  \u03c1  values.   source  #  ConstrainedLasso.lsq_classopath     Function .    lsq_classopath(X, y;\n      Aeq    :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq    :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq  :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq  :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      \u03c1ridge :: Number = zero(eltype(X)),\n      penidx ::Array{Bool} = fill(true, size(X, 2)),\n      solver = ECOSSolver(maxit=10e8, verbose=0)\n      )\n\nCalculate the solution path of the constrained lasso problem that minimizes\n    `0.5sumabs2(\u221aobswt .* (y - X * \u03b2)) + \u03c1 * sumabs(penwt .* \u03b2)`\nsubject to linear constraints.  Arguments   X        : predictor matrix.  y        : response vector.   Optional arguments   Aeq      : equality constraint matrix.  beq      : equality constraint vector.  Aineq    : inequality constraint matrix.  bineq    : inequality constraint vector.  \u03c1ridge   : tuning parameter for ridge penalty. Default is 0.  penidx   : a logical vector indicating penalized coefficients.  solver   : a solver Convex.jl supports. Default is ECOS.             Note that Mosek and Gurobi are more robust than ECOS. Unlike ECOS or             SCS, both Mosek and Gurobi require a license (free for academic             use).  http://convexjl.readthedocs.io/en/latest/solvers.html   Examples  See tutorial examples at https://github.com/Hua-Zhou/ConstrainedLasso  source  Private function in  ConstrainedLasso :  #  ConstrainedLasso.find_\u03c1max     Function .    find_\u03c1max(X, y;\n      Aeq    :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      beq    :: AbstractVector = zeros(eltype(X), size(Aeq, 1)),\n      Aineq  :: AbstractMatrix = zeros(eltype(X), 0, size(X, 2)),\n      bineq  :: AbstractVector = zeros(eltype(X), size(Aineq, 1)),\n      penidx :: Array{Bool} = fill(true, size(X, 2)),\n      solver = SCSSolver(verbose=0, max_iters=10e8)  Find the maximum tuning parameter value  \u03c1max  to kick-start the solution path.  source", 
            "title": "Interface"
        }, 
        {
            "location": "/demo/fixedparam/", 
            "text": "Optimize at fixed tuning parameter value(s)\n\n\nlsq_constrsparsereg.jl\n fits constrained lasso\n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\boldsymbol{A\\beta}=\\boldsymbol{b} \\text{ and } \\boldsymbol{C\\beta} \\leq \\boldsymbol{d}\n\\end{split}\n\n\n\n\n\nat a fixed tuning parameter value $\\rho$ or several tuning parameter values provided by user.\n\n\n\n\nSingle tuning parameter value\n\n\nWe demonstrate using a sum-to-zero constraint example\n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\sum_j \\beta_j = 0\n\\end{split}\n\n\n\n\n\nFirst, let's define a true parameter \n\u03b2\n such that \nsum(\u03b2) = 0\n.\n\n\nusing\n \nConstrainedLasso\n,\n \nBase\n.\nTest\n\n\n\nn\n,\n \np\n \n=\n \n50\n,\n \n100\n  \n\n\u03b2\n \n=\n \nzeros\n(\np\n)\n\n\n\u03b2\n[\n1\n:\nround\n(\nInt\n,\n \np\n \n/\n \n4\n)]\n \n=\n \n0\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \np\n \n/\n \n4\n)\n \n+\n \n1\n)\n:\nround\n(\nInt\n,\n \np\n \n/\n \n2\n)]\n \n=\n \n1\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \np\n \n/\n \n2\n)\n \n+\n \n1\n)\n:\nround\n(\nInt\n,\n \n3\np\n \n/\n \n4\n)]\n \n=\n \n0\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \n3\np\n \n/\n \n4\n)\n \n+\n \n1\n)\n:\np\n]\n \n=\n \n-\n1\n\n\n\u03b2\n\n\n\n\n\n\n100-element Array{Float64,1}:\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  \u22ee  \n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n\n\n\n\n\nNext we generate data based on the true parameter \n\u03b2\n.\n\n\nsrand\n(\n41\n)\n\n\nX\n \n=\n \nrandn\n(\nn\n,\n \np\n)\n\n\n\n\n\n\n50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627\n\n\n\n\n\ny\n \n=\n \nX\n \n*\n \n\u03b2\n \n+\n \nrandn\n(\nn\n)\n\n\n\n\n\n\n50-element Array{Float64,1}:\n  -9.90585 \n  -5.40562 \n   5.24289 \n  -6.29951 \n  -4.9586  \n  -6.1342  \n  -7.90981 \n   2.51009 \n  -5.79548 \n   1.61355 \n  -0.722766\n  10.4522  \n   4.03935 \n   \u22ee       \n   0.397781\n  -2.6661  \n   5.36896 \n  -3.56537 \n  -2.402   \n   0.11478 \n  -5.39248 \n   4.38391 \n   0.706801\n -10.1066  \n  -1.12558 \n  14.2473\n\n\n\n\n\nSince the equality constraint can be written as \n\n\n\n\n\n\\begin{split}\n\\begin{pmatrix} 1 & 1 & \\cdots & 1 \\end{pmatrix} \\beta = 0,\n\\end{split}\n\n\n\n\n\nwe define the constraint as below.\n\n\nbeq\n   \n=\n \n[\n0.0\n]\n\n\nAeq\n   \n=\n \nones\n(\n1\n,\n \np\n)\n\n\n\n\n\n\n1\u00d7100 Array{Float64,2}:\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \u2026  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n\n\n\n\n\nNow we are ready to fit the constrained lasso problem, say at \n\u03c1=10\n.\n\n\n\u03c1\n \n=\n \n10.0\n\n\n\u03b2\u0302\n,\n \n=\n \nlsq_constrsparsereg\n(\nX\n,\n \ny\n,\n \n\u03c1\n;\n \nAeq\n \n=\n \nAeq\n,\n \nbeq\n \n=\n \nbeq\n);\n\n\n\n\n\n\n\u03b2\u0302\n\n\n\n\n\n\n100\u00d71 Array{Float64,2}:\n  0.0334671 \n -1.1128e-5 \n  5.40916e-6\n -0.394908  \n  5.69496e-6\n  1.17002e-5\n -3.90243e-6\n -1.91365e-5\n  0.288553  \n  2.05743e-6\n  1.12622e-5\n  1.6712e-5 \n -2.17275e-6\n  \u22ee         \n -0.188179  \n -3.52148e-6\n  3.50838e-6\n  9.58236e-6\n -0.844097  \n -0.645805  \n -0.257241  \n -4.90509e-6\n -1.13326   \n -7.28399e-6\n  4.69294e-6\n -8.90143e-6\n\n\n\n\n\nWe see if the sum of estimated $\\beta$ coefficients equal to 0.\n\n\n@test\n \nsum\n(\n\u03b2\u0302\n)\n\u22480\n.\n0\n \natol\n=\n1e-4\n\n\n\n\n\n\n\u001bTest Passed\n\n\n\n\n\n\n\nMultiple tuning parameter values\n\n\nDefine \n\u03c1list\n to be a sequence of values from 1 to 10.\n\n\n\u03c1list\n \n=\n \n152.0\n:-\n15.0\n:\n2.0\n\n\n\n\n\n\n152.0\n:-\n15.0\n:\n2.0\n\n\n\n\n\n\nUsing the same equality constraints, we fit the constrained lasso.\n\n\n\u03b2\u0302\n,\n \n=\n \nlsq_constrsparsereg\n(\nX\n,\n \ny\n,\n \n\u03c1list\n;\n \nAeq\n \n=\n \nAeq\n,\n \nbeq\n \n=\n \nbeq\n);\n\n\n\n\n\n\n\u03b2\u0302\n\n\n\n\n\n\n100\u00d711 Array{Float64,2}:\n  1.01071e-5  -8.27382e-6  -9.98518e-7  \u2026   2.24039e-6  -1.73712e-8\n -9.20705e-6  -1.73958e-5  -2.05881e-6     -2.60796e-5   0.178782  \n -5.63613e-6  -1.75901e-5  -1.8899e-6      -1.01686e-5   3.41563e-8\n -1.15136e-5  -1.29555e-5  -1.18262e-6     -0.331097    -0.413631  \n -1.60099e-5  -1.38074e-5  -1.11624e-6     -1.1681e-5    4.44159e-8\n  1.6582e-5   -8.67409e-6  -7.82383e-7  \u2026   2.73047e-5   1.19985e-7\n -1.06227e-5  -1.47899e-5  -7.36221e-7      4.90329e-6   5.63884e-8\n -5.7368e-6   -1.48341e-5  -1.65699e-6     -9.84382e-8   3.01081e-8\n  1.42761e-5  -7.54447e-6  -9.46132e-7      0.205789     0.33502   \n -1.35551e-6  -1.40007e-5  -1.15001e-6      2.14801e-5  -0.158037  \n -8.39911e-6  -1.04558e-5  -1.59023e-6  \u2026   4.57889e-6   1.88987e-8\n  1.88431e-6  -1.34713e-5  -1.75855e-6     -2.03643e-6  -2.39007e-8\n -2.78397e-6  -9.21455e-6  -7.11068e-7     -1.14052e-6   3.91139e-8\n  \u22ee                                     \u22f1                \u22ee         \n -7.31054e-6  -1.5138e-5   -1.6294e-6      -0.137376    -0.392134  \n -2.28865e-6  -1.4119e-5   -1.41117e-6     -8.14484e-7  -0.333912  \n -3.48868e-6  -1.27998e-5  -1.60242e-6  \u2026   2.18783e-5   3.13386e-9\n -2.87686e-6  -1.96206e-5  -2.75763e-6      3.14833e-7   7.63403e-8\n -2.14002e-5  -0.0184219   -0.152241       -0.914608    -0.867679  \n -1.57276e-5  -1.80525e-5  -1.42057e-6     -0.218999    -1.00031   \n -1.09405e-5  -1.16499e-5  -1.0991e-6      -0.237285    -0.578412  \n -5.97059e-6  -6.69791e-6  -1.30652e-6  \u2026  -4.20856e-6  -3.80778e-8\n -1.95746e-5  -1.11737e-5  -1.27845e-6     -1.0569      -1.16938   \n  5.18594e-6  -1.26231e-5  -1.82091e-6     -1.37675e-5  -2.69827e-8\n -7.37912e-6  -1.58805e-5  -1.69692e-6     -7.48234e-6  -1.00642e-7\n  8.30253e-6  -7.94538e-6  -7.58241e-7      1.17015e-5   3.21803e-8\n\n\n\n\n\nusing\n \nPlots\n;\n \npyplot\n();\n\n\nplot\n(\n\u03c1list\n,\n \n\u03b2\u0302\n,\n \nlabel\n=\n,\n \nxaxis\n \n=\n \n(\n\u03c1\n,\n \n(\nminimum\n(\n\u03c1list\n),\n\n      \nmaximum\n(\n\u03c1list\n))),\n \nyaxis\n \n=\n \n(\n\u03b2\u0302(\u03c1)\n),\n \nwidth\n=\n0.5\n)\n \n\ntitle!\n(\nSolution Path at Fixed Parameter Values\n)\n \n\n\n\n\n\n\n\nFollow this \nlink\n to access the .ipynb file of this page.", 
            "title": "Fixed Tuning Parameter Value"
        }, 
        {
            "location": "/demo/fixedparam/#optimize-at-fixed-tuning-parameter-values", 
            "text": "lsq_constrsparsereg.jl  fits constrained lasso   \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\boldsymbol{A\\beta}=\\boldsymbol{b} \\text{ and } \\boldsymbol{C\\beta} \\leq \\boldsymbol{d}\n\\end{split}   at a fixed tuning parameter value $\\rho$ or several tuning parameter values provided by user.", 
            "title": "Optimize at fixed tuning parameter value(s)"
        }, 
        {
            "location": "/demo/fixedparam/#single-tuning-parameter-value", 
            "text": "We demonstrate using a sum-to-zero constraint example   \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\sum_j \\beta_j = 0\n\\end{split}   First, let's define a true parameter  \u03b2  such that  sum(\u03b2) = 0 .  using   ConstrainedLasso ,   Base . Test  n ,   p   =   50 ,   100    \u03b2   =   zeros ( p )  \u03b2 [ 1 : round ( Int ,   p   /   4 )]   =   0  \u03b2 [( round ( Int ,   p   /   4 )   +   1 ) : round ( Int ,   p   /   2 )]   =   1  \u03b2 [( round ( Int ,   p   /   2 )   +   1 ) : round ( Int ,   3 p   /   4 )]   =   0  \u03b2 [( round ( Int ,   3 p   /   4 )   +   1 ) : p ]   =   - 1  \u03b2   100-element Array{Float64,1}:\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  \u22ee  \n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0  Next we generate data based on the true parameter  \u03b2 .  srand ( 41 )  X   =   randn ( n ,   p )   50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627  y   =   X   *   \u03b2   +   randn ( n )   50-element Array{Float64,1}:\n  -9.90585 \n  -5.40562 \n   5.24289 \n  -6.29951 \n  -4.9586  \n  -6.1342  \n  -7.90981 \n   2.51009 \n  -5.79548 \n   1.61355 \n  -0.722766\n  10.4522  \n   4.03935 \n   \u22ee       \n   0.397781\n  -2.6661  \n   5.36896 \n  -3.56537 \n  -2.402   \n   0.11478 \n  -5.39248 \n   4.38391 \n   0.706801\n -10.1066  \n  -1.12558 \n  14.2473  Since the equality constraint can be written as    \n\\begin{split}\n\\begin{pmatrix} 1 & 1 & \\cdots & 1 \\end{pmatrix} \\beta = 0,\n\\end{split}   we define the constraint as below.  beq     =   [ 0.0 ]  Aeq     =   ones ( 1 ,   p )   1\u00d7100 Array{Float64,2}:\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \u2026  1.0  1.0  1.0  1.0  1.0  1.0  1.0  Now we are ready to fit the constrained lasso problem, say at  \u03c1=10 .  \u03c1   =   10.0  \u03b2\u0302 ,   =   lsq_constrsparsereg ( X ,   y ,   \u03c1 ;   Aeq   =   Aeq ,   beq   =   beq );   \u03b2\u0302   100\u00d71 Array{Float64,2}:\n  0.0334671 \n -1.1128e-5 \n  5.40916e-6\n -0.394908  \n  5.69496e-6\n  1.17002e-5\n -3.90243e-6\n -1.91365e-5\n  0.288553  \n  2.05743e-6\n  1.12622e-5\n  1.6712e-5 \n -2.17275e-6\n  \u22ee         \n -0.188179  \n -3.52148e-6\n  3.50838e-6\n  9.58236e-6\n -0.844097  \n -0.645805  \n -0.257241  \n -4.90509e-6\n -1.13326   \n -7.28399e-6\n  4.69294e-6\n -8.90143e-6  We see if the sum of estimated $\\beta$ coefficients equal to 0.  @test   sum ( \u03b2\u0302 ) \u22480 . 0   atol = 1e-4   \u001bTest Passed", 
            "title": "Single tuning parameter value"
        }, 
        {
            "location": "/demo/fixedparam/#multiple-tuning-parameter-values", 
            "text": "Define  \u03c1list  to be a sequence of values from 1 to 10.  \u03c1list   =   152.0 :- 15.0 : 2.0   152.0 :- 15.0 : 2.0   Using the same equality constraints, we fit the constrained lasso.  \u03b2\u0302 ,   =   lsq_constrsparsereg ( X ,   y ,   \u03c1list ;   Aeq   =   Aeq ,   beq   =   beq );   \u03b2\u0302   100\u00d711 Array{Float64,2}:\n  1.01071e-5  -8.27382e-6  -9.98518e-7  \u2026   2.24039e-6  -1.73712e-8\n -9.20705e-6  -1.73958e-5  -2.05881e-6     -2.60796e-5   0.178782  \n -5.63613e-6  -1.75901e-5  -1.8899e-6      -1.01686e-5   3.41563e-8\n -1.15136e-5  -1.29555e-5  -1.18262e-6     -0.331097    -0.413631  \n -1.60099e-5  -1.38074e-5  -1.11624e-6     -1.1681e-5    4.44159e-8\n  1.6582e-5   -8.67409e-6  -7.82383e-7  \u2026   2.73047e-5   1.19985e-7\n -1.06227e-5  -1.47899e-5  -7.36221e-7      4.90329e-6   5.63884e-8\n -5.7368e-6   -1.48341e-5  -1.65699e-6     -9.84382e-8   3.01081e-8\n  1.42761e-5  -7.54447e-6  -9.46132e-7      0.205789     0.33502   \n -1.35551e-6  -1.40007e-5  -1.15001e-6      2.14801e-5  -0.158037  \n -8.39911e-6  -1.04558e-5  -1.59023e-6  \u2026   4.57889e-6   1.88987e-8\n  1.88431e-6  -1.34713e-5  -1.75855e-6     -2.03643e-6  -2.39007e-8\n -2.78397e-6  -9.21455e-6  -7.11068e-7     -1.14052e-6   3.91139e-8\n  \u22ee                                     \u22f1                \u22ee         \n -7.31054e-6  -1.5138e-5   -1.6294e-6      -0.137376    -0.392134  \n -2.28865e-6  -1.4119e-5   -1.41117e-6     -8.14484e-7  -0.333912  \n -3.48868e-6  -1.27998e-5  -1.60242e-6  \u2026   2.18783e-5   3.13386e-9\n -2.87686e-6  -1.96206e-5  -2.75763e-6      3.14833e-7   7.63403e-8\n -2.14002e-5  -0.0184219   -0.152241       -0.914608    -0.867679  \n -1.57276e-5  -1.80525e-5  -1.42057e-6     -0.218999    -1.00031   \n -1.09405e-5  -1.16499e-5  -1.0991e-6      -0.237285    -0.578412  \n -5.97059e-6  -6.69791e-6  -1.30652e-6  \u2026  -4.20856e-6  -3.80778e-8\n -1.95746e-5  -1.11737e-5  -1.27845e-6     -1.0569      -1.16938   \n  5.18594e-6  -1.26231e-5  -1.82091e-6     -1.37675e-5  -2.69827e-8\n -7.37912e-6  -1.58805e-5  -1.69692e-6     -7.48234e-6  -1.00642e-7\n  8.30253e-6  -7.94538e-6  -7.58241e-7      1.17015e-5   3.21803e-8  using   Plots ;   pyplot ();  plot ( \u03c1list ,   \u03b2\u0302 ,   label = ,   xaxis   =   ( \u03c1 ,   ( minimum ( \u03c1list ), \n       maximum ( \u03c1list ))),   yaxis   =   ( \u03b2\u0302(\u03c1) ),   width = 0.5 )   title! ( Solution Path at Fixed Parameter Values )     Follow this  link  to access the .ipynb file of this page.", 
            "title": "Multiple tuning parameter values"
        }, 
        {
            "location": "/demo/admm/", 
            "text": "ADMM\n\n\nIn this section, we solve the same constrained lasso problem using the alternating direction method of multipliers (ADMM) algorithm. ADMM algorithm is advantageous since it can scale to larger size problems and is not restricted to linear constraints. See \nGaines and Zhou (2016)\n for details. \n\n\nIn order to use the algorithm, user needs to supply a projection function onto the constraint set. Some simple examples are discussed below. \n\n\n\n\nsum-to-zero constraint\n\n\nWe demonstrate using a sum-to-zero constraint example\n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\sum_j \\beta_j = 0.\n\\end{split}\n\n\n\n\n\nFor this constraint, the appropriate projection operator would be \n\n\n\n\n\n\\text{proj}(\\boldsymbol{x}) = \\boldsymbol{x} - \\bar{\\boldsymbol{x}}= \\boldsymbol{x} - \\sum_{j=1}^n x_j.\n\n\n\n\n\nNow let's define a true parameter \n\u03b2\n such that \nsum(\u03b2) = 0\n.\n\n\nusing\n \nConstrainedLasso\n,\n \nBase\n.\nTest\n\n\n\nn\n,\n \np\n \n=\n \n50\n,\n \n100\n  \n\n\u03b2\n \n=\n \nzeros\n(\np\n);\n\n\n\u03b2\n[\n1\n:\nround\n(\nInt\n,\n \np\n \n/\n \n4\n)]\n \n=\n \n0\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \np\n \n/\n \n4\n)\n \n+\n \n1\n)\n:\nround\n(\nInt\n,\n \np\n \n/\n \n2\n)]\n \n=\n \n1\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \np\n \n/\n \n2\n)\n \n+\n \n1\n)\n:\nround\n(\nInt\n,\n \n3\np\n \n/\n \n4\n)]\n \n=\n \n0\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \n3\np\n \n/\n \n4\n)\n \n+\n \n1\n)\n:\np\n]\n \n=\n \n-\n1\n\n\n\u03b2\n\n\n\n\n\n\n100-element Array{Float64,1}:\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  \u22ee  \n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n\n\n\n\n\nNext we generate data based on the true parameter \u03b2.\n\n\nsrand\n(\n41\n)\n\n\nX\n \n=\n \nrandn\n(\nn\n,\n \np\n)\n\n\n\n\n\n\n50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627\n\n\n\n\n\ny\n \n=\n \nX\n \n*\n \n\u03b2\n \n+\n \nrandn\n(\nn\n)\n\n\n\n\n\n\n50-element Array{Float64,1}:\n  -9.90585 \n  -5.40562 \n   5.24289 \n  -6.29951 \n  -4.9586  \n  -6.1342  \n  -7.90981 \n   2.51009 \n  -5.79548 \n   1.61355 \n  -0.722766\n  10.4522  \n   4.03935 \n   \u22ee       \n   0.397781\n  -2.6661  \n   5.36896 \n  -3.56537 \n  -2.402   \n   0.11478 \n  -5.39248 \n   4.38391 \n   0.706801\n -10.1066  \n  -1.12558 \n  14.2473\n\n\n\n\n\nNow we estimate coefficients at fixed tuning parameter value using ADMM alogrithm. \n\n\n\u03c1\n \n=\n \n2.0\n\n\n\u03b2\u0302admm\n \n=\n \nlsq_constrsparsereg_admm\n(\nX\n,\n \ny\n,\n \n\u03c1\n;\n \nproj\n \n=\n \nx\n \n-\n \nx\n \n-\n \nmean\n(\nx\n))\n\n\n\n\n\n\n\u03b2\u0302admm\n\n\n\n\n\n\n100\u00d71 GLMNet.CompressedPredictorMatrix:\n  0.0     \n  0.174344\n  0.0     \n -0.421288\n  0.0     \n  0.0     \n  0.0     \n  0.0     \n  0.324233\n -0.15384 \n  0.0     \n  0.0     \n  0.0     \n  \u22ee       \n -0.397027\n -0.32079 \n  0.0     \n  0.0     \n -0.868508\n -0.992272\n -0.571755\n  0.0     \n -1.16568 \n  0.0     \n  0.0     \n  0.0\n\n\n\n\n\nNow let's compare the estimated coefficients with those obtained using quadratic programming. \n\n\n\u03c1\n \n=\n \n2.0\n \n\nbeq\n \n=\n \n[\n0\n]\n\n\nAeq\n \n=\n \nones\n(\n1\n,\n \np\n)\n\n\nusing\n \nECOS\n;\n \nsolver\n=\nECOSSolver\n(\nverbose\n=\n0\n,\n \nmaxit\n=\n1e8\n);\n\n\n\u03b2\u0302\n,\n \n=\n \nlsq_constrsparsereg\n(\nX\n,\n \ny\n,\n \n\u03c1\n;\n \nAeq\n \n=\n \nAeq\n,\n \nbeq\n \n=\n \nbeq\n,\n \nsolver\n \n=\n \nsolver\n)\n \n\n\n\n\n\nhcat\n(\n\u03b2\u0302admm\n,\n \n\u03b2\u0302\n)\n\n\n\n\n\n\n100\u00d72 Array{Float64,2}:\n  0.0        1.51451e-8 \n  0.174344   0.178717   \n  0.0       -1.02944e-9 \n -0.421288  -0.414043   \n  0.0       -3.33221e-10\n  0.0       -4.14188e-10\n  0.0        3.28018e-11\n  0.0        8.38037e-11\n  0.324233   0.335375   \n -0.15384   -0.157908   \n  0.0       -2.44739e-9 \n  0.0       -8.79003e-10\n  0.0       -8.17787e-10\n  \u22ee                     \n -0.397027  -0.391635   \n -0.32079   -0.33352    \n  0.0       -5.51459e-9 \n  0.0        1.09637e-9 \n -0.868508  -0.867639   \n -0.992272  -0.999583   \n -0.571755  -0.577743   \n  0.0       -8.93601e-10\n -1.16568   -1.16862    \n  0.0        2.29367e-9 \n  0.0       -1.52035e-9 \n  0.0        2.71896e-9\n\n\n\n\n\n\n\nNon-negativity constraint\n\n\nHere we look at the non-negativity constraint. For the constraint $\\beta_j \\geq 0 \\forall j$, the appropriate projection function is \n\n\n$$\n\\text{proj}(x) = \n\\begin{cases}\nx, & \\text{if } x \\geq 0 \n0, & \\text{else }. \n\\end{cases}\n\n\n\n\n$$\n\n\nNow let's generate \nX\n and \ny\n.\n\n\nn\n,\n \np\n \n=\n \n50\n,\n \n100\n   \n\n\u03b2\n \n=\n \nzeros\n(\np\n)\n\n\n\u03b2\n[\n1\n:\n10\n]\n \n=\n \n1\n:\n10\n\n\nsrand\n(\n41\n)\n\n\nX\n \n=\n \nrandn\n(\nn\n,\n \np\n)\n\n\n\n\n\n\n50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627\n\n\n\n\n\ny\n \n=\n \nX\n \n*\n \n\u03b2\n \n+\n \nrandn\n(\nn\n)\n\n\n\n\n\n\n50-element Array{Float64,1}:\n  12.6173  \n  40.3776  \n   2.2169  \n  27.4631  \n  38.592   \n   7.82023 \n  22.7367  \n   7.88475 \n  -7.47037 \n   0.621035\n  -4.91899 \n -14.9363  \n   8.26901 \n   \u22ee       \n   7.83882 \n  -9.30699 \n -29.7205  \n  15.2482  \n -19.1784  \n  14.9865  \n   2.32728 \n  -9.11988 \n -15.3472  \n  22.9679  \n  -0.997964\n  42.6068\n\n\n\n\n\n\u03c1\n \n=\n \n2.0\n\n\n\u03b2\u0302admm\n \n=\n \nlsq_constrsparsereg_admm\n(\nX\n,\n \ny\n,\n \n\u03c1\n;\n \nproj\n \n=\n \nx\n \n-\n \nclamp\n.\n(\nx\n,\n \n0\n,\n \nInf\n))\n\n\n\n\n\n\n\u03b2\u0302admm\n\n\n\n\n\n\n100\u00d71 GLMNet.CompressedPredictorMatrix:\n 0.611673  \n 2.17111   \n 2.65667   \n 4.05568   \n 4.72435   \n 5.87293   \n 6.6957    \n 8.36528   \n 8.61945   \n 9.80517   \n 1.18896e-6\n 0.0       \n 0.0       \n \u22ee         \n 0.100613  \n 0.0       \n 0.0       \n 0.0       \n 0.0       \n 0.0       \n 0.0       \n 0.0       \n 0.0       \n 0.0       \n 6.84603e-7\n 0.0593872\n\n\n\n\n\nAgain we compare the estimates with those from quadratic programming. Here we use \nECOS\n solver instead of the default \nSCS\n. \n\n\n\u03c1\n \n=\n \n2.0\n \n\nbineq\n \n=\n \nzeros\n(\np\n)\n\n\nAineq\n \n=\n \n-\n \neye\n(\np\n)\n\n\nusing\n \nECOS\n;\n \nsolver\n=\nECOSSolver\n(\nverbose\n=\n0\n,\n \nmaxit\n=\n1e8\n);\n\n\n\u03b2\u0302\n,\n \n=\n \nlsq_constrsparsereg\n(\nX\n,\n \ny\n,\n \n\u03c1\n;\n \nAineq\n \n=\n \nAineq\n,\n \nbineq\n \n=\n \nbineq\n,\n \nsolver\n \n=\n \nsolver\n)\n \n\n\n\n\n\n\u03b2\u0302\n\n\n\n\n\n\n100\u00d71 Array{Float64,2}:\n  0.610587   \n  2.17169    \n  2.65765    \n  4.05601    \n  4.72551    \n  5.87414    \n  6.69414    \n  8.36632    \n  8.62049    \n  9.80458    \n -1.18573e-10\n -8.86515e-11\n -5.48968e-11\n  \u22ee          \n  0.10151    \n  1.08588e-9 \n  3.51552e-10\n -4.42556e-11\n -8.01753e-11\n  1.40626e-10\n -1.58472e-11\n  2.50567e-10\n -4.76544e-11\n  3.28495e-10\n -7.49201e-11\n  0.0602764\n\n\n\n\n\nFollow this \nlink\n to access the .ipynb file of this page.", 
            "title": "ADMM"
        }, 
        {
            "location": "/demo/admm/#admm", 
            "text": "In this section, we solve the same constrained lasso problem using the alternating direction method of multipliers (ADMM) algorithm. ADMM algorithm is advantageous since it can scale to larger size problems and is not restricted to linear constraints. See  Gaines and Zhou (2016)  for details.   In order to use the algorithm, user needs to supply a projection function onto the constraint set. Some simple examples are discussed below.", 
            "title": "ADMM"
        }, 
        {
            "location": "/demo/admm/#sum-to-zero-constraint", 
            "text": "We demonstrate using a sum-to-zero constraint example   \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{0.5em} \\sum_j \\beta_j = 0.\n\\end{split}   For this constraint, the appropriate projection operator would be    \n\\text{proj}(\\boldsymbol{x}) = \\boldsymbol{x} - \\bar{\\boldsymbol{x}}= \\boldsymbol{x} - \\sum_{j=1}^n x_j.   Now let's define a true parameter  \u03b2  such that  sum(\u03b2) = 0 .  using   ConstrainedLasso ,   Base . Test  n ,   p   =   50 ,   100    \u03b2   =   zeros ( p );  \u03b2 [ 1 : round ( Int ,   p   /   4 )]   =   0  \u03b2 [( round ( Int ,   p   /   4 )   +   1 ) : round ( Int ,   p   /   2 )]   =   1  \u03b2 [( round ( Int ,   p   /   2 )   +   1 ) : round ( Int ,   3 p   /   4 )]   =   0  \u03b2 [( round ( Int ,   3 p   /   4 )   +   1 ) : p ]   =   - 1  \u03b2   100-element Array{Float64,1}:\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  \u22ee  \n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0\n -1.0  Next we generate data based on the true parameter \u03b2.  srand ( 41 )  X   =   randn ( n ,   p )   50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627  y   =   X   *   \u03b2   +   randn ( n )   50-element Array{Float64,1}:\n  -9.90585 \n  -5.40562 \n   5.24289 \n  -6.29951 \n  -4.9586  \n  -6.1342  \n  -7.90981 \n   2.51009 \n  -5.79548 \n   1.61355 \n  -0.722766\n  10.4522  \n   4.03935 \n   \u22ee       \n   0.397781\n  -2.6661  \n   5.36896 \n  -3.56537 \n  -2.402   \n   0.11478 \n  -5.39248 \n   4.38391 \n   0.706801\n -10.1066  \n  -1.12558 \n  14.2473  Now we estimate coefficients at fixed tuning parameter value using ADMM alogrithm.   \u03c1   =   2.0  \u03b2\u0302admm   =   lsq_constrsparsereg_admm ( X ,   y ,   \u03c1 ;   proj   =   x   -   x   -   mean ( x ))   \u03b2\u0302admm   100\u00d71 GLMNet.CompressedPredictorMatrix:\n  0.0     \n  0.174344\n  0.0     \n -0.421288\n  0.0     \n  0.0     \n  0.0     \n  0.0     \n  0.324233\n -0.15384 \n  0.0     \n  0.0     \n  0.0     \n  \u22ee       \n -0.397027\n -0.32079 \n  0.0     \n  0.0     \n -0.868508\n -0.992272\n -0.571755\n  0.0     \n -1.16568 \n  0.0     \n  0.0     \n  0.0  Now let's compare the estimated coefficients with those obtained using quadratic programming.   \u03c1   =   2.0   beq   =   [ 0 ]  Aeq   =   ones ( 1 ,   p )  using   ECOS ;   solver = ECOSSolver ( verbose = 0 ,   maxit = 1e8 );  \u03b2\u0302 ,   =   lsq_constrsparsereg ( X ,   y ,   \u03c1 ;   Aeq   =   Aeq ,   beq   =   beq ,   solver   =   solver )    hcat ( \u03b2\u0302admm ,   \u03b2\u0302 )   100\u00d72 Array{Float64,2}:\n  0.0        1.51451e-8 \n  0.174344   0.178717   \n  0.0       -1.02944e-9 \n -0.421288  -0.414043   \n  0.0       -3.33221e-10\n  0.0       -4.14188e-10\n  0.0        3.28018e-11\n  0.0        8.38037e-11\n  0.324233   0.335375   \n -0.15384   -0.157908   \n  0.0       -2.44739e-9 \n  0.0       -8.79003e-10\n  0.0       -8.17787e-10\n  \u22ee                     \n -0.397027  -0.391635   \n -0.32079   -0.33352    \n  0.0       -5.51459e-9 \n  0.0        1.09637e-9 \n -0.868508  -0.867639   \n -0.992272  -0.999583   \n -0.571755  -0.577743   \n  0.0       -8.93601e-10\n -1.16568   -1.16862    \n  0.0        2.29367e-9 \n  0.0       -1.52035e-9 \n  0.0        2.71896e-9", 
            "title": "sum-to-zero constraint"
        }, 
        {
            "location": "/demo/admm/#non-negativity-constraint", 
            "text": "Here we look at the non-negativity constraint. For the constraint $\\beta_j \\geq 0 \\forall j$, the appropriate projection function is   $$\n\\text{proj}(x) =  \\begin{cases}\nx, & \\text{if } x \\geq 0 \n0, & \\text{else }. \n\\end{cases}   $$  Now let's generate  X  and  y .  n ,   p   =   50 ,   100     \u03b2   =   zeros ( p )  \u03b2 [ 1 : 10 ]   =   1 : 10  srand ( 41 )  X   =   randn ( n ,   p )   50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627  y   =   X   *   \u03b2   +   randn ( n )   50-element Array{Float64,1}:\n  12.6173  \n  40.3776  \n   2.2169  \n  27.4631  \n  38.592   \n   7.82023 \n  22.7367  \n   7.88475 \n  -7.47037 \n   0.621035\n  -4.91899 \n -14.9363  \n   8.26901 \n   \u22ee       \n   7.83882 \n  -9.30699 \n -29.7205  \n  15.2482  \n -19.1784  \n  14.9865  \n   2.32728 \n  -9.11988 \n -15.3472  \n  22.9679  \n  -0.997964\n  42.6068  \u03c1   =   2.0  \u03b2\u0302admm   =   lsq_constrsparsereg_admm ( X ,   y ,   \u03c1 ;   proj   =   x   -   clamp . ( x ,   0 ,   Inf ))   \u03b2\u0302admm   100\u00d71 GLMNet.CompressedPredictorMatrix:\n 0.611673  \n 2.17111   \n 2.65667   \n 4.05568   \n 4.72435   \n 5.87293   \n 6.6957    \n 8.36528   \n 8.61945   \n 9.80517   \n 1.18896e-6\n 0.0       \n 0.0       \n \u22ee         \n 0.100613  \n 0.0       \n 0.0       \n 0.0       \n 0.0       \n 0.0       \n 0.0       \n 0.0       \n 0.0       \n 0.0       \n 6.84603e-7\n 0.0593872  Again we compare the estimates with those from quadratic programming. Here we use  ECOS  solver instead of the default  SCS .   \u03c1   =   2.0   bineq   =   zeros ( p )  Aineq   =   -   eye ( p )  using   ECOS ;   solver = ECOSSolver ( verbose = 0 ,   maxit = 1e8 );  \u03b2\u0302 ,   =   lsq_constrsparsereg ( X ,   y ,   \u03c1 ;   Aineq   =   Aineq ,   bineq   =   bineq ,   solver   =   solver )    \u03b2\u0302   100\u00d71 Array{Float64,2}:\n  0.610587   \n  2.17169    \n  2.65765    \n  4.05601    \n  4.72551    \n  5.87414    \n  6.69414    \n  8.36632    \n  8.62049    \n  9.80458    \n -1.18573e-10\n -8.86515e-11\n -5.48968e-11\n  \u22ee          \n  0.10151    \n  1.08588e-9 \n  3.51552e-10\n -4.42556e-11\n -8.01753e-11\n  1.40626e-10\n -1.58472e-11\n  2.50567e-10\n -4.76544e-11\n  3.28495e-10\n -7.49201e-11\n  0.0602764  Follow this  link  to access the .ipynb file of this page.", 
            "title": "Non-negativity constraint"
        }, 
        {
            "location": "/demo/path/", 
            "text": "Path algorithm\n\n\n\n\nSum-to-zero constraint\n\n\nIn this example, we will solve a problem defined by \n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\boldsymbol{\\beta}||_1  \\\\\n& \\text{subject to} \\hspace{1em} \\sum_j \\beta_j = 0\n\\end{split}\n\n\n\n\n\nNote that we can re-write the constraint as  $\\boldsymbol{A\\beta} = \\boldsymbol{b}$\n\n\nwhere \n\n\n\n\n\n\\boldsymbol{A} = \\begin{pmatrix} 1 & 1 & \\cdots & 1 \\end{pmatrix} \\text{ and } \\boldsymbol{b} = 0.\n\n\n\n\n\nFirst let's generate the predictor matrix \nX\n and response vector \ny\n. To do so, we need a true parameter vector \n\u03b2\n whose sum equals to 0. Note \nn\n is the number of observations \nn\n and \np\n is the number of predictors. \n\n\nn\n,\n \np\n \n=\n \n50\n,\n \n100\n  \n\n\u03b2\n \n=\n \nzeros\n(\np\n)\n\n\n\u03b2\n[\n1\n:\nround\n(\nInt\n,\n \np\n \n/\n \n4\n)]\n \n=\n \n0\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \np\n \n/\n \n4\n)\n \n+\n \n1\n)\n:\nround\n(\nInt\n,\n \np\n \n/\n \n2\n)]\n \n=\n \n1\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \np\n \n/\n \n2\n)\n \n+\n \n1\n)\n:\nround\n(\nInt\n,\n \n3\np\n \n/\n \n4\n)]\n \n=\n \n0\n\n\n\u03b2\n[(\nround\n(\nInt\n,\n \n3\np\n \n/\n \n4\n)\n \n+\n \n1\n)\n:\np\n]\n \n=\n \n-\n1\n\n\nsrand\n(\n41\n)\n\n\nX\n \n=\n \nrandn\n(\nn\n,\n \np\n)\n\n\n\n\n\n\n50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627\n\n\n\n\n\ny\n \n=\n \nX\n \n*\n \n\u03b2\n \n+\n \nrandn\n(\nn\n)\n\n\n\n\n\n\n50-element Array{Float64,1}:\n  -9.90585 \n  -5.40562 \n   5.24289 \n  -6.29951 \n  -4.9586  \n  -6.1342  \n  -7.90981 \n   2.51009 \n  -5.79548 \n   1.61355 \n  -0.722766\n  10.4522  \n   4.03935 \n   \u22ee       \n   0.397781\n  -2.6661  \n   5.36896 \n  -3.56537 \n  -2.402   \n   0.11478 \n  -5.39248 \n   4.38391 \n   0.706801\n -10.1066  \n  -1.12558 \n  14.2473\n\n\n\n\n\nSince the problem has equality constraints only, we define the constraints as below. \n\n\nbeq\n \n=\n \n[\n0\n]\n\n\nAeq\n \n=\n \nones\n(\n1\n,\n \np\n)\n\n\n\n\n\n\n1\u00d7100 Array{Float64,2}:\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \u2026  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n\n\n\n\n\nusing\n \nConstrainedLasso\n\n\n\u03b2\u0302path1\n,\n \n\u03c1path1\n,\n \nobjpath\n,\n \n=\n \nlsq_classopath\n(\nX\n,\n \ny\n;\n \nAeq\n \n=\n \nAeq\n,\n \nbeq\n \n=\n \nbeq\n);\n\n\n\n\n\n\nNow we are ready to obtain the solution path using the path algorithm. By default, we use the solver SCS. \n\n\n\u03b2\u0302path1\n\n\n\n\n\n\n100\u00d764 Array{Float64,2}:\n 0.0  0.0   0.0         0.0       \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.206561   0.212696   0.22402 \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0          -0.378352  -0.411385  -0.41288 \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0       \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.325283   0.3467     0.357212\n 0.0  0.0   0.0         0.0          -0.19861   -0.181371  -0.181397\n 0.0  0.0   0.0         0.0       \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n \u22ee                                \u22f1                                 \n 0.0  0.0   0.0         0.0          -0.46258   -0.452943  -0.458164\n 0.0  0.0   0.0         0.0          -0.401578  -0.359423  -0.358849\n 0.0  0.0   0.0         0.0       \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0  -0.0558231  -0.101399     -0.850614  -0.874227  -0.881474\n 0.0  0.0   0.0         0.0          -1.07203   -1.05001   -1.06761 \n 0.0  0.0   0.0         0.0          -0.674324  -0.621432  -0.622139\n 0.0  0.0   0.0         0.0       \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0          -1.25239   -1.20357   -1.2081  \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0\n\n\n\n\n\nLet's see if sums of coefficients at all $\\rho$ values are approximately 0. \n\n\nall\n(\nabs\n.\n(\nsum\n(\n\u03b2\u0302path1\n,\n \n1\n))\n \n.\n \n1e-6\n)\n\n\n\n\n\n\ntrue\n\n\n\n\n\nWe plot the solution path below. \n\n\nusing\n \nPlots\n;\n \npyplot\n();\n\n\nplot\n(\n\u03c1path1\n,\n \n\u03b2\u0302path1\n,\n \nlabel\n=\n,\n \nxaxis\n \n=\n \n(\n\u03c1\n,\n \n(\nminimum\n(\n\u03c1path1\n),\n\n      \nmaximum\n(\n\u03c1path1\n))),\n \nyaxis\n \n=\n \n(\n\u03b2\u0302(\u03c1)\n),\n \nwidth\n=\n0.5\n)\n \n\ntitle!\n(\nSimulation 1: Solution Path via Constrained Lasso\n)\n \n\n\n\n\n\n\n\n\n\nNon-negativity constraint\n\n\nIn this example, the problem is defined by \n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\boldsymbol{\\beta}||_1  \\\\\n& \\text{subject to} \\hspace{1em} \\beta_j \\geq 0 \\forall j\n\\end{split}\n\n\n\n\n\nWe can re-write the inequality constraint as $\\boldsymbol{C\\beta} \\leq \\boldsymbol{d}$ where \n\n\n\n\n\n\\boldsymbol{C} = \\begin{pmatrix} \n-1 & & & \\\\\n    & -1 & & \\\\\n    &   & \\ddots & \\\\\n    &   &   & -1\n\\end{pmatrix}\n\\text{ and } \\boldsymbol{d} = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}\n\n\n\n\n\nFirst we define a true parameter vector \n\u03b2\n that is sparse with a few non-zero coefficients. Let \nn\n and \np\n be the number of observations and predictors, respectively. \n\n\nn\n,\n \np\n \n=\n \n50\n,\n \n100\n   \n\n\u03b2\n \n=\n \nzeros\n(\np\n)\n\n\n\u03b2\n[\n1\n:\n10\n]\n \n=\n \n1\n:\n10\n\n\nsrand\n(\n41\n)\n\n\nX\n \n=\n \nrandn\n(\nn\n,\n \np\n)\n\n\n\n\n\n\n50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627\n\n\n\n\n\ny\n \n=\n \nX\n \n*\n \n\u03b2\n \n+\n \nrandn\n(\nn\n)\n\n\n\n\n\n\n50-element Array{Float64,1}:\n  12.6173  \n  40.3776  \n   2.2169  \n  27.4631  \n  38.592   \n   7.82023 \n  22.7367  \n   7.88475 \n  -7.47037 \n   0.621035\n  -4.91899 \n -14.9363  \n   8.26901 \n   \u22ee       \n   7.83882 \n  -9.30699 \n -29.7205  \n  15.2482  \n -19.1784  \n  14.9865  \n   2.32728 \n  -9.11988 \n -15.3472  \n  22.9679  \n  -0.997964\n  42.6068\n\n\n\n\n\nNow set up the inequality constraint for the problem.\n\n\nbineq\n \n=\n \nzeros\n(\np\n)\n\n\nAineq\n \n=\n \n-\n \neye\n(\np\n)\n\n\n\n\n\n\n100\u00d7100 Array{Float64,2}:\n -1.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -1.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -1.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -1.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -1.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -1.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n  \u22ee                             \u22ee    \u22f1         \u22ee                          \n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -1.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -1.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -1.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -1.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -1.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -1.0\n\n\n\n\n\nNow we are ready to obtain the solution path using the path algorithm. Here, let's try using different solver \nECOS\n for \nConvex.jl\n. \n\n\nusing\n \nECOS\n;\n \nsolver\n=\nECOSSolver\n(\nverbose\n=\n0\n,\n \nmaxit\n=\n1e8\n);\n\n\n\u03b2\u0302path2\n,\n \n\u03c1path2\n,\n \n=\n \nlsq_classopath\n(\nX\n,\n \ny\n;\n \nAineq\n \n=\n \nAineq\n,\n \nbineq\n \n=\n \nbineq\n,\n \nsolver\n \n=\n \nsolver\n)\n \n\n\n\n\n\n\u03b2\u0302path2\n\n\n\n\n\n\n100\u00d7183 Array{Float64,2}:\n 0.0         0.0      0.0      0.0      \u2026  0.783939   0.791708   0.796529 \n 0.0         0.0      0.0      0.0         2.17561    2.18099    2.18875  \n 0.0         0.0      0.0      0.0         2.99935    3.008      3.01471  \n 0.0         0.0      0.0      0.0         4.30984    4.31056    4.30849  \n 0.0         0.0      0.0      0.0         4.98995    4.99358    4.9955   \n 0.0         0.0      0.0      0.0      \u2026  6.18666    6.18814    6.18596  \n 0.0         0.0      0.0      0.0         6.92076    6.92371    6.92749  \n 0.0         0.0      0.0      0.0         8.56963    8.55907    8.54642  \n 0.0         0.0      0.0      0.0         8.86323    8.864      8.86137  \n 0.00616069  2.01444  2.41323  2.42264     9.8864     9.89486    9.90491  \n 0.0         0.0      0.0      0.0      \u2026  0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n \u22ee                                      \u22f1  \u22ee                              \n 0.0         0.0      0.0      0.0         0.127693   0.122633   0.114126 \n 0.0         0.0      0.0      0.0         0.257807   0.261246   0.265255 \n 0.0         0.0      0.0      0.0      \u2026  0.294213   0.285664   0.272772 \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0      \u2026  0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0838146  0.0914735  0.0978112\n 0.0         0.0      0.0      0.0         0.200482   0.202642   0.201151 \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0\n\n\n\n\n\nWe plot the solution path below. \n\n\nplot\n(\n\u03c1path2\n,\n \n\u03b2\u0302path2\n,\n \nlabel\n=\n,\n \nxaxis\n \n=\n \n(\n\u03c1\n,\n \n(\nminimum\n(\n\u03c1path2\n),\n\n      \nmaximum\n(\n\u03c1path2\n))),\n \nyaxis\n \n=\n \n(\n\u03b2\u0302(\u03c1)\n),\n \nwidth\n=\n0.5\n)\n \n\ntitle!\n(\nSimulation 2: Solution Path via Constrained Lasso\n)\n \n\n\n\n\n\n\n\nFollow this \nlink\n to access the .ipynb file of this page.", 
            "title": "Path Algorithm"
        }, 
        {
            "location": "/demo/path/#path-algorithm", 
            "text": "", 
            "title": "Path algorithm"
        }, 
        {
            "location": "/demo/path/#sum-to-zero-constraint", 
            "text": "In this example, we will solve a problem defined by    \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\boldsymbol{\\beta}||_1  \\\\\n& \\text{subject to} \\hspace{1em} \\sum_j \\beta_j = 0\n\\end{split}   Note that we can re-write the constraint as  $\\boldsymbol{A\\beta} = \\boldsymbol{b}$  where    \n\\boldsymbol{A} = \\begin{pmatrix} 1 & 1 & \\cdots & 1 \\end{pmatrix} \\text{ and } \\boldsymbol{b} = 0.   First let's generate the predictor matrix  X  and response vector  y . To do so, we need a true parameter vector  \u03b2  whose sum equals to 0. Note  n  is the number of observations  n  and  p  is the number of predictors.   n ,   p   =   50 ,   100    \u03b2   =   zeros ( p )  \u03b2 [ 1 : round ( Int ,   p   /   4 )]   =   0  \u03b2 [( round ( Int ,   p   /   4 )   +   1 ) : round ( Int ,   p   /   2 )]   =   1  \u03b2 [( round ( Int ,   p   /   2 )   +   1 ) : round ( Int ,   3 p   /   4 )]   =   0  \u03b2 [( round ( Int ,   3 p   /   4 )   +   1 ) : p ]   =   - 1  srand ( 41 )  X   =   randn ( n ,   p )   50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627  y   =   X   *   \u03b2   +   randn ( n )   50-element Array{Float64,1}:\n  -9.90585 \n  -5.40562 \n   5.24289 \n  -6.29951 \n  -4.9586  \n  -6.1342  \n  -7.90981 \n   2.51009 \n  -5.79548 \n   1.61355 \n  -0.722766\n  10.4522  \n   4.03935 \n   \u22ee       \n   0.397781\n  -2.6661  \n   5.36896 \n  -3.56537 \n  -2.402   \n   0.11478 \n  -5.39248 \n   4.38391 \n   0.706801\n -10.1066  \n  -1.12558 \n  14.2473  Since the problem has equality constraints only, we define the constraints as below.   beq   =   [ 0 ]  Aeq   =   ones ( 1 ,   p )   1\u00d7100 Array{Float64,2}:\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \u2026  1.0  1.0  1.0  1.0  1.0  1.0  1.0  using   ConstrainedLasso  \u03b2\u0302path1 ,   \u03c1path1 ,   objpath ,   =   lsq_classopath ( X ,   y ;   Aeq   =   Aeq ,   beq   =   beq );   Now we are ready to obtain the solution path using the path algorithm. By default, we use the solver SCS.   \u03b2\u0302path1   100\u00d764 Array{Float64,2}:\n 0.0  0.0   0.0         0.0       \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.206561   0.212696   0.22402 \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0          -0.378352  -0.411385  -0.41288 \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0       \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.325283   0.3467     0.357212\n 0.0  0.0   0.0         0.0          -0.19861   -0.181371  -0.181397\n 0.0  0.0   0.0         0.0       \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n \u22ee                                \u22f1                                 \n 0.0  0.0   0.0         0.0          -0.46258   -0.452943  -0.458164\n 0.0  0.0   0.0         0.0          -0.401578  -0.359423  -0.358849\n 0.0  0.0   0.0         0.0       \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0  -0.0558231  -0.101399     -0.850614  -0.874227  -0.881474\n 0.0  0.0   0.0         0.0          -1.07203   -1.05001   -1.06761 \n 0.0  0.0   0.0         0.0          -0.674324  -0.621432  -0.622139\n 0.0  0.0   0.0         0.0       \u2026   0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0          -1.25239   -1.20357   -1.2081  \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0     \n 0.0  0.0   0.0         0.0           0.0        0.0        0.0  Let's see if sums of coefficients at all $\\rho$ values are approximately 0.   all ( abs . ( sum ( \u03b2\u0302path1 ,   1 ))   .   1e-6 )   true  We plot the solution path below.   using   Plots ;   pyplot ();  plot ( \u03c1path1 ,   \u03b2\u0302path1 ,   label = ,   xaxis   =   ( \u03c1 ,   ( minimum ( \u03c1path1 ), \n       maximum ( \u03c1path1 ))),   yaxis   =   ( \u03b2\u0302(\u03c1) ),   width = 0.5 )   title! ( Simulation 1: Solution Path via Constrained Lasso )", 
            "title": "Sum-to-zero constraint"
        }, 
        {
            "location": "/demo/path/#non-negativity-constraint", 
            "text": "In this example, the problem is defined by    \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\boldsymbol{\\beta}||_1  \\\\\n& \\text{subject to} \\hspace{1em} \\beta_j \\geq 0 \\forall j\n\\end{split}   We can re-write the inequality constraint as $\\boldsymbol{C\\beta} \\leq \\boldsymbol{d}$ where    \n\\boldsymbol{C} = \\begin{pmatrix} \n-1 & & & \\\\\n    & -1 & & \\\\\n    &   & \\ddots & \\\\\n    &   &   & -1\n\\end{pmatrix}\n\\text{ and } \\boldsymbol{d} = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}   First we define a true parameter vector  \u03b2  that is sparse with a few non-zero coefficients. Let  n  and  p  be the number of observations and predictors, respectively.   n ,   p   =   50 ,   100     \u03b2   =   zeros ( p )  \u03b2 [ 1 : 10 ]   =   1 : 10  srand ( 41 )  X   =   randn ( n ,   p )   50\u00d7100 Array{Float64,2}:\n  1.21212    -0.153889    0.141533  \u2026  -0.458125    0.0951976  -2.14019   \n  0.345895    1.30676     1.60944      -0.409901    0.323719    0.989333  \n -1.27859    -1.18894     0.512064      1.80509     1.62606    -1.44251   \n  0.230616    2.54741    -0.523533      2.73358     1.07999     0.432834  \n -1.17103    -0.39082     0.441921     -0.179239   -0.158189   -0.640611  \n  1.67135     0.0829011   0.964089  \u2026  -0.720038    1.99359    -0.671572  \n -0.614717    2.16204    -0.0602       -0.324456   -0.616887    1.11243   \n -0.810535    0.974719   -0.045405      0.881578    1.29611     0.696869  \n -1.10879    -1.32489    -1.18272       0.579381   -0.971269   -0.687591  \n -0.219752   -0.447897   -0.974186     -0.880804   -0.480702   -1.36887   \n  0.0952544  -0.126203   -0.273737  \u2026  -0.264421    0.565684   -0.798719  \n  1.4126      0.295896   -0.213161     -1.46343    -1.27144    -0.0589753 \n -0.418407   -0.479389    0.324243      1.96976     0.867659   -1.2999    \n  \u22ee                                 \u22f1                                     \n  0.504861   -1.03911    -0.357771      0.815027    0.919037    1.07463   \n -0.820358   -0.955319    0.097768      0.553219    1.56424     0.10535   \n  1.39684     1.93183     0.706641  \u2026  -0.0222014   0.987281   -0.0646814 \n -1.55206     0.446778    1.48206      -1.42384    -1.04209     0.0460478 \n  0.928527    0.933087   -0.641975     -1.16347    -0.313851   -1.20434   \n  0.380879   -0.144713    1.54374      -0.605637    0.408246    0.632131  \n -1.30233    -2.31664     1.51324       0.765034   -0.515553    0.984551  \n  1.36747     1.34059    -0.114778  \u2026   0.846682   -0.565511   -0.539113  \n -2.82496    -0.0447351   0.426242     -0.353497   -0.14583    -0.00304009\n -0.847741    1.49306     1.15522       0.637659    1.70818     0.641035  \n -0.22286    -0.43932    -0.373259      0.788337    0.223785   -0.343495  \n  1.32145     0.104516   -0.993017     -0.272744   -0.133748    0.968627  y   =   X   *   \u03b2   +   randn ( n )   50-element Array{Float64,1}:\n  12.6173  \n  40.3776  \n   2.2169  \n  27.4631  \n  38.592   \n   7.82023 \n  22.7367  \n   7.88475 \n  -7.47037 \n   0.621035\n  -4.91899 \n -14.9363  \n   8.26901 \n   \u22ee       \n   7.83882 \n  -9.30699 \n -29.7205  \n  15.2482  \n -19.1784  \n  14.9865  \n   2.32728 \n  -9.11988 \n -15.3472  \n  22.9679  \n  -0.997964\n  42.6068  Now set up the inequality constraint for the problem.  bineq   =   zeros ( p )  Aineq   =   -   eye ( p )   100\u00d7100 Array{Float64,2}:\n -1.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -1.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -1.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -1.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -1.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -1.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n  \u22ee                             \u22ee    \u22f1         \u22ee                          \n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -1.0  -0.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  \u2026  -0.0  -1.0  -0.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -1.0  -0.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -1.0  -0.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -1.0  -0.0\n -0.0  -0.0  -0.0  -0.0  -0.0  -0.0     -0.0  -0.0  -0.0  -0.0  -0.0  -1.0  Now we are ready to obtain the solution path using the path algorithm. Here, let's try using different solver  ECOS  for  Convex.jl .   using   ECOS ;   solver = ECOSSolver ( verbose = 0 ,   maxit = 1e8 );  \u03b2\u0302path2 ,   \u03c1path2 ,   =   lsq_classopath ( X ,   y ;   Aineq   =   Aineq ,   bineq   =   bineq ,   solver   =   solver )    \u03b2\u0302path2   100\u00d7183 Array{Float64,2}:\n 0.0         0.0      0.0      0.0      \u2026  0.783939   0.791708   0.796529 \n 0.0         0.0      0.0      0.0         2.17561    2.18099    2.18875  \n 0.0         0.0      0.0      0.0         2.99935    3.008      3.01471  \n 0.0         0.0      0.0      0.0         4.30984    4.31056    4.30849  \n 0.0         0.0      0.0      0.0         4.98995    4.99358    4.9955   \n 0.0         0.0      0.0      0.0      \u2026  6.18666    6.18814    6.18596  \n 0.0         0.0      0.0      0.0         6.92076    6.92371    6.92749  \n 0.0         0.0      0.0      0.0         8.56963    8.55907    8.54642  \n 0.0         0.0      0.0      0.0         8.86323    8.864      8.86137  \n 0.00616069  2.01444  2.41323  2.42264     9.8864     9.89486    9.90491  \n 0.0         0.0      0.0      0.0      \u2026  0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n \u22ee                                      \u22f1  \u22ee                              \n 0.0         0.0      0.0      0.0         0.127693   0.122633   0.114126 \n 0.0         0.0      0.0      0.0         0.257807   0.261246   0.265255 \n 0.0         0.0      0.0      0.0      \u2026  0.294213   0.285664   0.272772 \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0      \u2026  0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0838146  0.0914735  0.0978112\n 0.0         0.0      0.0      0.0         0.200482   0.202642   0.201151 \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0      \n 0.0         0.0      0.0      0.0         0.0        0.0        0.0  We plot the solution path below.   plot ( \u03c1path2 ,   \u03b2\u0302path2 ,   label = ,   xaxis   =   ( \u03c1 ,   ( minimum ( \u03c1path2 ), \n       maximum ( \u03c1path2 ))),   yaxis   =   ( \u03b2\u0302(\u03c1) ),   width = 0.5 )   title! ( Simulation 2: Solution Path via Constrained Lasso )     Follow this  link  to access the .ipynb file of this page.", 
            "title": "Non-negativity constraint"
        }, 
        {
            "location": "/demo/prostate/", 
            "text": "Prostate Data\n\n\nThis demonstration solves a regular, unconstrained lasso problem using the constrained lasso solution path (\nlsq_classopath.jl\n).\n\n\nThe \nprostate\n data come from a study that examined the correlation between the level of prostate specific antigen and a number of clinical measures in men who were about to receive a radical prostatectomy. (\nStamey et al. (1989)\n)\n\n\nLet's load and organize the \nprostate\n data. Since we are interested in the following variables as predictors, we extract them and create a design matrix \nXz\n:\n\n\n\n\nlcavol\n : log(cancer volume)\n\n\nlweight\n: log(prostate weight)\n\n\nage\n    : age\n\n\nlbph\n   : log(benign prostatic hyperplasia amount)\n\n\nsvi\n    : seminal vesicle invasion\n\n\nlcp\n    : log(capsular penetration)\n\n\ngleason\n: Gleason score\n\n\npgg45\n  : percentage Gleason scores 4 or 5\n\n\n\n\nThe response variable is \nlpsa\n, which is log(prostate specific antigen). \n\n\nusing\n \nConstrainedLasso\n \n\nprostate\n \n=\n \nreadcsv\n(\nmisc/prostate.csv\n,\n \nheader\n=\ntrue\n)\n\n\ntmp\n \n=\n \n[]\n\n\nlabels\n \n=\n \n[\nlcavol\n \nlweight\n \nage\n \nlbph\n \nsvi\n \nlcp\n \ngleason\n \npgg45\n]\n\n\nfor\n \ni\n \nin\n \nlabels\n\n    \npush!\n(\ntmp\n,\n \nfind\n(\nx\n \n-\n \nx\n \n==\n \ni\n,\n \nprostate\n[\n2\n])[\n1\n])\n\n\nend\n\n\nXz\n \n=\n \nArray\n{\nFloat64\n}(\nprostate\n[\n1\n][\n:\n,\n \ntmp\n])\n\n\n\n\n\n\n97\u00d78 Array{Float64,2}:\n -0.579818  2.76946  50.0  -1.38629   0.0  -1.38629   6.0   0.0\n -0.994252  3.31963  58.0  -1.38629   0.0  -1.38629   6.0   0.0\n -0.510826  2.69124  74.0  -1.38629   0.0  -1.38629   7.0  20.0\n -1.20397   3.28279  58.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.751416  3.43237  62.0  -1.38629   0.0  -1.38629   6.0   0.0\n -1.04982   3.22883  50.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.737164  3.47352  64.0   0.615186  0.0  -1.38629   6.0   0.0\n  0.693147  3.53951  58.0   1.53687   0.0  -1.38629   6.0   0.0\n -0.776529  3.53951  47.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.223144  3.24454  63.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.254642  3.60414  65.0  -1.38629   0.0  -1.38629   6.0   0.0\n -1.34707   3.59868  63.0   1.26695   0.0  -1.38629   6.0   0.0\n  1.61343   3.02286  63.0  -1.38629   0.0  -0.597837  7.0  30.0\n  \u22ee                                         \u22ee                  \n  3.30285   3.51898  64.0  -1.38629   1.0   2.32728   7.0  60.0\n  2.02419   3.7317   58.0   1.639     0.0  -1.38629   6.0   0.0\n  1.73166   3.36902  62.0  -1.38629   1.0   0.300105  7.0  30.0\n  2.80759   4.71805  65.0  -1.38629   1.0   2.46385   7.0  60.0\n  1.56235   3.69511  76.0   0.936093  1.0   0.81093   7.0  75.0\n  3.24649   4.10182  68.0  -1.38629   0.0  -1.38629   6.0   0.0\n  2.5329    3.67757  61.0   1.34807   1.0  -1.38629   7.0  15.0\n  2.83027   3.8764   68.0  -1.38629   1.0   1.32176   7.0  60.0\n  3.821     3.89691  44.0  -1.38629   1.0   2.16905   7.0  40.0\n  2.90745   3.39619  52.0  -1.38629   1.0   2.46385   7.0  10.0\n  2.88256   3.77391  68.0   1.55814   1.0   1.55814   7.0  80.0\n  3.47197   3.975    68.0   0.438255  1.0   2.90417   7.0  20.0\n\n\n\n\n\ny\n \n=\n \nArray\n{\nFloat64\n}(\nprostate\n[\n1\n][\n:\n,\n \nend\n-\n1\n])\n\n\n\n\n\n\n97-element Array{Float64,1}:\n -0.430783\n -0.162519\n -0.162519\n -0.162519\n  0.371564\n  0.765468\n  0.765468\n  0.854415\n  1.04732 \n  1.04732 \n  1.26695 \n  1.26695 \n  1.26695 \n  \u22ee       \n  3.63099 \n  3.68009 \n  3.71235 \n  3.98434 \n  3.9936  \n  4.02981 \n  4.12955 \n  4.38515 \n  4.68444 \n  5.14312 \n  5.47751 \n  5.58293\n\n\n\n\n\nFirst we standardize the data by subtracting its mean and dividing by its standard deviation. \n\n\nn\n,\n \np\n \n=\n \nsize\n(\nXz\n)\n\n\nfor\n \ni\n \nin\n \n1\n:\nsize\n(\nXz\n,\n2\n)\n\n    \nXz\n[\n:\n,\n \ni\n]\n \n-=\n \nmean\n(\nXz\n[\n:\n,\n \ni\n])\n\n    \nXz\n[\n:\n,\n \ni\n]\n \n/=\n \nstd\n(\nXz\n[\n:\n,\n \ni\n])\n\n\nend\n\n\nXz\n\n\n\n\n\n\n97\u00d78 Array{Float64,2}:\n -1.63736   -2.00621    -1.86243    \u2026  -0.863171  -1.04216   -0.864467\n -1.98898   -0.722009   -0.787896      -0.863171  -1.04216   -0.864467\n -1.57882   -2.18878     1.36116       -0.863171   0.342627  -0.155348\n -2.16692   -0.807994   -0.787896      -0.863171  -1.04216   -0.864467\n -0.507874  -0.458834   -0.250631      -0.863171  -1.04216   -0.864467\n -2.03613   -0.933955   -1.86243    \u2026  -0.863171  -1.04216   -0.864467\n -0.519967  -0.362793    0.0180011     -0.863171  -1.04216   -0.864467\n -0.557313  -0.208757   -0.787896      -0.863171  -1.04216   -0.864467\n -1.80425   -0.208757   -2.26537       -0.863171  -1.04216   -0.864467\n -0.956085  -0.897266   -0.116315      -0.863171  -1.04216   -0.864467\n -0.92936   -0.0578992   0.152317   \u2026  -0.863171  -1.04216   -0.864467\n -2.28833   -0.0706369  -0.116315      -0.863171  -1.04216   -0.864467\n  0.223498  -1.41472    -0.116315      -0.299282   0.342627   0.199211\n  \u22ee                                 \u22f1   \u22ee                             \n  1.65688   -0.256675    0.0180011  \u2026   1.7927     0.342627   1.26289 \n  0.572009   0.239854   -0.787896      -0.863171  -1.04216   -0.864467\n  0.323806  -0.606718   -0.250631       0.342907   0.342627   0.199211\n  1.23668    2.54221     0.152317       1.89038    0.342627   1.26289 \n  0.180156   0.154448    1.6298         0.70824    0.342627   1.79473 \n  1.60906    1.10379     0.555266   \u2026  -0.863171  -1.04216   -0.864467\n  1.00362    0.113497   -0.384948      -0.863171   0.342627  -0.332628\n  1.25592    0.577607    0.555266       1.07357    0.342627   1.26289 \n  2.09651    0.625489   -2.66832        1.67954    0.342627   0.55377 \n  1.3214    -0.543304   -1.59379        1.89038    0.342627  -0.509907\n  1.30029    0.338384    0.555266   \u2026   1.24263    0.342627   1.97201 \n  1.80037    0.807764    0.555266       2.20528    0.342627  -0.155348\n\n\n\n\n\nNow we solve the problem using solution path algorithm. \n\n\n\u03b2path\n,\n \n\u03c1path\n,\n \n=\n \nlsq_classopath\n(\nXz\n,\n \ny\n);\n\n\n\u03b2path\n\n\n\n\n\n\n8\u00d79 Array{Float64,2}:\n 0.0  0.421131  0.461588   0.557453  \u2026   0.597269    0.602923    0.665147 \n 0.0  0.0       0.0        0.171805      0.232686    0.246293    0.26648  \n 0.0  0.0       0.0        0.0          -0.0598626  -0.0937415  -0.158195 \n 0.0  0.0       0.0        0.0           0.088065    0.108124    0.140311 \n 0.0  0.0       0.0404563  0.182941      0.2436      0.252692    0.315329 \n 0.0  0.0       0.0        0.0       \u2026   0.0         0.0        -0.148286 \n 0.0  0.0       0.0        0.0           0.0         0.0123116   0.0355492\n 0.0  0.0       0.0        0.0           0.0645835   0.0700037   0.12572\n\n\n\n\n\nWe plot the solution path below. \n\n\nusing\n \nPlots\n;\n \npyplot\n();\n \n\ncolors\n \n=\n \n[\n:\ngreen\n \n:\norange\n \n:\nblack\n \n:\npurple\n \n:\nred\n \n:\ngrey\n \n:\nbrown\n \n:\nblue\n]\n \n\nplot\n(\n\u03c1path\n,\n \n\u03b2path\n,\n \nxaxis\n \n=\n \n(\n\u03c1\n,\n \n(\nminimum\n(\n\u03c1path\n),\n\n      \nmaximum\n(\n\u03c1path\n))),\n \nyaxis\n \n=\n \n(\n\u03b2\u0302(\u03c1)\n),\n \nlabel\n=\nlabels\n,\n \ncolor\n=\ncolors\n)\n\n\ntitle!\n(\nProstrate Data: Solution Path via Constrained Lasso\n)\n\n\n\n\n\n\n\n\nBelow, we solve the same problem using \nGLMNet.jl\n package. \n\n\nusing\n \nGLMNet\n;\n  \n\npath\n \n=\n \nglmnet\n(\nXz\n,\n \ny\n,\n \nintercept\n=\nfalse\n);\n\n\npath\n.\nbetas\n\n\n\n\n\n\n8\u00d770 GLMNet.CompressedPredictorMatrix:\n 0.0  0.075317  0.143943  0.206473  \u2026   0.660413   0.660823   0.661201 \n 0.0  0.0       0.0       0.0           0.264962   0.265099   0.265223 \n 0.0  0.0       0.0       0.0          -0.153231  -0.153657  -0.154055 \n 0.0  0.0       0.0       0.0           0.137859   0.138074   0.138272 \n 0.0  0.0       0.0       0.0           0.310435   0.310849   0.311241 \n 0.0  0.0       0.0       0.0       \u2026  -0.136715  -0.137682  -0.138598 \n 0.0  0.0       0.0       0.0           0.033943   0.034123   0.0342689\n 0.0  0.0       0.0       0.0           0.121233   0.121575   0.121916\n\n\n\n\n\nplot\n(\npath\n.\nlambda\n,\n \npath\n.\nbetas\n,\n \ncolor\n=\ncolors\n,\n \nlabel\n=\nlabels\n,\n \n        \nxaxis\n=\n(\n\u03bb\n),\n \nyaxis\n=\n \n(\n\u03b2\u0302(\u03bb)\n))\n\n\n\n\n\n\n\n\nFollow this \nlink\n to access the .ipynb file of this page.", 
            "title": "Prostate Data"
        }, 
        {
            "location": "/demo/prostate/#prostate-data", 
            "text": "This demonstration solves a regular, unconstrained lasso problem using the constrained lasso solution path ( lsq_classopath.jl ).  The  prostate  data come from a study that examined the correlation between the level of prostate specific antigen and a number of clinical measures in men who were about to receive a radical prostatectomy. ( Stamey et al. (1989) )  Let's load and organize the  prostate  data. Since we are interested in the following variables as predictors, we extract them and create a design matrix  Xz :   lcavol  : log(cancer volume)  lweight : log(prostate weight)  age     : age  lbph    : log(benign prostatic hyperplasia amount)  svi     : seminal vesicle invasion  lcp     : log(capsular penetration)  gleason : Gleason score  pgg45   : percentage Gleason scores 4 or 5   The response variable is  lpsa , which is log(prostate specific antigen).   using   ConstrainedLasso   prostate   =   readcsv ( misc/prostate.csv ,   header = true )  tmp   =   []  labels   =   [ lcavol   lweight   age   lbph   svi   lcp   gleason   pgg45 ]  for   i   in   labels \n     push! ( tmp ,   find ( x   -   x   ==   i ,   prostate [ 2 ])[ 1 ])  end  Xz   =   Array { Float64 }( prostate [ 1 ][ : ,   tmp ])   97\u00d78 Array{Float64,2}:\n -0.579818  2.76946  50.0  -1.38629   0.0  -1.38629   6.0   0.0\n -0.994252  3.31963  58.0  -1.38629   0.0  -1.38629   6.0   0.0\n -0.510826  2.69124  74.0  -1.38629   0.0  -1.38629   7.0  20.0\n -1.20397   3.28279  58.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.751416  3.43237  62.0  -1.38629   0.0  -1.38629   6.0   0.0\n -1.04982   3.22883  50.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.737164  3.47352  64.0   0.615186  0.0  -1.38629   6.0   0.0\n  0.693147  3.53951  58.0   1.53687   0.0  -1.38629   6.0   0.0\n -0.776529  3.53951  47.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.223144  3.24454  63.0  -1.38629   0.0  -1.38629   6.0   0.0\n  0.254642  3.60414  65.0  -1.38629   0.0  -1.38629   6.0   0.0\n -1.34707   3.59868  63.0   1.26695   0.0  -1.38629   6.0   0.0\n  1.61343   3.02286  63.0  -1.38629   0.0  -0.597837  7.0  30.0\n  \u22ee                                         \u22ee                  \n  3.30285   3.51898  64.0  -1.38629   1.0   2.32728   7.0  60.0\n  2.02419   3.7317   58.0   1.639     0.0  -1.38629   6.0   0.0\n  1.73166   3.36902  62.0  -1.38629   1.0   0.300105  7.0  30.0\n  2.80759   4.71805  65.0  -1.38629   1.0   2.46385   7.0  60.0\n  1.56235   3.69511  76.0   0.936093  1.0   0.81093   7.0  75.0\n  3.24649   4.10182  68.0  -1.38629   0.0  -1.38629   6.0   0.0\n  2.5329    3.67757  61.0   1.34807   1.0  -1.38629   7.0  15.0\n  2.83027   3.8764   68.0  -1.38629   1.0   1.32176   7.0  60.0\n  3.821     3.89691  44.0  -1.38629   1.0   2.16905   7.0  40.0\n  2.90745   3.39619  52.0  -1.38629   1.0   2.46385   7.0  10.0\n  2.88256   3.77391  68.0   1.55814   1.0   1.55814   7.0  80.0\n  3.47197   3.975    68.0   0.438255  1.0   2.90417   7.0  20.0  y   =   Array { Float64 }( prostate [ 1 ][ : ,   end - 1 ])   97-element Array{Float64,1}:\n -0.430783\n -0.162519\n -0.162519\n -0.162519\n  0.371564\n  0.765468\n  0.765468\n  0.854415\n  1.04732 \n  1.04732 \n  1.26695 \n  1.26695 \n  1.26695 \n  \u22ee       \n  3.63099 \n  3.68009 \n  3.71235 \n  3.98434 \n  3.9936  \n  4.02981 \n  4.12955 \n  4.38515 \n  4.68444 \n  5.14312 \n  5.47751 \n  5.58293  First we standardize the data by subtracting its mean and dividing by its standard deviation.   n ,   p   =   size ( Xz )  for   i   in   1 : size ( Xz , 2 ) \n     Xz [ : ,   i ]   -=   mean ( Xz [ : ,   i ]) \n     Xz [ : ,   i ]   /=   std ( Xz [ : ,   i ])  end  Xz   97\u00d78 Array{Float64,2}:\n -1.63736   -2.00621    -1.86243    \u2026  -0.863171  -1.04216   -0.864467\n -1.98898   -0.722009   -0.787896      -0.863171  -1.04216   -0.864467\n -1.57882   -2.18878     1.36116       -0.863171   0.342627  -0.155348\n -2.16692   -0.807994   -0.787896      -0.863171  -1.04216   -0.864467\n -0.507874  -0.458834   -0.250631      -0.863171  -1.04216   -0.864467\n -2.03613   -0.933955   -1.86243    \u2026  -0.863171  -1.04216   -0.864467\n -0.519967  -0.362793    0.0180011     -0.863171  -1.04216   -0.864467\n -0.557313  -0.208757   -0.787896      -0.863171  -1.04216   -0.864467\n -1.80425   -0.208757   -2.26537       -0.863171  -1.04216   -0.864467\n -0.956085  -0.897266   -0.116315      -0.863171  -1.04216   -0.864467\n -0.92936   -0.0578992   0.152317   \u2026  -0.863171  -1.04216   -0.864467\n -2.28833   -0.0706369  -0.116315      -0.863171  -1.04216   -0.864467\n  0.223498  -1.41472    -0.116315      -0.299282   0.342627   0.199211\n  \u22ee                                 \u22f1   \u22ee                             \n  1.65688   -0.256675    0.0180011  \u2026   1.7927     0.342627   1.26289 \n  0.572009   0.239854   -0.787896      -0.863171  -1.04216   -0.864467\n  0.323806  -0.606718   -0.250631       0.342907   0.342627   0.199211\n  1.23668    2.54221     0.152317       1.89038    0.342627   1.26289 \n  0.180156   0.154448    1.6298         0.70824    0.342627   1.79473 \n  1.60906    1.10379     0.555266   \u2026  -0.863171  -1.04216   -0.864467\n  1.00362    0.113497   -0.384948      -0.863171   0.342627  -0.332628\n  1.25592    0.577607    0.555266       1.07357    0.342627   1.26289 \n  2.09651    0.625489   -2.66832        1.67954    0.342627   0.55377 \n  1.3214    -0.543304   -1.59379        1.89038    0.342627  -0.509907\n  1.30029    0.338384    0.555266   \u2026   1.24263    0.342627   1.97201 \n  1.80037    0.807764    0.555266       2.20528    0.342627  -0.155348  Now we solve the problem using solution path algorithm.   \u03b2path ,   \u03c1path ,   =   lsq_classopath ( Xz ,   y );  \u03b2path   8\u00d79 Array{Float64,2}:\n 0.0  0.421131  0.461588   0.557453  \u2026   0.597269    0.602923    0.665147 \n 0.0  0.0       0.0        0.171805      0.232686    0.246293    0.26648  \n 0.0  0.0       0.0        0.0          -0.0598626  -0.0937415  -0.158195 \n 0.0  0.0       0.0        0.0           0.088065    0.108124    0.140311 \n 0.0  0.0       0.0404563  0.182941      0.2436      0.252692    0.315329 \n 0.0  0.0       0.0        0.0       \u2026   0.0         0.0        -0.148286 \n 0.0  0.0       0.0        0.0           0.0         0.0123116   0.0355492\n 0.0  0.0       0.0        0.0           0.0645835   0.0700037   0.12572  We plot the solution path below.   using   Plots ;   pyplot ();   colors   =   [ : green   : orange   : black   : purple   : red   : grey   : brown   : blue ]   plot ( \u03c1path ,   \u03b2path ,   xaxis   =   ( \u03c1 ,   ( minimum ( \u03c1path ), \n       maximum ( \u03c1path ))),   yaxis   =   ( \u03b2\u0302(\u03c1) ),   label = labels ,   color = colors )  title! ( Prostrate Data: Solution Path via Constrained Lasso )    Below, we solve the same problem using  GLMNet.jl  package.   using   GLMNet ;    path   =   glmnet ( Xz ,   y ,   intercept = false );  path . betas   8\u00d770 GLMNet.CompressedPredictorMatrix:\n 0.0  0.075317  0.143943  0.206473  \u2026   0.660413   0.660823   0.661201 \n 0.0  0.0       0.0       0.0           0.264962   0.265099   0.265223 \n 0.0  0.0       0.0       0.0          -0.153231  -0.153657  -0.154055 \n 0.0  0.0       0.0       0.0           0.137859   0.138074   0.138272 \n 0.0  0.0       0.0       0.0           0.310435   0.310849   0.311241 \n 0.0  0.0       0.0       0.0       \u2026  -0.136715  -0.137682  -0.138598 \n 0.0  0.0       0.0       0.0           0.033943   0.034123   0.0342689\n 0.0  0.0       0.0       0.0           0.121233   0.121575   0.121916  plot ( path . lambda ,   path . betas ,   color = colors ,   label = labels ,  \n         xaxis = ( \u03bb ),   yaxis =   ( \u03b2\u0302(\u03bb) ))    Follow this  link  to access the .ipynb file of this page.", 
            "title": "Prostate Data"
        }, 
        {
            "location": "/demo/warming/", 
            "text": "Global Warming Data\n\n\nHere we consider the annual data on temperature anomalies. As has been previously noted in the literature on isotonic regression, in general temperature appears to increase monotonically over the time period of 1850 to 2015 (\nWu et al., 2001\n; \nTibshirani et al., 2011\n). This monotonicity can be imposed on the coeffcient estimates using the constrained lasso with the inequality constraint matrix:\n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{1em} \\boldsymbol{C\\beta} \\leq \\boldsymbol{d} \n\\end{split}\n\n\n\n\n\nwhere \n\n\n\n\n\n\\boldsymbol{C} = \\begin{pmatrix} \n1 & -1 &     &        &       &     & \\\\\n  & 1  & -1  &        &         &   & \\\\\n  &    &  1  & -1     &         &   & \\\\\n  &     &       & \\ddots & \\ddots &  & \\\\\n  &     &       &        &          & 1 & -1 \\\\\n\\end{pmatrix}\n\n\n\n\n\nand $\\boldsymbol{d} = \\boldsymbol{0}.$\n\n\nusing\n \nConstrainedLasso\n \n\nusing\n \nECOS\n\n\n\n\n\n\nFirst we load and organize the data. \n\n\nwarming\n \n=\n \nreadcsv\n(\nmisc/warming.csv\n,\n \nheader\n=\ntrue\n)[\n1\n]\n\n\nyear\n \n=\n \nwarming\n[\n:\n,\n \n1\n]\n\n\ny\n    \n=\n \nwarming\n[\n:\n,\n \n2\n]\n\n\n\n\n\n\n166-element Array{Float64,1}:\n -0.375\n -0.223\n -0.224\n -0.271\n -0.246\n -0.271\n -0.352\n -0.46 \n -0.466\n -0.286\n -0.346\n -0.409\n -0.522\n  \u22ee    \n  0.45 \n  0.544\n  0.505\n  0.493\n  0.395\n  0.506\n  0.559\n  0.422\n  0.47 \n  0.499\n  0.567\n  0.746\n\n\n\n\n\nn\n \n=\n \np\n \n=\n \nsize\n(\ny\n,\n \n1\n)\n\n\nX\n \n=\n \neye\n(\nn\n)\n\n\n\n\n\n\n166\u00d7166 Array{Float64,2}:\n 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n \u22ee                        \u22ee              \u22f1       \u22ee                        \u22ee  \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  1.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  1.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  1.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n\n\n\n\n\nNow we define inequality constraints as specified earlier. \n\n\nC\n \n=\n \n[\neye\n(\np\n-\n1\n)\n \nzeros\n(\np\n-\n1\n,\n \n1\n)]\n \n-\n \n[\nzeros\n(\np\n-\n1\n,\n \n1\n)\n \neye\n(\np\n-\n1\n)]\n\n\n\n\n\n\n165\u00d7166 Array{Float64,2}:\n 1.0  -1.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   1.0  -1.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   1.0  -1.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   1.0  -1.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   1.0  -1.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   1.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n \u22ee                             \u22ee    \u22f1   \u22ee                             \u22ee  \n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0     -1.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   1.0  -1.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   1.0  -1.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   1.0  -1.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   1.0  -1.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   1.0  -1.0\n\n\n\n\n\nd\n \n=\n \nzeros\n(\nsize\n(\nC\n,\n \n1\n))\n\n\n\n\n\n\n165-element Array{Float64,1}:\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n \u22ee  \n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n\n\n\n\n\nThen we estimate constrained lasso solution path. Here we use \nECOS\n solver rather than the default \nSCS\n solver. \n\n\n\u03b2\u0302path\n,\n \n\u03c1path\n,\n \n=\n \nlsq_classopath\n(\nX\n,\n \ny\n;\n \nAineq\n \n=\n \nC\n,\n \nbineq\n \n=\n \nd\n,\n \nsolver\n \n=\n \nECOSSolver\n(\nverbose\n=\n0\n,\n \nmaxit\n=\n1e8\n));\n \n\n\n\n\n\n\u03b2\u0302path\n\n\n\n\n\n\n166\u00d7198 Array{Float64,2}:\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026  -0.323225  -0.366     -0.375   \n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026  -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026  -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n \u22ee                        \u22ee         \u22f1   \u22ee                             \n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026   0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026   0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0           0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0           0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0           0.447225   0.49       0.499   \n 0.0  0.0  0.0  0.0  0.0  0.0           0.515225   0.558      0.567   \n 0.0  0.0  0.0  0.0  0.0  0.011639  \u2026   0.699138   0.737854   0.746\n\n\n\n\n\nIn this formulation, isotonic regression is a special case of the constrained lasso with $\\rho=0.$ Below, \nmonoreg\n is coefficient estimates obtained using isotonic regression. \n\n\nmonoreg\n \n=\n \nreaddlm\n(\nmisc/monoreg.txt\n)\n\n\n\n\n\n\n166\u00d71 Array{Float64,2}:\n -0.375   \n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n  \u22ee       \n  0.484571\n  0.484571\n  0.484571\n  0.484571\n  0.484571\n  0.48925 \n  0.48925 \n  0.48925 \n  0.48925 \n  0.499   \n  0.567   \n  0.746\n\n\n\n\n\nNow let's compare estimates by obtaining the largest absolute difference between isotonic regression constrained lasso fit. \n\n\nmaximum\n(\nabs\n.\n(\nmonoreg\n \n-\n \n\u03b2\u0302path\n[\n:\n,\n \nend\n]))\n\n\n\n\n\n\n1.2212453270876722e-15\n\n\n\n\n\nBelow is a figure that plots the constrained lasso fit at $\\rho = 0$ with the estimates using isotonic regression.\n\n\nusing\n \nPlots\n;\n \npyplot\n();\n \n\nscatter\n(\nyear\n,\n \ny\n,\n \nlabel\n=\nObserved Data\n,\n \nmarkerstrokecolor\n=\ndarkblue\n,\n \n        \nmarkercolor\n=\nwhite\n)\n\n\nscatter!\n(\nyear\n,\n \n\u03b2\u0302path\n[\n:\n,\n \nend\n],\n \nlabel\n=\nClassopath (\u03c1=0)\n,\n \n        \nmarkerstrokecolor\n=\nblack\n,\n \nmarker\n=:\nrect\n,\n \nmarkercolor\n=\nwhite\n)\n\n\nscatter!\n(\nyear\n,\n \nmonoreg\n,\n \nlabel\n=\nIsotonic Regression\n,\n \nmarker\n=:\nx\n,\n\n        \nmarkercolor\n=\nred\n,\n \nmarkersize\n=\n2\n)\n\n\nxaxis!\n(\nYear\n)\n \n\nyaxis!\n(\nTemperature anomalies\n)\n\n\ntitle!\n(\nGlobal Warming Data\n)\n\n\n\n\n\n\n\n\nFollow this \nlink\n to access the .ipynb file of this page.", 
            "title": "Global Warming Data"
        }, 
        {
            "location": "/demo/warming/#global-warming-data", 
            "text": "Here we consider the annual data on temperature anomalies. As has been previously noted in the literature on isotonic regression, in general temperature appears to increase monotonically over the time period of 1850 to 2015 ( Wu et al., 2001 ;  Tibshirani et al., 2011 ). This monotonicity can be imposed on the coeffcient estimates using the constrained lasso with the inequality constraint matrix:   \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho||\\beta||_1 \\\\\n& \\text{ subject to} \\hspace{1em} \\boldsymbol{C\\beta} \\leq \\boldsymbol{d} \n\\end{split}   where    \n\\boldsymbol{C} = \\begin{pmatrix} \n1 & -1 &     &        &       &     & \\\\\n  & 1  & -1  &        &         &   & \\\\\n  &    &  1  & -1     &         &   & \\\\\n  &     &       & \\ddots & \\ddots &  & \\\\\n  &     &       &        &          & 1 & -1 \\\\\n\\end{pmatrix}   and $\\boldsymbol{d} = \\boldsymbol{0}.$  using   ConstrainedLasso   using   ECOS   First we load and organize the data.   warming   =   readcsv ( misc/warming.csv ,   header = true )[ 1 ]  year   =   warming [ : ,   1 ]  y      =   warming [ : ,   2 ]   166-element Array{Float64,1}:\n -0.375\n -0.223\n -0.224\n -0.271\n -0.246\n -0.271\n -0.352\n -0.46 \n -0.466\n -0.286\n -0.346\n -0.409\n -0.522\n  \u22ee    \n  0.45 \n  0.544\n  0.505\n  0.493\n  0.395\n  0.506\n  0.559\n  0.422\n  0.47 \n  0.499\n  0.567\n  0.746  n   =   p   =   size ( y ,   1 )  X   =   eye ( n )   166\u00d7166 Array{Float64,2}:\n 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n \u22ee                        \u22ee              \u22f1       \u22ee                        \u22ee  \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  1.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  1.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  1.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  1.0  Now we define inequality constraints as specified earlier.   C   =   [ eye ( p - 1 )   zeros ( p - 1 ,   1 )]   -   [ zeros ( p - 1 ,   1 )   eye ( p - 1 )]   165\u00d7166 Array{Float64,2}:\n 1.0  -1.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   1.0  -1.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   1.0  -1.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   1.0  -1.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   1.0  -1.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   1.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n \u22ee                             \u22ee    \u22f1   \u22ee                             \u22ee  \n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0     -1.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   1.0  -1.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   1.0  -1.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   1.0  -1.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   1.0  -1.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   1.0  -1.0  d   =   zeros ( size ( C ,   1 ))   165-element Array{Float64,1}:\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n \u22ee  \n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0  Then we estimate constrained lasso solution path. Here we use  ECOS  solver rather than the default  SCS  solver.   \u03b2\u0302path ,   \u03c1path ,   =   lsq_classopath ( X ,   y ;   Aineq   =   C ,   bineq   =   d ,   solver   =   ECOSSolver ( verbose = 0 ,   maxit = 1e8 ));    \u03b2\u0302path   166\u00d7198 Array{Float64,2}:\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026  -0.323225  -0.366     -0.375   \n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026  -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026  -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n 0.0  0.0  0.0  0.0  0.0  0.0          -0.294082  -0.336857  -0.345857\n \u22ee                        \u22ee         \u22f1   \u22ee                             \n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026   0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.432796   0.475571   0.484571\n 0.0  0.0  0.0  0.0  0.0  0.0           0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0       \u2026   0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0           0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0           0.437475   0.48025    0.48925 \n 0.0  0.0  0.0  0.0  0.0  0.0           0.447225   0.49       0.499   \n 0.0  0.0  0.0  0.0  0.0  0.0           0.515225   0.558      0.567   \n 0.0  0.0  0.0  0.0  0.0  0.011639  \u2026   0.699138   0.737854   0.746  In this formulation, isotonic regression is a special case of the constrained lasso with $\\rho=0.$ Below,  monoreg  is coefficient estimates obtained using isotonic regression.   monoreg   =   readdlm ( misc/monoreg.txt )   166\u00d71 Array{Float64,2}:\n -0.375   \n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n -0.345857\n  \u22ee       \n  0.484571\n  0.484571\n  0.484571\n  0.484571\n  0.484571\n  0.48925 \n  0.48925 \n  0.48925 \n  0.48925 \n  0.499   \n  0.567   \n  0.746  Now let's compare estimates by obtaining the largest absolute difference between isotonic regression constrained lasso fit.   maximum ( abs . ( monoreg   -   \u03b2\u0302path [ : ,   end ]))   1.2212453270876722e-15  Below is a figure that plots the constrained lasso fit at $\\rho = 0$ with the estimates using isotonic regression.  using   Plots ;   pyplot ();   scatter ( year ,   y ,   label = Observed Data ,   markerstrokecolor = darkblue ,  \n         markercolor = white )  scatter! ( year ,   \u03b2\u0302path [ : ,   end ],   label = Classopath (\u03c1=0) ,  \n         markerstrokecolor = black ,   marker =: rect ,   markercolor = white )  scatter! ( year ,   monoreg ,   label = Isotonic Regression ,   marker =: x , \n         markercolor = red ,   markersize = 2 )  xaxis! ( Year )   yaxis! ( Temperature anomalies )  title! ( Global Warming Data )    Follow this  link  to access the .ipynb file of this page.", 
            "title": "Global Warming Data"
        }, 
        {
            "location": "/demo/tumor/", 
            "text": "Brain Tumor Data\n\n\nHere we estimate a generalized lasso model (sparse fused lasso) via the constrained lasso. \n\n\nIn this example, we use a version of the comparative genomic hybridization (CGH) data from \nBredel et al. (2005)\n that was modified and studied by \nTibshirani and Wang (2008)\n\n\nThe dataset here contains CGH measurements from 2 glioblastoma multiforme (GBM) brain tumors. Tibshirani and Wang (2008) proposed using the sparse fused lasso to approximate the CGH signal by a sparse, piecewise constant function in order to determine the areas with non-zero values, as positive (negative) CGH values correspond to possible gains (losses). The sparse fused lasso (Tibshirani et al., 2005) is given by\n\n\n\n\n\n\\begin{split}\n\\text{minimize} \\hspace{1em} \\frac 12 ||\\boldsymbol{y}-\\boldsymbol{\\beta}||_2^2 + \\rho_1||\\boldsymbol{\\beta}||_1 + \\rho_2\\sum_{j=2}^p |\\beta_j - \\beta_{j-1}| \\hspace{5em} (1)\n\\end{split}\n\n\n\n\n\nThe sparse fused lasso is a special case of the generalized lasso with the penalty matrix. Therefore, the problem $(1)$ is equivalent to the following: \n\n\n\n\n\n\\begin{split} \n\\text{minimize} \\hspace{1em} \\frac 12 ||\\boldsymbol{y}-\\boldsymbol{X\\beta}||_2^2 + \\rho ||\\boldsymbol{D\\beta}||_1 \\hspace{5em} (2)\n\\end{split}\n\n\n\n\n\nwhere \n\n\n\n\n\n\\boldsymbol{D} = \\begin{pmatrix} \n1 & -1 &     &        &       &     & \\\\\n  & 1  & -1  &        &         &   & \\\\\n  &    &  1  & -1     &         &   & \\\\\n  &     &       & \\ddots & \\ddots &  & \\\\\n  &     &       &        &          & 1 & -1 \\\\\n1 &  &     &          &       &     & \\\\\n  & 1  &   &          &         &   & \\\\\n  &    &  \\ddots  &       &         &   & \\\\\n  &     &          & & \\ddots & & \\\\\n  &     &            &      &       & 1 & \\\\\n  &     &       &        &              &  & 1\\\\  \n\\end{pmatrix} \\in \\mathbb{R}^{(2P-1)\\times p}.\n\n\n\n\n\nAs discussed in \nGaines, B.R. and Zhou, H., (2016)\n, the sparse fused lasso can be reformulated and solved as a constrained lasso problem. The generalized lasso problem $(2)$ is equivalent to \n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12 ||\\widetilde{\\boldsymbol{y}} -\\widetilde{\\boldsymbol{X}}\\boldsymbol{\\alpha}||_2^2 + \\rho||\\boldsymbol{\\alpha}||_1 \\hspace{5em} (3) \\\\\n& \\text{subject to} \\hspace{1em} \\boldsymbol{U}^T_2\\boldsymbol{\\alpha} = \\boldsymbol{0}\n\\end{split}\n\n\n\n\n\nwhere $\\widetilde{\\boldsymbol{y}} = (\\boldsymbol{I}-\\boldsymbol{P}\n{\\boldsymbol{XV}_2})\\boldsymbol{y}$ and $\\widetilde{\\boldsymbol{X}} = (\\boldsymbol{I}-\\boldsymbol{P}\n_2})\\boldsymbol{XD}^+$. Note $D^+$ is the Moore-Penrose inverse of the matrix $\\boldsymbol{D}.$ and $\\boldsymbol{U_2}, \\boldsymbol{V_2}$ are obtained from singular value decomposition (SVD) of $\\boldsymbol{D}$ matrix. Then, the solution path $\\widehat{\\boldsymbol{\\alpha}}(\\rho)$ can be translated back to that of the original generalized lasso problem via \n\n\n\n\n\n\\widehat{\\boldsymbol{\\beta}}(\\rho) = [\\boldsymbol{I}-\\boldsymbol{V}_2(\\boldsymbol{V}_2^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{V}_2)^-\\boldsymbol{V}_2^T\\boldsymbol{X}^T\\boldsymbol{X}]\\boldsymbol{D}^+\\hat{\\boldsymbol{\\alpha}}(\\rho)+ \\boldsymbol{V}_2(\\boldsymbol{V}_2^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{V}_2)^-\\boldsymbol{V}_2^T\\boldsymbol{X}^T\\boldsymbol{y}\n\n\n\n\n\nwhere $\\boldsymbol{X}^-$ denotes the generalized inverse of a matrix $\\boldsymbol{X}$.\n\n\nDetails are found in Section 2 of [\n3\n]. \n\n\nusing\n \nConstrainedLasso\n\n\n\n\n\n\nWe load and organize the data first. Here, \ny\n is the response vector. The design matrix \nX\n is an identity matrix since the objective function in $(1)$ does not involve \nX\n. \n\n\ny\n \n=\n \nreaddlm\n(\nmisc/tumor.txt\n)\n\n\n\n\n\n\n990\u00d71 Array{Float64,2}:\n  0.333661 \n -0.152838 \n  0.101485 \n -0.0342123\n  0.344761 \n  0.151108 \n  0.798318 \n  0.282754 \n  0.116233 \n -0.232173 \n -0.754577 \n  1.06762  \n -0.017392 \n  \u22ee        \n -0.170825 \n -0.161826 \n -0.348987 \n -0.001227 \n -0.221422 \n  0.552795 \n -0.603429 \n -0.447907 \n -0.317569 \n -0.728202 \n -0.505593 \n -0.147661\n\n\n\n\n\nn\n \n=\n \np\n \n=\n \nsize\n(\ny\n,\n \n1\n)\n\n\nX\n \n=\n \neye\n(\nn\n)\n\n\n\n\n\n\n990\u00d7990 Array{Float64,2}:\n 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n \u22ee                        \u22ee              \u22f1            \u22ee                      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  1.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  1.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  1.0\n\n\n\n\n\nFirst we create a penalty matrix \nD\n. \n\n\nD\n \n=\n \n[\neye\n(\np\n-\n1\n)\n \nzeros\n(\np\n-\n1\n,\n \n1\n)]\n \n-\n \n[\nzeros\n(\np\n-\n1\n,\n \n1\n)\n \neye\n(\np\n-\n1\n)]\n\n\n\n\n\n\n989\u00d7990 Array{Float64,2}:\n 1.0  -1.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   1.0  -1.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   1.0  -1.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   1.0  -1.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   1.0  -1.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   1.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n \u22ee                             \u22ee    \u22f1         \u22ee                          \n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0     -1.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      1.0  -1.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   1.0  -1.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   1.0  -1.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   1.0  -1.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   1.0  -1.0\n\n\n\n\n\nNow we transform the problem to the constrained lasso problem. We do the singular value decomposition on \nD\n and extract singular values and necessary submatrices.  \n\n\nm\n \n=\n \nsize\n(\nD\n,\n \n1\n)\n\n\nF\n \n=\n \nsvdfact!\n(\nD\n,\n \nthin\n \n=\n \nfalse\n)\n\n\nsingvals\n \n=\n \nF\n[\n:\nS\n]\n\n\nrankD\n \n=\n \ncountnz\n(\nF\n[\n:\nS\n]\n \n.\n \nabs\n(\nF\n[\n:\nS\n][\n1\n])\n \n*\n \neps\n(\nF\n[\n:\nS\n][\n1\n])\n \n*\n \nmaximum\n(\nsize\n(\nD\n)))\n\n\n\nV1\n \n=\n \nF\n[\n:\nV\n][\n:\n,\n \n1\n:\nrankD\n]\n\n\nV2\n \n=\n \nF\n[\n:\nV\n][\n:\n,\n \nrankD\n+\n1\n:\nend\n]\n\n\nU1\n \n=\n \nF\n[\n:\nU\n][\n:\n,\n \n1\n:\nrankD\n]\n\n\nU2\n \n=\n \nF\n[\n:\nU\n][\n:\n,\n \nrankD\n+\n1\n:\nend\n];\n\n\n\n\n\n\nNow we calculate the Moore-Penrose inverse of \nD\n, which is $D^+$ in $(3)$, and transform the design matrix by multiplying by $D^+$. \n\n\nDplus\n \n=\n \nV1\n \n*\n \nbroadcast\n(\n*\n,\n \nU1\n,\n \n1.\n/\nF\n[\n:\nS\n])\n\n\nXDplus\n \n=\n \nX\n \n*\n \nDplus\n;\n\n\n\n\n\n\nIn the following code snippet, \nPxv2\n is a projection matrix onto \nC(XV2)\n and \nMxv2\n is the orthogonal projection matrix. Then we obtain the design matrix and response vector in their tilde form as shown in $(3)$. \n\n\nXV2\n \n=\n \nX\n \n*\n \nV2\n\n\nPxv2\n \n=\n \n(\n1\n \n/\n \ndot\n(\nXV2\n,\n \nXV2\n))\n \n*\n \nA_mul_Bt\n(\nXV2\n,\n \nXV2\n)\n\n\nMxv2\n \n=\n \neye\n(\nsize\n(\nXV2\n,\n \n1\n))\n \n-\n \nPxv2\n\n\n\u1ef9\n \n=\n \nvec\n(\nMxv2\n \n*\n \ny\n)\n\n\n\n\n\n\n990-element Array{Float64,1}:\n  0.351554   \n -0.134946   \n  0.119377   \n -0.0163198  \n  0.362654   \n  0.169001   \n  0.816211   \n  0.300646   \n  0.134125   \n -0.214281   \n -0.736685   \n  1.08552    \n  0.000500562\n  \u22ee          \n -0.152933   \n -0.143933   \n -0.331094   \n  0.0166656  \n -0.203529   \n  0.570688   \n -0.585536   \n -0.430014   \n -0.299676   \n -0.71031    \n -0.487701   \n -0.129769\n\n\n\n\n\nX\u0303\n \n=\n \nMxv2\n \n*\n \nXDplus\n\n\n\n\n\n\n990\u00d7989 Array{Float64,2}:\n  0.99899     0.99798     0.99697    \u2026   0.0030303   0.0020202   0.0010101\n -0.0010101   0.99798     0.99697        0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202   0.99697        0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303  \u2026   0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303  \u2026   0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n  \u22ee                                  \u22f1                                    \n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303  \u2026   0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303  \u2026   0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303     -0.99697     0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303     -0.99697    -0.99798     0.0010101\n -0.0010101  -0.0020202  -0.0030303     -0.99697    -0.99798    -0.99899\n\n\n\n\n\nWe solve the constrained lasso problem and obtain $\\widehat{\\boldsymbol{\\alpha}}(\\rho)$.\n\n\n\u03b1\u0302path\n,\n \n\u03c1path\n,\n \n=\n \nlsq_classopath\n(\nX\u0303\n,\n \n\u1ef9\n;\n \nsolver\n=\nsolver\n)\n\n\n\n\n\n\nNow we need to transform $\\widehat{\\boldsymbol{\\alpha}} (\\rho)$ back to $\\widehat{\\boldsymbol{\\beta}} (\\rho)$ as seen in (3).\n\n\n# transform back to beta\n\n\n\u03b2\u0302path\n \n=\n \nBase\n.\nLinAlg\n.\nBLAS\n.\nger!\n(\n1.0\n,\n \nvec\n(\nV2\n \n*\n \n((\n1\n \n/\n \ndot\n(\nXV2\n,\n \nXV2\n))\n \n*\n \n        \nAt_mul_B\n(\nXV2\n,\n \ny\n))),\n \nones\n(\nsize\n(\n\u03c1path\n)),\n \n(\neye\n(\nsize\n(\nV2\n,\n \n1\n))\n \n-\n \n        \nV2\n \n*\n \n((\n1\n \n/\n \ndot\n(\nXV2\n,\n \nXV2\n))\n \n*\n \nAt_mul_B\n(\nXV2\n,\n \nX\n)))\n \n*\n \nDplus\n \n*\n \n\u03b1\u0302path\n \n)\n\n\n\n\n\n\n990\u00d7989 Array{Float64,2}:\n -0.0178926  -0.0046696   0.118695    0.258754   \u2026   0.333516     0.333644  \n -0.0178926  -0.0046696   0.118695    0.258754      -0.152548    -0.152803  \n -0.0178926  -0.0046696   0.118695    0.258754       0.101194     0.10145   \n -0.0178926  -0.0046696   0.118695    0.258754      -0.0339217   -0.0341772 \n -0.0178926  -0.0046696   0.118695    0.258754       0.34447      0.344726  \n -0.0178926  -0.0046696   0.118695    0.258754   \u2026   0.151399     0.151143  \n -0.0178926  -0.0046696   0.118695    0.258754       0.798027     0.798283  \n -0.0178926  -0.0046696   0.118695    0.258754       0.282754     0.282754  \n -0.0178926  -0.0046696   0.118695    0.258754       0.116233     0.116233  \n -0.0178926  -0.0046696   0.118695    0.258754      -0.232173    -0.232173  \n -0.0178926  -0.0046696   0.118695    0.258754   \u2026  -0.754287    -0.754542  \n -0.0178926  -0.0046696   0.118695    0.258754       1.06733      1.06759   \n -0.0178926  -0.0046696   0.118695    0.258754      -0.0171014   -0.0173569 \n  \u22ee                                              \u22f1                          \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.170534    -0.17079   \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.162116    -0.161861  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215  \u2026  -0.348696    -0.348952  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.00151764  -0.00126215\n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.221131    -0.221386  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215      0.552504     0.55276   \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.603138    -0.603394  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215  \u2026  -0.447907    -0.447907  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.31786     -0.317604  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.727912    -0.728167  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.505593    -0.505593  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.147807    -0.147679\n\n\n\n\n\nWe plot the constrained lasso solution path below. \n\n\nusing\n \nPlots\n;\n \npyplot\n();\n \n# hide\n\n\nplot\n(\n\u03c1path\n,\n \n\u03b2\u0302path\n,\n \nlabel\n=\n,\n \nxaxis\n \n=\n \n(\n\u03c1\n,\n \n(\nminimum\n(\n\u03c1path\n),\n\n      \nmaximum\n(\n\u03c1path\n))),\n \nyaxis\n \n=\n \n(\n\u03b2\u0302(\u03c1)\n),\n \nwidth\n=\n0.5\n)\n\n\ntitle!\n(\nBrain Tumor Data: Solution Path via Constrained Lasso\n)\n\n\n\n\n\n\n\n\nNow let's compare our estimates with those from generalized lasso.  \n\n\nVariables \nlambda_path\n and \nbeta_path_fused\n are lambda values and estimated beta coefficients, respectively, obtained from \ngenlasso\n package in \nR\n. \n\n\nlambda_path\n \n=\n \nreaddlm\n(\nmisc/lambda_path.txt\n)\n\n\nbeta_path_fused\n \n=\n \nreaddlm\n(\nmisc/beta_path_fused.txt\n)[\n2\n:\nend\n,\n \n:\n]\n\n\n\n\n\n\nThe following figure plots generalized lasso solution path. \n\n\nplot\n(\nlambda_path\n,\n \nbeta_path_fused\n,\n \nlabel\n=\n,\n \nxaxis\n \n=\n \n(\n\u03bb\n,\n \n(\nminimum\n(\nlambda_path\n),\n\n      \nmaximum\n(\nlambda_path\n))),\n \nyaxis\n \n=\n \n(\n\u03b2\u0302(\u03bb)\n),\n \nwidth\n=\n0.5\n)\n\n\ntitle!\n(\nBrain Tumor Data: Generalized Lasso Solution Path\n)\n\n\n\n\n\n\n\n\nNow we extract common values of $\\rho$ and compare estimates at those values. \n\n\nsame\u03c1\n \n=\n \nintersect\n(\nround\n.\n(\n\u03c1path\n,\n \n4\n),\n \nround\n.\n(\nlambda_path\n,\n \n4\n))\n\n\nsame\u03c1_err\n \n=\n \n[]\n\n\nfor\n \ni\n \nin\n \neachindex\n(\nsame\u03c1\n)\n\n \ncur\u03c1\n \n=\n \nsame\u03c1\n[\ni\n]\n\n \nidx1\n \n=\n \nfindmin\n(\nabs\n.\n(\n\u03c1path\n \n-\n \ncur\u03c1\n))[\n2\n]\n\n \nidx2\n \n=\n \nfindmin\n(\nabs\n.\n(\nlambda_path\n \n-\n \ncur\u03c1\n))[\n2\n]\n\n \npush!\n(\nsame\u03c1_err\n,\n \nmaximum\n(\nabs\n.\n(\n\u03b2\u0302path\n[\n:\n,\n \nidx1\n]\n \n-\n \nbeta_path_fused\n[\n:\n,\n \nidx2\n])))\n\n\nend\n\n\nsame\u03c1_err\n\n\n\n\n\n\n988-element Array{Any,1}:\n 1.22121e-9\n 2.47148e-9\n 4.33354e-9\n 4.3335e-9 \n 3.64469e-8\n 4.41366e-8\n 3.35e-8   \n 4.53911e-8\n 4.29568e-7\n 3.18004e-7\n 4.94452e-7\n 2.62003e-7\n 3.8775e-7 \n \u22ee         \n 5.00008e-7\n 4.89495e-7\n 4.87999e-7\n 4.95e-7   \n 4.78989e-7\n 4.86001e-7\n 4.96003e-7\n 4.99498e-7\n 4.86994e-7\n 4.86994e-7\n 4.89997e-7\n 4.83994e-7\n\n\n\n\n\nBelow are the mean, median, and maximum of the errors between estimated coefficients at common $\\rho$ values. \n\n\nprintln\n([\nmean\n(\nsame\u03c1_err\n);\n \nmedian\n(\nsame\u03c1_err\n);\n \nmaximum\n(\nsame\u03c1_err\n)])\n\n\n\n\n\n\n[4.77914e-7, 4.80866e-7, 5.00013e-7]\n\n\n\n\n\nFollow this \nlink\n to access the .ipynb file of this page.", 
            "title": "Brain Tumor Data"
        }, 
        {
            "location": "/demo/tumor/#brain-tumor-data", 
            "text": "Here we estimate a generalized lasso model (sparse fused lasso) via the constrained lasso.   In this example, we use a version of the comparative genomic hybridization (CGH) data from  Bredel et al. (2005)  that was modified and studied by  Tibshirani and Wang (2008)  The dataset here contains CGH measurements from 2 glioblastoma multiforme (GBM) brain tumors. Tibshirani and Wang (2008) proposed using the sparse fused lasso to approximate the CGH signal by a sparse, piecewise constant function in order to determine the areas with non-zero values, as positive (negative) CGH values correspond to possible gains (losses). The sparse fused lasso (Tibshirani et al., 2005) is given by   \n\\begin{split}\n\\text{minimize} \\hspace{1em} \\frac 12 ||\\boldsymbol{y}-\\boldsymbol{\\beta}||_2^2 + \\rho_1||\\boldsymbol{\\beta}||_1 + \\rho_2\\sum_{j=2}^p |\\beta_j - \\beta_{j-1}| \\hspace{5em} (1)\n\\end{split}   The sparse fused lasso is a special case of the generalized lasso with the penalty matrix. Therefore, the problem $(1)$ is equivalent to the following:    \n\\begin{split} \n\\text{minimize} \\hspace{1em} \\frac 12 ||\\boldsymbol{y}-\\boldsymbol{X\\beta}||_2^2 + \\rho ||\\boldsymbol{D\\beta}||_1 \\hspace{5em} (2)\n\\end{split}   where    \n\\boldsymbol{D} = \\begin{pmatrix} \n1 & -1 &     &        &       &     & \\\\\n  & 1  & -1  &        &         &   & \\\\\n  &    &  1  & -1     &         &   & \\\\\n  &     &       & \\ddots & \\ddots &  & \\\\\n  &     &       &        &          & 1 & -1 \\\\\n1 &  &     &          &       &     & \\\\\n  & 1  &   &          &         &   & \\\\\n  &    &  \\ddots  &       &         &   & \\\\\n  &     &          & & \\ddots & & \\\\\n  &     &            &      &       & 1 & \\\\\n  &     &       &        &              &  & 1\\\\  \n\\end{pmatrix} \\in \\mathbb{R}^{(2P-1)\\times p}.   As discussed in  Gaines, B.R. and Zhou, H., (2016) , the sparse fused lasso can be reformulated and solved as a constrained lasso problem. The generalized lasso problem $(2)$ is equivalent to    \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12 ||\\widetilde{\\boldsymbol{y}} -\\widetilde{\\boldsymbol{X}}\\boldsymbol{\\alpha}||_2^2 + \\rho||\\boldsymbol{\\alpha}||_1 \\hspace{5em} (3) \\\\\n& \\text{subject to} \\hspace{1em} \\boldsymbol{U}^T_2\\boldsymbol{\\alpha} = \\boldsymbol{0}\n\\end{split}   where $\\widetilde{\\boldsymbol{y}} = (\\boldsymbol{I}-\\boldsymbol{P} {\\boldsymbol{XV}_2})\\boldsymbol{y}$ and $\\widetilde{\\boldsymbol{X}} = (\\boldsymbol{I}-\\boldsymbol{P} _2})\\boldsymbol{XD}^+$. Note $D^+$ is the Moore-Penrose inverse of the matrix $\\boldsymbol{D}.$ and $\\boldsymbol{U_2}, \\boldsymbol{V_2}$ are obtained from singular value decomposition (SVD) of $\\boldsymbol{D}$ matrix. Then, the solution path $\\widehat{\\boldsymbol{\\alpha}}(\\rho)$ can be translated back to that of the original generalized lasso problem via    \n\\widehat{\\boldsymbol{\\beta}}(\\rho) = [\\boldsymbol{I}-\\boldsymbol{V}_2(\\boldsymbol{V}_2^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{V}_2)^-\\boldsymbol{V}_2^T\\boldsymbol{X}^T\\boldsymbol{X}]\\boldsymbol{D}^+\\hat{\\boldsymbol{\\alpha}}(\\rho)+ \\boldsymbol{V}_2(\\boldsymbol{V}_2^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{V}_2)^-\\boldsymbol{V}_2^T\\boldsymbol{X}^T\\boldsymbol{y}   where $\\boldsymbol{X}^-$ denotes the generalized inverse of a matrix $\\boldsymbol{X}$.  Details are found in Section 2 of [ 3 ].   using   ConstrainedLasso   We load and organize the data first. Here,  y  is the response vector. The design matrix  X  is an identity matrix since the objective function in $(1)$ does not involve  X .   y   =   readdlm ( misc/tumor.txt )   990\u00d71 Array{Float64,2}:\n  0.333661 \n -0.152838 \n  0.101485 \n -0.0342123\n  0.344761 \n  0.151108 \n  0.798318 \n  0.282754 \n  0.116233 \n -0.232173 \n -0.754577 \n  1.06762  \n -0.017392 \n  \u22ee        \n -0.170825 \n -0.161826 \n -0.348987 \n -0.001227 \n -0.221422 \n  0.552795 \n -0.603429 \n -0.447907 \n -0.317569 \n -0.728202 \n -0.505593 \n -0.147661  n   =   p   =   size ( y ,   1 )  X   =   eye ( n )   990\u00d7990 Array{Float64,2}:\n 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n \u22ee                        \u22ee              \u22f1            \u22ee                      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  1.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  1.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  1.0  First we create a penalty matrix  D .   D   =   [ eye ( p - 1 )   zeros ( p - 1 ,   1 )]   -   [ zeros ( p - 1 ,   1 )   eye ( p - 1 )]   989\u00d7990 Array{Float64,2}:\n 1.0  -1.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   1.0  -1.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   1.0  -1.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   1.0  -1.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   1.0  -1.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   1.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n \u22ee                             \u22ee    \u22f1         \u22ee                          \n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0     -1.0   0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      1.0  -1.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0  \u2026   0.0   1.0  -1.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   1.0  -1.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   1.0  -1.0   0.0\n 0.0   0.0   0.0   0.0   0.0   0.0      0.0   0.0   0.0   0.0   1.0  -1.0  Now we transform the problem to the constrained lasso problem. We do the singular value decomposition on  D  and extract singular values and necessary submatrices.    m   =   size ( D ,   1 )  F   =   svdfact! ( D ,   thin   =   false )  singvals   =   F [ : S ]  rankD   =   countnz ( F [ : S ]   .   abs ( F [ : S ][ 1 ])   *   eps ( F [ : S ][ 1 ])   *   maximum ( size ( D )))  V1   =   F [ : V ][ : ,   1 : rankD ]  V2   =   F [ : V ][ : ,   rankD + 1 : end ]  U1   =   F [ : U ][ : ,   1 : rankD ]  U2   =   F [ : U ][ : ,   rankD + 1 : end ];   Now we calculate the Moore-Penrose inverse of  D , which is $D^+$ in $(3)$, and transform the design matrix by multiplying by $D^+$.   Dplus   =   V1   *   broadcast ( * ,   U1 ,   1. / F [ : S ])  XDplus   =   X   *   Dplus ;   In the following code snippet,  Pxv2  is a projection matrix onto  C(XV2)  and  Mxv2  is the orthogonal projection matrix. Then we obtain the design matrix and response vector in their tilde form as shown in $(3)$.   XV2   =   X   *   V2  Pxv2   =   ( 1   /   dot ( XV2 ,   XV2 ))   *   A_mul_Bt ( XV2 ,   XV2 )  Mxv2   =   eye ( size ( XV2 ,   1 ))   -   Pxv2  \u1ef9   =   vec ( Mxv2   *   y )   990-element Array{Float64,1}:\n  0.351554   \n -0.134946   \n  0.119377   \n -0.0163198  \n  0.362654   \n  0.169001   \n  0.816211   \n  0.300646   \n  0.134125   \n -0.214281   \n -0.736685   \n  1.08552    \n  0.000500562\n  \u22ee          \n -0.152933   \n -0.143933   \n -0.331094   \n  0.0166656  \n -0.203529   \n  0.570688   \n -0.585536   \n -0.430014   \n -0.299676   \n -0.71031    \n -0.487701   \n -0.129769  X\u0303   =   Mxv2   *   XDplus   990\u00d7989 Array{Float64,2}:\n  0.99899     0.99798     0.99697    \u2026   0.0030303   0.0020202   0.0010101\n -0.0010101   0.99798     0.99697        0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202   0.99697        0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303  \u2026   0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303  \u2026   0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n  \u22ee                                  \u22f1                                    \n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303  \u2026   0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303  \u2026   0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303      0.0030303   0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303     -0.99697     0.0020202   0.0010101\n -0.0010101  -0.0020202  -0.0030303     -0.99697    -0.99798     0.0010101\n -0.0010101  -0.0020202  -0.0030303     -0.99697    -0.99798    -0.99899  We solve the constrained lasso problem and obtain $\\widehat{\\boldsymbol{\\alpha}}(\\rho)$.  \u03b1\u0302path ,   \u03c1path ,   =   lsq_classopath ( X\u0303 ,   \u1ef9 ;   solver = solver )   Now we need to transform $\\widehat{\\boldsymbol{\\alpha}} (\\rho)$ back to $\\widehat{\\boldsymbol{\\beta}} (\\rho)$ as seen in (3).  # transform back to beta  \u03b2\u0302path   =   Base . LinAlg . BLAS . ger! ( 1.0 ,   vec ( V2   *   (( 1   /   dot ( XV2 ,   XV2 ))   *  \n         At_mul_B ( XV2 ,   y ))),   ones ( size ( \u03c1path )),   ( eye ( size ( V2 ,   1 ))   -  \n         V2   *   (( 1   /   dot ( XV2 ,   XV2 ))   *   At_mul_B ( XV2 ,   X )))   *   Dplus   *   \u03b1\u0302path   )   990\u00d7989 Array{Float64,2}:\n -0.0178926  -0.0046696   0.118695    0.258754   \u2026   0.333516     0.333644  \n -0.0178926  -0.0046696   0.118695    0.258754      -0.152548    -0.152803  \n -0.0178926  -0.0046696   0.118695    0.258754       0.101194     0.10145   \n -0.0178926  -0.0046696   0.118695    0.258754      -0.0339217   -0.0341772 \n -0.0178926  -0.0046696   0.118695    0.258754       0.34447      0.344726  \n -0.0178926  -0.0046696   0.118695    0.258754   \u2026   0.151399     0.151143  \n -0.0178926  -0.0046696   0.118695    0.258754       0.798027     0.798283  \n -0.0178926  -0.0046696   0.118695    0.258754       0.282754     0.282754  \n -0.0178926  -0.0046696   0.118695    0.258754       0.116233     0.116233  \n -0.0178926  -0.0046696   0.118695    0.258754      -0.232173    -0.232173  \n -0.0178926  -0.0046696   0.118695    0.258754   \u2026  -0.754287    -0.754542  \n -0.0178926  -0.0046696   0.118695    0.258754       1.06733      1.06759   \n -0.0178926  -0.0046696   0.118695    0.258754      -0.0171014   -0.0173569 \n  \u22ee                                              \u22f1                          \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.170534    -0.17079   \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.162116    -0.161861  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215  \u2026  -0.348696    -0.348952  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.00151764  -0.00126215\n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.221131    -0.221386  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215      0.552504     0.55276   \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.603138    -0.603394  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215  \u2026  -0.447907    -0.447907  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.31786     -0.317604  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.727912    -0.728167  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.505593    -0.505593  \n -0.0178926  -0.0212612  -0.0504995  -0.0838215     -0.147807    -0.147679  We plot the constrained lasso solution path below.   using   Plots ;   pyplot ();   # hide  plot ( \u03c1path ,   \u03b2\u0302path ,   label = ,   xaxis   =   ( \u03c1 ,   ( minimum ( \u03c1path ), \n       maximum ( \u03c1path ))),   yaxis   =   ( \u03b2\u0302(\u03c1) ),   width = 0.5 )  title! ( Brain Tumor Data: Solution Path via Constrained Lasso )    Now let's compare our estimates with those from generalized lasso.    Variables  lambda_path  and  beta_path_fused  are lambda values and estimated beta coefficients, respectively, obtained from  genlasso  package in  R .   lambda_path   =   readdlm ( misc/lambda_path.txt )  beta_path_fused   =   readdlm ( misc/beta_path_fused.txt )[ 2 : end ,   : ]   The following figure plots generalized lasso solution path.   plot ( lambda_path ,   beta_path_fused ,   label = ,   xaxis   =   ( \u03bb ,   ( minimum ( lambda_path ), \n       maximum ( lambda_path ))),   yaxis   =   ( \u03b2\u0302(\u03bb) ),   width = 0.5 )  title! ( Brain Tumor Data: Generalized Lasso Solution Path )    Now we extract common values of $\\rho$ and compare estimates at those values.   same\u03c1   =   intersect ( round . ( \u03c1path ,   4 ),   round . ( lambda_path ,   4 ))  same\u03c1_err   =   []  for   i   in   eachindex ( same\u03c1 ) \n  cur\u03c1   =   same\u03c1 [ i ] \n  idx1   =   findmin ( abs . ( \u03c1path   -   cur\u03c1 ))[ 2 ] \n  idx2   =   findmin ( abs . ( lambda_path   -   cur\u03c1 ))[ 2 ] \n  push! ( same\u03c1_err ,   maximum ( abs . ( \u03b2\u0302path [ : ,   idx1 ]   -   beta_path_fused [ : ,   idx2 ])))  end  same\u03c1_err   988-element Array{Any,1}:\n 1.22121e-9\n 2.47148e-9\n 4.33354e-9\n 4.3335e-9 \n 3.64469e-8\n 4.41366e-8\n 3.35e-8   \n 4.53911e-8\n 4.29568e-7\n 3.18004e-7\n 4.94452e-7\n 2.62003e-7\n 3.8775e-7 \n \u22ee         \n 5.00008e-7\n 4.89495e-7\n 4.87999e-7\n 4.95e-7   \n 4.78989e-7\n 4.86001e-7\n 4.96003e-7\n 4.99498e-7\n 4.86994e-7\n 4.86994e-7\n 4.89997e-7\n 4.83994e-7  Below are the mean, median, and maximum of the errors between estimated coefficients at common $\\rho$ values.   println ([ mean ( same\u03c1_err );   median ( same\u03c1_err );   maximum ( same\u03c1_err )])   [4.77914e-7, 4.80866e-7, 5.00013e-7]  Follow this  link  to access the .ipynb file of this page.", 
            "title": "Brain Tumor Data"
        }, 
        {
            "location": "/demo/micro/", 
            "text": "Microbiome Data\n\n\nThis real data application uses microbiome data [\n7\n]. The dataset itself contains information on 160 bacteria genera from 37 patients. The bacteria counts were $\\log_2$-transformed and normalized to have a constant average across samples.\n\n\nFirst, let's load and organize data.\n\n\nzerosum\n \n=\n \nreadcsv\n(\nmisc/zerosum.csv\n,\n \nheader\n=\ntrue\n)[\n1\n]\n\n\ny\n \n=\n \nzerosum\n[\n:\n,\n \n1\n]\n\n\n\n\n\n\n37-element Array{Float64,1}:\n   3.1158 \n   3.21448\n -11.1341 \n  -5.13988\n  -4.8247 \n  -4.79219\n -11.5719 \n  -5.77868\n   4.97972\n  -3.38806\n  -9.90973\n   3.07384\n   2.77814\n   \u22ee      \n  -7.41032\n   4.70871\n   1.49355\n   3.93736\n  -3.29476\n -10.9239 \n -11.021  \n   3.18789\n  -8.73771\n -11.499  \n   3.66284\n  -8.88277\n\n\n\n\n\nX\n \n=\n \nzerosum\n[\n:\n,\n \n2\n:\nend\n]\n\n\n\n\n\n\n37\u00d7160 Array{Float64,2}:\n 0.0  3.32193  0.0  1.0      10.5304   \u2026  0.0      0.0  0.0  0.0  12.513  \n 0.0  4.08746  0.0  0.0       7.35755     0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  3.32193  0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  1.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   1.0    \n 0.0  3.16993  0.0  0.0       0.0      \u2026  0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       1.0         1.58496  0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       4.64386     0.0      0.0  0.0  0.0   1.58496\n 0.0  1.0      0.0  0.0       0.0         0.0      0.0  0.0  1.0   0.0    \n 0.0  0.0      0.0  0.0       0.0      \u2026  0.0      0.0  0.0  0.0   1.0    \n 0.0  5.45943  0.0  0.0       3.90689     0.0      0.0  0.0  0.0   1.58496\n 0.0  4.70044  0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0    \n \u22ee                                     \u22f1  \u22ee                               \n 0.0  0.0      0.0  0.0       0.0      \u2026  0.0      0.0  0.0  0.0   0.0    \n 0.0  3.58496  0.0  0.0      12.2886      0.0      0.0  0.0  0.0   7.39232\n 0.0  4.80735  0.0  6.87036   8.01123     0.0      0.0  0.0  0.0   3.0    \n 0.0  0.0      0.0  0.0       5.93074     0.0      0.0  0.0  0.0   0.0    \n 0.0  7.7211   0.0  0.0       1.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       0.0      \u2026  0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      1.0  0.0       1.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  4.08746  0.0  0.0       3.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   1.0    \n 0.0  5.04439  0.0  0.0       0.0      \u2026  0.0      0.0  0.0  0.0  10.444  \n 0.0  0.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0\n\n\n\n\n\nAltenbuchinger et al.\n demonstrated that a sum-to-zero constraint is useful anytime the normalization of data relative to some reference point results in proportional data, as is often the case in biological applications, since the analysis using the constraint is insensitive to the choice of the reference. \nAltenbuchinger et al.\n derived a coordinate descent algorithm for the elastic net with a zero-sum constraint, \n\n\n\n\n\n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho\\Big(||\\boldsymbol{\\beta}||_1 + \\frac{1-\\alpha}{2}||\\boldsymbol{\\beta}||_2^2\\Big) \\\\\n& \\text{subject to} \\hspace{1em} \\sum_j \\beta_j = 0\n\\end{split}\n\n\n\n\n\nbut the focus of their analysis corresponds to $\\alpha = 1$. Hence the problem is reduced to the constrained lasso.\n\n\nWe set up the zero-sum constraint.\n\n\nn\n,\n \np\n \n=\n \nsize\n(\nX\n)\n\n\nAeq\n \n=\n \nones\n(\n1\n,\n \np\n)\n\n\nbeq\n \n=\n \n[\n0\n]\n\n\nm1\n \n=\n \nsize\n(\nAeq\n,\n \n1\n);\n\n\n\n\n\n\nNow we estimate the constrained lasso solution path using path algorithm.\n\n\nusing\n \nConstrainedLasso\n\n\nusing\n \nECOS\n\n\nsolver\n \n=\n \nECOSSolver\n(\nverbose\n=\n0\n,\n \nmaxit\n=\n1e8\n);\n\n\n\u03b2\u0302path\n,\n \n\u03c1path\n,\n \n=\n \nlsq_classopath\n(\nX\n,\n \ny\n;\n \nAeq\n \n=\n \nAeq\n,\n \nbeq\n \n=\n \nbeq\n,\n \nsolver\n \n=\n \nsolver\n)\n\n\n\n\n\n\n\u03b2\u0302path\n\n\n\n\n\n\n160\u00d779 Array{Float64,2}:\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026   0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.484646   0.488903   0.490802 \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     -0.209662  -0.248845  -0.253282 \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026   0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.897273   0.888551   0.889344 \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026   0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n \u22ee                        \u22ee              \u22f1                                  \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026   0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0       -0.0220978\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026   0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.659693   0.659783   0.658592\n\n\n\n\n\nThen we calculate \nL1\n norm of coefficients at each $\\rho$.\n\n\nnorm1path\n \n=\n \nzeros\n(\nsize\n(\n\u03b2\u0302path\n,\n \n2\n))\n\n\nfor\n \ni\n \nin\n \neachindex\n(\nnorm1path\n)\n\n    \nnorm1path\n[\ni\n]\n \n=\n \nnorm\n(\n\u03b2\u0302path\n[\n:\n,\n \ni\n],\n \n1\n)\n\n\nend\n\n\nnorm1path\n\n\n\n\n\n\n79-element Array{Float64,1}:\n  0.0     \n  0.0     \n  0.1462  \n  0.236204\n  0.368326\n  0.75722 \n  1.08009 \n  1.43022 \n  1.45122 \n  1.6103  \n  1.95359 \n  1.9595  \n  1.96205 \n  \u22ee       \n 11.2506  \n 11.5025  \n 11.8854  \n 11.9934  \n 12.0591  \n 12.2261  \n 12.2836  \n 12.7327  \n 12.9722  \n 13.0533  \n 13.221   \n 13.2708\n\n\n\n\n\nNow, let's plot the solution path, $\\widehat{\\boldsymbol{\\beta}}(\\rho)$, as a function of $||\\widehat{\\boldsymbol{\\beta}}(\\rho)||_1$ using constrained lasso.\n\n\nusing\n \nPlots\n;\n \npyplot\n();\n\n\nplot\n(\nnorm1path\n,\n \n\u03b2\u0302path\n,\n \nxaxis\n \n=\n \n(\n||\u03b2\u0302||\u2081\n),\n \nyaxis\n=\n(\n\u03b2\u0302\n),\n \nlabel\n=\n)\n\n\ntitle!\n(\nMicrobiome Data: Solution Path via Constrained Lasso\n)\n\n\n\n\n\n\n\n\nFollow this \nlink\n to access the .ipynb file of this page.", 
            "title": "Microbiome Data"
        }, 
        {
            "location": "/demo/micro/#microbiome-data", 
            "text": "This real data application uses microbiome data [ 7 ]. The dataset itself contains information on 160 bacteria genera from 37 patients. The bacteria counts were $\\log_2$-transformed and normalized to have a constant average across samples.  First, let's load and organize data.  zerosum   =   readcsv ( misc/zerosum.csv ,   header = true )[ 1 ]  y   =   zerosum [ : ,   1 ]   37-element Array{Float64,1}:\n   3.1158 \n   3.21448\n -11.1341 \n  -5.13988\n  -4.8247 \n  -4.79219\n -11.5719 \n  -5.77868\n   4.97972\n  -3.38806\n  -9.90973\n   3.07384\n   2.77814\n   \u22ee      \n  -7.41032\n   4.70871\n   1.49355\n   3.93736\n  -3.29476\n -10.9239 \n -11.021  \n   3.18789\n  -8.73771\n -11.499  \n   3.66284\n  -8.88277  X   =   zerosum [ : ,   2 : end ]   37\u00d7160 Array{Float64,2}:\n 0.0  3.32193  0.0  1.0      10.5304   \u2026  0.0      0.0  0.0  0.0  12.513  \n 0.0  4.08746  0.0  0.0       7.35755     0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  3.32193  0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  1.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   1.0    \n 0.0  3.16993  0.0  0.0       0.0      \u2026  0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       1.0         1.58496  0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       4.64386     0.0      0.0  0.0  0.0   1.58496\n 0.0  1.0      0.0  0.0       0.0         0.0      0.0  0.0  1.0   0.0    \n 0.0  0.0      0.0  0.0       0.0      \u2026  0.0      0.0  0.0  0.0   1.0    \n 0.0  5.45943  0.0  0.0       3.90689     0.0      0.0  0.0  0.0   1.58496\n 0.0  4.70044  0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0    \n \u22ee                                     \u22f1  \u22ee                               \n 0.0  0.0      0.0  0.0       0.0      \u2026  0.0      0.0  0.0  0.0   0.0    \n 0.0  3.58496  0.0  0.0      12.2886      0.0      0.0  0.0  0.0   7.39232\n 0.0  4.80735  0.0  6.87036   8.01123     0.0      0.0  0.0  0.0   3.0    \n 0.0  0.0      0.0  0.0       5.93074     0.0      0.0  0.0  0.0   0.0    \n 0.0  7.7211   0.0  0.0       1.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       0.0      \u2026  0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      1.0  0.0       1.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  4.08746  0.0  0.0       3.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0    \n 0.0  0.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   1.0    \n 0.0  5.04439  0.0  0.0       0.0      \u2026  0.0      0.0  0.0  0.0  10.444  \n 0.0  0.0      0.0  0.0       0.0         0.0      0.0  0.0  0.0   0.0  Altenbuchinger et al.  demonstrated that a sum-to-zero constraint is useful anytime the normalization of data relative to some reference point results in proportional data, as is often the case in biological applications, since the analysis using the constraint is insensitive to the choice of the reference.  Altenbuchinger et al.  derived a coordinate descent algorithm for the elastic net with a zero-sum constraint,    \n\\begin{split}\n& \\text{minimize} \\hspace{1em} \\frac 12||\\boldsymbol{y}-\\boldsymbol{X\\beta}||^2_2 + \\rho\\Big(||\\boldsymbol{\\beta}||_1 + \\frac{1-\\alpha}{2}||\\boldsymbol{\\beta}||_2^2\\Big) \\\\\n& \\text{subject to} \\hspace{1em} \\sum_j \\beta_j = 0\n\\end{split}   but the focus of their analysis corresponds to $\\alpha = 1$. Hence the problem is reduced to the constrained lasso.  We set up the zero-sum constraint.  n ,   p   =   size ( X )  Aeq   =   ones ( 1 ,   p )  beq   =   [ 0 ]  m1   =   size ( Aeq ,   1 );   Now we estimate the constrained lasso solution path using path algorithm.  using   ConstrainedLasso  using   ECOS  solver   =   ECOSSolver ( verbose = 0 ,   maxit = 1e8 );  \u03b2\u0302path ,   \u03c1path ,   =   lsq_classopath ( X ,   y ;   Aeq   =   Aeq ,   beq   =   beq ,   solver   =   solver )   \u03b2\u0302path   160\u00d779 Array{Float64,2}:\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026   0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.484646   0.488903   0.490802 \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     -0.209662  -0.248845  -0.253282 \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026   0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.897273   0.888551   0.889344 \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026   0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n \u22ee                        \u22ee              \u22f1                                  \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026   0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0       -0.0220978\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \u2026   0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0        0.0        0.0      \n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      0.659693   0.659783   0.658592  Then we calculate  L1  norm of coefficients at each $\\rho$.  norm1path   =   zeros ( size ( \u03b2\u0302path ,   2 ))  for   i   in   eachindex ( norm1path ) \n     norm1path [ i ]   =   norm ( \u03b2\u0302path [ : ,   i ],   1 )  end  norm1path   79-element Array{Float64,1}:\n  0.0     \n  0.0     \n  0.1462  \n  0.236204\n  0.368326\n  0.75722 \n  1.08009 \n  1.43022 \n  1.45122 \n  1.6103  \n  1.95359 \n  1.9595  \n  1.96205 \n  \u22ee       \n 11.2506  \n 11.5025  \n 11.8854  \n 11.9934  \n 12.0591  \n 12.2261  \n 12.2836  \n 12.7327  \n 12.9722  \n 13.0533  \n 13.221   \n 13.2708  Now, let's plot the solution path, $\\widehat{\\boldsymbol{\\beta}}(\\rho)$, as a function of $||\\widehat{\\boldsymbol{\\beta}}(\\rho)||_1$ using constrained lasso.  using   Plots ;   pyplot ();  plot ( norm1path ,   \u03b2\u0302path ,   xaxis   =   ( ||\u03b2\u0302||\u2081 ),   yaxis = ( \u03b2\u0302 ),   label = )  title! ( Microbiome Data: Solution Path via Constrained Lasso )    Follow this  link  to access the .ipynb file of this page.", 
            "title": "Microbiome Data"
        }, 
        {
            "location": "/references/", 
            "text": "References\n\n\n\n\n[1] Altenbuchinger, M., Rehberg, T., Zacharias, H.U., St\u00e4mmler, F., Dettmer, K., Weber, D., Hiergeist, A., Gessner, A., Holler, E., Oefner, P.J. and Spang, R. (2016). Reference point insensitive molecular data analysis. \nBioinformatics\n, 33(2):219-226.\n\n\n[2] Bredel, M., Bredel, C., Juric, D., Harsh, G. R., Vogel, H., Recht, L. D., and Sikic, B. I. (2005). High-resolution genome-wide mapping of genetic alterations in human glial brain tumors. \nCancer Research\n, 65:4088\u20134096.\n\n\n[3] Gaines, B.R. and Zhou, H. (2016). Algorithms for fitting the constrained lasso. arXiv preprint arXiv:1611.01511.\n\n\n[4] Stamey, T.A., Kabalin, J.N., McNeal, J.E., Johnstone, I.M., Freiha, F., Redwine, E.A. and Yang, N. (1989). Prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate: II. radical prostatectomy treated patients. \nJournal of Urology\n 141(5):1076\u20131083.\n\n\n[5] Tibshirani, R. J., Hoefling, H., and Tibshirani, R. (2011). Nearly-isotonic regression. \nTechnometrics\n, 53:54\u201361.\n\n\n[6] Tibshirani, R. and Wang, P. (2008). Spatial smoothing and hot spot detection for CGH data using the fused lasso. \nBiostatistics\n, 9:18\u201329.\n\n\n[7] Weber, D., Oefner, P. J., Hiergeist, A., Koestler, J., Gessner, A., Weber, M., Hahn, J., Wolff, D., Sta \u0308mmler, F., Spang, R., Herr, W., Dettmer, K., and Holler, E. (2015). Low urinary undoxyl sulfate levels early after transplantation reflect a disrupted microbiome and are associated with poor outcome. \nBlood\n, 126:1723\u20131728.\n\n\n[8] Wu, W. B., Woodroofe, M., and Mentz, G. (2001). Isotonic regression: another look at the changepoint problem. \nBiometrika\n, 88:793\u2013804.", 
            "title": "References"
        }, 
        {
            "location": "/references/#references", 
            "text": "[1] Altenbuchinger, M., Rehberg, T., Zacharias, H.U., St\u00e4mmler, F., Dettmer, K., Weber, D., Hiergeist, A., Gessner, A., Holler, E., Oefner, P.J. and Spang, R. (2016). Reference point insensitive molecular data analysis.  Bioinformatics , 33(2):219-226.  [2] Bredel, M., Bredel, C., Juric, D., Harsh, G. R., Vogel, H., Recht, L. D., and Sikic, B. I. (2005). High-resolution genome-wide mapping of genetic alterations in human glial brain tumors.  Cancer Research , 65:4088\u20134096.  [3] Gaines, B.R. and Zhou, H. (2016). Algorithms for fitting the constrained lasso. arXiv preprint arXiv:1611.01511.  [4] Stamey, T.A., Kabalin, J.N., McNeal, J.E., Johnstone, I.M., Freiha, F., Redwine, E.A. and Yang, N. (1989). Prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate: II. radical prostatectomy treated patients.  Journal of Urology  141(5):1076\u20131083.  [5] Tibshirani, R. J., Hoefling, H., and Tibshirani, R. (2011). Nearly-isotonic regression.  Technometrics , 53:54\u201361.  [6] Tibshirani, R. and Wang, P. (2008). Spatial smoothing and hot spot detection for CGH data using the fused lasso.  Biostatistics , 9:18\u201329.  [7] Weber, D., Oefner, P. J., Hiergeist, A., Koestler, J., Gessner, A., Weber, M., Hahn, J., Wolff, D., Sta \u0308mmler, F., Spang, R., Herr, W., Dettmer, K., and Holler, E. (2015). Low urinary undoxyl sulfate levels early after transplantation reflect a disrupted microbiome and are associated with poor outcome.  Blood , 126:1723\u20131728.  [8] Wu, W. B., Woodroofe, M., and Mentz, G. (2001). Isotonic regression: another look at the changepoint problem.  Biometrika , 88:793\u2013804.", 
            "title": "References"
        }
    ]
}